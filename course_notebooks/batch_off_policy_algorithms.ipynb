{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import Tensor, LongTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import numbers\n",
    "from datetime import datetime\n",
    "# Multi-threading\n",
    "import threading\n",
    "# Plotting\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "# Reinforcement learning\n",
    "import gym\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "outputs": [],
   "source": [
    "def default_params():\n",
    "    \"\"\" These are the default parameters used int eh framework. \"\"\"\n",
    "    return {  # Debugging outputs and plotting during training\n",
    "        'plot_frequency': 10,  # plots a debug message avery n steps\n",
    "        'plot_train_samples': True,  # whether the x-axis is env.steps (True) or episodes (False)\n",
    "        'print_when_plot': True,  # prints debug message if True\n",
    "        'print_dots': False,  # prints dots for every gradient update\n",
    "        # Environment parameters\n",
    "        'env': 'CartPole-v0',  # the environment the agent is learning in\n",
    "        'run_steps': 2048,  # samples whole episodes if run_steps <= 0\n",
    "        'max_episode_length': 200,  # maximum number of steps per episode\n",
    "        # Runner parameters\n",
    "        'max_episodes': int(1E6),  # experiment stops after this many episodes\n",
    "        # 'max_episodes': 1000,         # experiment stops after this many episodes\n",
    "        'max_steps': int(2E6),  # experiment stops after this many steps\n",
    "        # 'max_steps': 10000,            # experiment stops after this many steps\n",
    "        'multi_runner': True,  # uses multiple runners if True\n",
    "        'parallel_environments': 4,  # number of parallel runners  (only if multi_runner==True)\n",
    "        # Exploration parameters\n",
    "        'epsilon_anneal_time': int(2),  # exploration anneals epsilon over these many steps\n",
    "        'epsilon_finish': 0.1,  # annealing stops at (and keeps) this epsilon\n",
    "        'epsilon_start': 1,  # annealing starts at this epsilon\n",
    "        # Optimization parameters\n",
    "        'lr': 1E-3,  # learning rate of optimizer\n",
    "        'gamma': 0.99,  # discount factor gamma\n",
    "        'mini_batch_size': 200,  # number of transitions in a mini-batch\n",
    "        'batch_size': 500,  # number of transitions in a mini-batch\n",
    "        'grad_norm_clip': 1,  # gradent clipping if grad norm is larger than this\n",
    "        # Actor-critic parameters\n",
    "        'value_loss_param': 0.1,  # governs the relative impact of the value relative to policy loss\n",
    "        'advantage_bias': True,  # whether the advantages have the value as bias\n",
    "        'advantage_bootstrap': True,  # whether advantages use bootstrapping (alternatively: returns)\n",
    "        'offpolicy_iterations': 0,  # how many off-policy iterations are performed\n",
    "        'value_targets': 'returns',  # either 'returns' or 'td' as regression targets of the value function\n",
    "        # PPO parameters\n",
    "        'ppo_clipping': True,  # whether we use the PPO loss\n",
    "        'ppo_clip_eps': 0.1,  # the epsilon for the PPO loss\n",
    "\n",
    "        'states_shape': (1,),  # Amount of states\n",
    "        'num_actions': 5,  # delay between the two stacked observations\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "outputs": [],
   "source": [
    "class TransitionBatch:\n",
    "    \"\"\" Simple implementation of a batchof transitionsm (or another dictionary-based tensor structure).\n",
    "        Read and write operations are thread-safe, but the iterator is not (you cannot interate\n",
    "        over the same TransitionBatch in two threads at the same time). \"\"\"\n",
    "\n",
    "    def __init__(self, max_size, transition_format, batch_size=32):\n",
    "        self.indices = []\n",
    "        self.size = 0\n",
    "        self.first = 0\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dict = {}\n",
    "        for key, spec in transition_format.items():\n",
    "            self.dict[key] = th.zeros([max_size, *spec[0]], dtype=spec[1])\n",
    "\n",
    "    def _clone_empty_batch(self, max_size=None, batch_size=None):\n",
    "        \"\"\" Clones this TransitionBatch without cloning the data. \"\"\"\n",
    "        max_size = self.max_size if max_size is None else max_size\n",
    "        batch_size = self.batch_size if batch_size is None else batch_size\n",
    "        return TransitionBatch(max_size=max_size, transition_format={}, batch_size=batch_size)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\" Access the TransitionBatch with the [] operator. Use as key either\n",
    "            - the string name of a variable to get the full tensor of that variable,\n",
    "            - a slice to get a time-slice over all variables in the batch,\n",
    "            - a LongTensor that selects a subset of indices for all variables in the batch. \"\"\"\n",
    "        # Return the entry of the transition called \"key\"\n",
    "        if isinstance(key, str):\n",
    "            return self.dict[key]\n",
    "        # Return a slice of the batch\n",
    "        if isinstance(key, slice):\n",
    "            key = slice(0 if key.start is None else key.start, self.size if key.stop is None else key.stop,\n",
    "                        1 if key.step is None else key.step)\n",
    "            try:\n",
    "                batch = self._clone_empty_batch()\n",
    "                batch.size = (key.stop - key.start) // key.step\n",
    "                for k, v in self.dict.items():\n",
    "                    batch.dict[k] = v[key]\n",
    "            finally:\n",
    "                pass\n",
    "            return batch\n",
    "        # Collect and return a set of transitions specified by the LongTensor \"key\"\n",
    "        if isinstance(key, th.Tensor):\n",
    "            try:\n",
    "                batch = self._clone_empty_batch(max_size=key.shape[0])\n",
    "                batch.size = key.shape[0]\n",
    "                for k, v in self.dict.items():\n",
    "                    key = key.view(batch.size, *[1 for _ in range(len(v.shape[1:]))])\n",
    "                    batch.dict[k] = v.gather(dim=0, index=key.expand(batch.size, *v.shape[1:]))\n",
    "            finally:\n",
    "                pass\n",
    "            return batch\n",
    "        return None\n",
    "\n",
    "    def get_first(self):\n",
    "        \"\"\" Returns a batch of the oldest entries of all variables. \"\"\"\n",
    "        batch = self._clone_empty_batch(max_size=1)\n",
    "        try:\n",
    "            batch.size = 1\n",
    "            for k, v in self.dict.items():\n",
    "                batch.dict[k] = v[self.first].unsqueeze(dim=0)\n",
    "        finally:\n",
    "            pass\n",
    "        return batch\n",
    "\n",
    "    def get_last(self):\n",
    "        \"\"\" Returns a batch of the newest entries of all variables. \"\"\"\n",
    "        batch = self._clone_empty_batch(max_size=1)\n",
    "        try:\n",
    "            batch.size = 1\n",
    "            for k, v in self.dict.items():\n",
    "                batch.dict[k] = v[(self.first + self.size - 1) % self.size].unsqueeze(dim=0)\n",
    "        finally:\n",
    "            pass\n",
    "        return batch\n",
    "\n",
    "    def add(self, trans: dict):\n",
    "        \"\"\" Adding transition dictionaries, which can contain Tensors of arbitrary length. \"\"\"\n",
    "        if isinstance(trans, TransitionBatch):\n",
    "            trans = trans.dict\n",
    "        try:\n",
    "            n = 0\n",
    "            idx = None\n",
    "            for k, v in trans.items():\n",
    "                if idx is None:\n",
    "                    n = v.shape[0]\n",
    "                    idx = th.LongTensor([(self.first + self.size + i) % self.max_size for i in range(n)])\n",
    "                else:\n",
    "                    assert n == v.shape[0], 'all tensors in a transition need to have the same batch_size'\n",
    "                idx = idx.view(idx.shape[0], *[1 for _ in range(len(v.shape) - 1)])\n",
    "                self.dict[k].scatter_(dim=0, index=idx.expand_as(v), src=v)\n",
    "            # Increase the size (and handle overflow)\n",
    "            self.size += n\n",
    "            if self.size > self.max_size:\n",
    "                self.first = (self.first + n) % self.max_size\n",
    "                self.size = self.max_size\n",
    "        finally:\n",
    "            pass\n",
    "        return self\n",
    "\n",
    "    def trim(self):\n",
    "        \"\"\" Reduces the length of the max_size to its actual size (in-place). Returns self. \"\"\"\n",
    "        try:\n",
    "            for k, v in self.dict.items():\n",
    "                self.dict[k] = v[:self.size]\n",
    "            self.max_size = self.size\n",
    "        finally:\n",
    "            pass\n",
    "        return self\n",
    "\n",
    "    def replace(self, batch, index=0):\n",
    "        \"\"\" Replaces parts of this batch with another batch (which must be smaller). \"\"\"\n",
    "        try:\n",
    "            # assert batch.max_size <= self.max_size - index, \"Replacement is larger then target area in batch.\"\n",
    "            assert batch.size <= self.max_size - index, \"Replacement is larger then target area in batch.\"\n",
    "            for k, v in batch.dict.items():\n",
    "                if batch.size < batch.max_size:\n",
    "                    v = v[:batch.size]\n",
    "                self.dict[k][index:(index + batch.max_size)] = v\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    def sample(self, batch_size=0):\n",
    "        \"\"\" Samples a random mini-batch from the batch. \"\"\"\n",
    "        if batch_size==0:\n",
    "            batch_size = self.batch_size\n",
    "        return self[th.randint(high=self.size, size=(batch_size, 1))]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the length of the batch. \"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Initializes an iterator over the batch. \"\"\"\n",
    "        self.indices = list(range(self.size))\n",
    "        np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\" Iterates through batch, returns list of contiguous tensors. \"\"\"\n",
    "        if len(self.indices) == 0: raise StopIteration\n",
    "        size = min(self.batch_size, len(self.indices))\n",
    "        batch = self[th.LongTensor(self.indices[-size:])]\n",
    "        self.indices = self.indices[:-size]\n",
    "        return batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    \"\"\" Implements a simple single-thread runner class. \"\"\"\n",
    "\n",
    "    def __init__(self, controller, params={}, exploration_step=1):\n",
    "        self.env = gym.make(params.get('env', 'CartPole-v0'))\n",
    "        self.cont_actions = isinstance(self.env.action_space, gym.spaces.Box)\n",
    "        self.controller = controller\n",
    "        self.epi_len = params.get('max_episode_length', self.env._max_episode_steps)\n",
    "        self.gamma = params.get('gamma', 0.99)\n",
    "        self.use_pixels = params.get('pixel_observations', False)\n",
    "        if self.use_pixels:\n",
    "            self.grayscale = params.get('pixel_grayscale', True)\n",
    "            self.add_last_obs = params.get('pixel_add_last_obs', False)\n",
    "            self.last_obs_delay = params.get('pixel_last_obs_delay', 4)\n",
    "            n_colors = 1 if self.grayscale else 3\n",
    "            n_feats = n_colors * (2 if self.add_last_obs else 1)\n",
    "            resolution = params.get('pixel_resolution', (25, 25))\n",
    "            self.state_shape = (n_feats, *resolution)\n",
    "            self.last_observations = TransitionBatch(max_size=self.last_obs_delay,\n",
    "                                                     transition_format={'img': ((n_colors, *resolution), th.float32)})\n",
    "        else:\n",
    "            self.state_shape = self.env.observation_space.shape\n",
    "        # Set up current state and time step\n",
    "        self.sum_rewards = 0\n",
    "        self.state = None\n",
    "        self.time = 0\n",
    "        self._next_step()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Closes the underlying environment. Should always when ending an experiment. \"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "    def transition_format(self):\n",
    "        \"\"\" Returns the format of transtions: a dictionary of (shape, dtype) entries for each key. \"\"\"\n",
    "        return {'actions': ((1,), th.long),\n",
    "                'states': (self.state_shape, th.float32),\n",
    "                'next_states': (self.state_shape, th.float32),\n",
    "                'rewards': ((1,), th.float32),\n",
    "                'dones': ((1,), th.bool),\n",
    "                'returns': ((1,), th.float32)}\n",
    "\n",
    "    def _wrap_transition(self, s, a, r, ns, d):\n",
    "        \"\"\" Takes a transition and returns a corresponding dictionary. \"\"\"\n",
    "        trans = {}\n",
    "        form = self.transition_format()\n",
    "        for key, val in [('states', s), ('actions', a), ('rewards', r), ('next_states', ns), ('dones', d)]:\n",
    "            if not isinstance(val, th.Tensor):\n",
    "                if isinstance(val, numbers.Number) or isinstance(val, bool): val = [val]\n",
    "                val = th.tensor(val, dtype=form[key][1])\n",
    "            if len(val.shape) < len(form[key][0]) + 1: val = val.unsqueeze(dim=0)\n",
    "            trans[key] = val\n",
    "        return trans\n",
    "\n",
    "    def _pixel_observation(self, reset=False):\n",
    "        \"\"\" Returns the pixel-observation fo the current state. Opens extra window for rendering. \"\"\"\n",
    "        img = self.env.render(mode='rgb_array')\n",
    "        img = cv2.resize(img, dsize=self.state_shape[1:], interpolation=cv2.INTER_CUBIC)\n",
    "        img = th.from_numpy(img.astype(np.float32) / 255).transpose(dim0=0, dim1=2).unsqueeze(dim=0)\n",
    "        if self.grayscale: img = img.mean(dim=1, keepdim=True)\n",
    "        if self.add_last_obs:\n",
    "            if reset: self.last_observations.size = 0\n",
    "            if self.last_observations.size < self.last_observations.max_size:\n",
    "                obs = img * 0\n",
    "            else:\n",
    "                obs = self.last_observations.get_first()['img'].clone()\n",
    "            self.last_observations.add({'img': img})\n",
    "            img = th.cat([obs, img], dim=1)\n",
    "        return img\n",
    "\n",
    "    def _run_step(self, a):\n",
    "        \"\"\" Make a step in the environment (and update internal bookeeping) \"\"\"\n",
    "        ns, r, d, _ = self.env.step(a.item())\n",
    "        self.sum_rewards += r\n",
    "        if self.use_pixels: ns = self._pixel_observation()\n",
    "        return r, ns, d\n",
    "\n",
    "    def _next_step(self, done=True, next_state=None):\n",
    "        \"\"\" Switch to the next time-step (and update internal bookeeping) \"\"\"\n",
    "        self.time = 0 if done else self.time + 1\n",
    "        if done:\n",
    "            self.sum_rewards = 0\n",
    "            self.state = self.env.reset()\n",
    "            if self.use_pixels: self.state = self._pixel_observation(reset=True)\n",
    "        else:\n",
    "            self.state = next_state\n",
    "\n",
    "    def run(self, n_steps, transition_buffer=None, trim=True, return_dict=None):\n",
    "        \"\"\" Runs n_steps in the environment and stores them in the trainsition_buffer (newly created if None).\n",
    "            If n_steps <= 0, stops at the end of an episode and optionally trins the transition_buffer.\n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        my_transition_buffer = TransitionBatch(n_steps if n_steps > 0 else self.epi_len, self.transition_format())\n",
    "        time, episode_start, episode_lengths, episode_rewards = 0, 0, [], []\n",
    "        max_steps = n_steps if n_steps > 0 else self.epi_len\n",
    "        for t in range(max_steps):\n",
    "            # One step in the envionment\n",
    "            a = self.controller.choose(self.state)\n",
    "            r, ns, d = self._run_step(a)\n",
    "            terminal = d and self.time < self.epi_len - 1\n",
    "            my_transition_buffer.add(self._wrap_transition(self.state, a, r, ns, terminal))\n",
    "            if t == self.epi_len - 1: d = True\n",
    "            # Compute discounted returns if episode has ended or max_steps has been reached\n",
    "            if d or t == (max_steps - 1):\n",
    "                my_transition_buffer['returns'][t] = my_transition_buffer['rewards'][t]\n",
    "                for i in range(t - 1, episode_start - 1, -1):\n",
    "                    my_transition_buffer['returns'][i] = my_transition_buffer['rewards'][i] \\\n",
    "                                                         + self.gamma * my_transition_buffer['returns'][i + 1]\n",
    "                episode_start = t + 1\n",
    "            # Remember statistics and advance (potentially initilaizing a new episode)\n",
    "            if d:\n",
    "                episode_lengths.append(self.time + 1)\n",
    "                episode_rewards.append(self.sum_rewards)\n",
    "            self._next_step(done=d, next_state=ns)\n",
    "            time += 1\n",
    "            # If n_steps <= 0, we return after one episode (trimmed if specified)\n",
    "            if d and n_steps <= 0:\n",
    "                my_transition_buffer.trim()\n",
    "                break\n",
    "        # Add the sampled transitions to the given transition buffer\n",
    "        transition_buffer = my_transition_buffer if transition_buffer is None \\\n",
    "            else transition_buffer.add(my_transition_buffer)\n",
    "        if trim: transition_buffer.trim()\n",
    "        # Return statistics (mean reward, mean length and environment steps)\n",
    "        if return_dict is None: return_dict = {}\n",
    "        return_dict.update({'buffer': transition_buffer,\n",
    "                            'episode_reward': None if len(episode_rewards) == 0 else np.mean(episode_rewards),\n",
    "                            'episode_length': None if len(episode_lengths) == 0 else np.mean(episode_lengths),\n",
    "                            'env_steps': time})\n",
    "        return return_dict\n",
    "\n",
    "    def run_episode(self, transition_buffer=None, trim=True, return_dict=None):\n",
    "        \"\"\" Runs one episode in the environemnt.\n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        return self.run(0, transition_buffer, trim, return_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "outputs": [],
   "source": [
    "class QController:\n",
    "    \"\"\" Controller for Q-value functions, synchronizes the model calls. \"\"\"\n",
    "\n",
    "    def __init__(self, model, num_actions=None, params={}):\n",
    "        self.lock = threading.Lock()\n",
    "        self.num_actions = model[-1].out_features if num_actions is None else num_actions\n",
    "\n",
    "        self.model =model\n",
    "        self.model_actor = model[0]\n",
    "        self.model_critic = model[1]\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\" Shallow copy of this controller that does not copy the model. \"\"\"\n",
    "        return QController(model=self.model, num_actions=self.num_actions)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\" Returns a generator of the underlying model parameters. \"\"\"\n",
    "\n",
    "        return self.model[0].parameters(), self.model[1].parameters(), self.model[2].parameters()\n",
    "\n",
    "    def sanitize_inputs(self, observation, **kwargs):\n",
    "        \"\"\" Casts numpy arrays as Tensors. \"\"\"\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = th.Tensor(observation).unsqueeze(dim=0)\n",
    "        return observation\n",
    "\n",
    "    def choose(self, observation, **kwargs):\n",
    "        \"\"\" Returns the greedy actions the agent would choose when facing an \"observation\". \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            mx = self.model[0](self.sanitize_inputs(observation))\n",
    "            if mx.shape[-1] > self.num_actions: mx = mx[:, :self.num_actions]\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return th.max(mx, dim=-1)[1]\n",
    "\n",
    "    def probabilities(self, observation, **kwargs):\n",
    "        \"\"\" Returns the probabilities with which the agent would choose actions (here one-hot because greedy). \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            mx = self.model[0](self.sanitize_inputs(observation))\n",
    "            if mx.shape[-1] > self.num_actions: mx = mx[:, :self.num_actions]\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return th.zeros(*mx.shape).scatter_(dim=-1, index=th.max(mx, dim=-1)[1].unsqueeze(dim=-1), src=th.ones(1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "outputs": [],
   "source": [
    "class ACController(QController):\n",
    "    \"\"\" A controller that interprets the first num_actions model outputs as logits of a softmax distribution. \"\"\"\n",
    "\n",
    "    def probabilities(self, observation, precomputed=False, **kwargs):\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            mx = observation if precomputed else self.model[0](self.sanitize_inputs(observation))[:, :self.num_actions]\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return th.nn.functional.softmax(mx, dim=-1)\n",
    "\n",
    "    def choose(self, observation, **kwargs):\n",
    "        return th.distributions.Categorical(probs=self.probabilities(observation, **kwargs)).sample()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "outputs": [],
   "source": [
    "class EpsilonGreedyController:\n",
    "    \"\"\" A wrapper that makes any controller into an epsilon-greedy controller.\n",
    "        Keeps track of training-steps to decay exploration automatically. \"\"\"\n",
    "\n",
    "    def __init__(self, controller, params={}, exploration_step=1):\n",
    "        self.controller = controller\n",
    "        self.num_actions = controller.num_actions\n",
    "        self.max_eps = params.get('epsilon_start', 1.0)\n",
    "        self.min_eps = params.get('epsilon_finish', 0.05)\n",
    "        self.anneal_time = int(params.get('epsilon_anneal_time', 10000) / exploration_step)\n",
    "        self.num_decisions = 0\n",
    "\n",
    "    def epsilon(self):\n",
    "        \"\"\" Returns current epsilon. \"\"\"\n",
    "        return max(1 - self.num_decisions / (self.anneal_time - 1), 0) \\\n",
    "               * (self.max_eps - self.min_eps) + self.min_eps\n",
    "\n",
    "    def choose(self, observation, increase_counter=True, **kwargs):\n",
    "        \"\"\" Returns the (possibly random) actions the agent takes when faced with \"observation\".\n",
    "            Decays epsilon only when increase_counter=True\". \"\"\"\n",
    "        eps = self.epsilon()\n",
    "        if increase_counter: self.num_decisions += 1\n",
    "        if np.random.rand() < eps:\n",
    "            return th.randint(self.controller.num_actions, (1,), dtype=th.long)\n",
    "        else:\n",
    "            return self.controller.choose(observation, **kwargs)\n",
    "\n",
    "    def probabilities(self, observation, **kwargs):\n",
    "        \"\"\" Returns the probabilities with which the agent would choose actions. \"\"\"\n",
    "        eps = self.epsilon()\n",
    "        return eps * th.ones(1, 1) / self.num_actions + \\\n",
    "               (1 - eps) * self.controller.probabilities(observation, **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "outputs": [],
   "source": [
    "class BatchReinforceLearner:\n",
    "    \"\"\" A learner that performs a version of REINFORCE. \"\"\"\n",
    "\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        self.learner = None\n",
    "        self.model_actor = model[0]\n",
    "        self.model_critic = model[1]\n",
    "        self.model_w = model[2]\n",
    "\n",
    "        self.controller = controller\n",
    "        self.value_loss_param = params.get('value_loss_param', 1)\n",
    "        self.offpolicy_iterations = params.get('offpolicy_iterations', 10)\n",
    "\n",
    "        self.all_parameters_actor = list(model_actor.parameters())\n",
    "        self.optimizer_actor = th.optim.Adam(self.all_parameters_actor, lr=params.get('lr', 1E-3))\n",
    "\n",
    "        self.all_parameters_critic = list(model_critic.parameters())\n",
    "        self.optimizer_critic = th.optim.Adam(self.all_parameters_critic, lr=params.get('lr', 1E-3))\n",
    "\n",
    "        self.gamma = params.get('gamma')\n",
    "\n",
    "        self.grad_norm_clip = params.get('grad_norm_clip', 10)\n",
    "        self.compute_next_val = False  # whether the next state's value is computed\n",
    "        self.opposd = params.get('opposd', False)\n",
    "        self.num_actions = params.get('num_actions', 5)\n",
    "        self.old_pi = th.ones(1, 1) / self.num_actions\n",
    "        self.pi_0 = None\n",
    "\n",
    "    def set_controller(self, controller):\n",
    "        \"\"\" This function is called in the experiment to set the controller. \"\"\"\n",
    "        self.controller = controller\n",
    "\n",
    "    def _advantages(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the advantages, Q-values or returns for the policy loss. \"\"\"\n",
    "        return batch['returns']\n",
    "\n",
    "    def _value_loss(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the value loss (if there is one). \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def _policy_loss(self, pi, advantages):\n",
    "        \"\"\" Computes the policy loss. \"\"\"\n",
    "        return -(advantages.detach() * pi.log()).mean()\n",
    "\n",
    "    def update_policy_distribution(self, batch, ratios):\n",
    "        pass\n",
    "\n",
    "    def train(self, batch):\n",
    "        assert self.controller is not None, \"Before train() is called, a controller must be specified. \"\n",
    "\n",
    "        # model = [model_actor, model_critic, model_w]\n",
    "        loss_sum = 0.0\n",
    "        for _ in range(1 + self.offpolicy_iterations):\n",
    "# HERE START\n",
    "            if self.opposd:\n",
    "                for _ in range(50):\n",
    "                    # batch_w = self.runner.run(self.batch_size, transition_buffer)\n",
    "                    batch_w = batch.sample(200)\n",
    "                    self.pi_0 = self.old_pi + 0 * batch_w['returns']\n",
    "                    # Compute the model-output for given batch\n",
    "                    pi = th.nn.functional.softmax(self.model_actor(batch_w['states']), dim=-1).gather(dim=-1, index=batch_w['actions'])\n",
    "                    self.update_policy_distribution(batch_w, pi.detach()/self.pi_0)\n",
    "\n",
    "            for _ in range(10):\n",
    "                batch_c = batch.sample(int(5e3))\n",
    "\n",
    "                val = self.model_critic(batch_c['states'])\n",
    "                next_val = self.model_critic(batch_c['next_states'])\n",
    "                pi = th.nn.functional.softmax(self.model_actor(batch_c['states']), dim=-1).gather(dim=-1, index=batch_c['actions'])\n",
    "                pi.detach()\n",
    "                self.pi_0 = self.old_pi + 0 * batch_c['returns']\n",
    "\n",
    "                # value_loss = self._value_loss(batch_c, val, next_val)\n",
    "\n",
    "                targets = batch_c['returns']\n",
    "\n",
    "                # loss_fn = th.nn.MSELoss()\n",
    "                # value_loss = loss_fn(val, targets)\n",
    "\n",
    "                value_loss = th.mean((pi/self.pi_0) * (targets - val)**2)\n",
    "\n",
    "                self.optimizer_critic.zero_grad()\n",
    "                value_loss.backward()\n",
    "                th.nn.utils.clip_grad_norm_(self.all_parameters_critic, self.grad_norm_clip)\n",
    "                self.optimizer_critic.step()\n",
    "\n",
    "            batch_a = batch.sample(int(5e3))\n",
    "\n",
    "            pi = th.nn.functional.softmax(self.model_actor(batch_a['states']), dim=-1).gather(dim=-1, index=batch_a['actions'])\n",
    "\n",
    "            val = self.model_critic(batch_a['states'])\n",
    "            next_val = self.model_critic(batch_a['next_states'])\n",
    "            self.pi_0 = self.old_pi + 0 * batch_a['returns']\n",
    "\n",
    "            Q = self._advantages(batch_a, val, next_val)\n",
    "            ratios = pi / self.pi_0.detach()\n",
    "            if self.opposd:\n",
    "                w = self.model_w(batch_a['states']).detach()\n",
    "                w /= th.mean(w)\n",
    "                ratios = w * ratios\n",
    "            policy_loss = -(Q.detach() * ratios).mean()\n",
    "\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            th.nn.utils.clip_grad_norm_(self.all_parameters_actor, self.grad_norm_clip)\n",
    "            self.optimizer_actor.step()\n",
    "\n",
    "            # Combine policy and value loss\n",
    "            loss = policy_loss.detach().item()\n",
    "\n",
    "            loss_sum += loss\n",
    "        return loss_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\" Abstract class of an experiment. Contains logging and plotting functionality.\"\"\"\n",
    "\n",
    "    def __init__(self, params, model, **kwargs):\n",
    "        self.params = params\n",
    "        self.plot_frequency = params.get('plot_frequency', 10)\n",
    "        self.plot_train_samples = params.get('plot_train_samples', True)\n",
    "        self.print_when_plot = params.get('print_when_plot', False)\n",
    "        self.print_dots = params.get('print_dots', False)\n",
    "        self.episode_returns = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_losses = []\n",
    "        self.env_steps = []\n",
    "        self.total_run_time = 0.0\n",
    "\n",
    "    def plot_training(self, update=False):\n",
    "        \"\"\" Plots logged training results. Use \"update=True\" if the plot is continuously updated\n",
    "            or use \"update=False\" if this is the final call (otherwise there will be double plotting). \"\"\"\n",
    "        # Smooth curves\n",
    "        window = max(int(len(self.episode_returns) / 3), 1)\n",
    "        returns = np.convolve(self.episode_returns, np.ones(window) / window, 'valid')\n",
    "        # Determine x-axis based on samples or episodes\n",
    "        x_returns = [(i + window) for i in range(len(returns))]\n",
    "        # Create plot\n",
    "        colors = ['b', 'g', 'r']\n",
    "        # fig.set_size_inches(16, 4)\n",
    "        plt.clf()\n",
    "        # Plot the losses in the left subplot\n",
    "        # pl.subplot(1, 3, 1)\n",
    "        plt.plot(x_returns, returns, colors[0])\n",
    "        plt.xlabel('Environment steps' if self.plot_train_samples else 'Policy gradient steps')\n",
    "        plt.ylabel('Episode return')\n",
    "        display.clear_output(wait=True)\n",
    "        if update:\n",
    "            display.display(pl.gcf())\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Frees all allocated runtime ressources, but allows to continue the experiment later.\n",
    "            Calling the run() method after close must be able to pick up the experiment where it was. \"\"\"\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Starts (or continues) the experiment. \"\"\"\n",
    "        assert False, \"You need to extend the Expeirment class and override the method run(). \""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "outputs": [],
   "source": [
    "class BatchActorCriticExperiment(Experiment):\n",
    "    def __init__(self, params, model, learner=None, **kwargs):\n",
    "        super().__init__(params, model, **kwargs)\n",
    "        self.models = model\n",
    "        self.max_episodes = params.get('max_episodes', int(1E6))\n",
    "        self.max_batch_episodes = params.get('max_batch_episodes', int(1E6))\n",
    "        self.max_steps = params.get('max_steps', int(1E9))\n",
    "        self.grad_repeats = params.get('grad_repeats', 1)\n",
    "        self.batch_size = params.get('batch_size', 1e5)\n",
    "        self.mini_batch_size = params.get('mini_batch_size', 200)\n",
    "        self.controller = ACController(model, num_actions=gym.make(params['env']).action_space.n, params=params)\n",
    "        # self.controller = EpsilonGreedyController(controller=self.controller, params=params)\n",
    "        self.runner = Runner(self.controller, params=params)\n",
    "        self.learner = BatchReinforceLearner(model, params=params) if learner is None else learner\n",
    "        self.learner.set_controller(self.controller)\n",
    "        self.opposd = params.get('opposd', False)\n",
    "        self.opposd_iterations = params.get('opposd_iterations', 50)\n",
    "\n",
    "    def get_transition_batch(self):\n",
    "        transition_buffer = TransitionBatch(self.batch_size, self.runner.transition_format(), self.mini_batch_size)\n",
    "        batch = self.runner.run(self.batch_size, transition_buffer)\n",
    "        return batch\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Overrides Experiment.close() \"\"\"\n",
    "        self.runner.close()\n",
    "\n",
    "    def run(self, batch=None):\n",
    "        \"\"\" Overrides Experiment.run() \"\"\"\n",
    "        # Plot past results if available\n",
    "        if self.plot_frequency is not None and len(self.episode_losses) > 2:\n",
    "            self.plot_training(update=True)\n",
    "        # Run the experiment\n",
    "        if not batch:\n",
    "            batch = self.get_transition_batch()\n",
    "        for e in range(self.max_batch_episodes):\n",
    "            # Make a gradient update step\n",
    "            self.learner.train(batch['buffer'])\n",
    "\n",
    "            for _ in range(10):\n",
    "                partial_result = self.runner.run_episode()\n",
    "                if partial_result['episode_length'] is not None:\n",
    "                    self.episode_lengths.append(partial_result['episode_length'])\n",
    "                    self.episode_returns.append(partial_result['episode_reward'])\n",
    "\n",
    "            if self.plot_frequency is not None and (e + 1) % self.plot_frequency == 0:\n",
    "                self.plot_training(update=True)\n",
    "                if self.print_when_plot:\n",
    "                    print('Batch %u, epi-return %.4g +- %.3g, length %u' %\n",
    "                          (len(self.episode_returns), np.mean(self.episode_returns[-10:]),\n",
    "                           np.std(self.episode_returns[-10:]), np.mean(self.episode_lengths[-10:])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "outputs": [],
   "source": [
    "class BiasedReinforceLearner(BatchReinforceLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "        self.value_criterion = th.nn.MSELoss()\n",
    "        self.advantage_bias = params.get('advantage_bias', True)\n",
    "        self.value_targets = params.get('value_targets', 'returns')\n",
    "        self.gamma = params.get('gamma')\n",
    "        self.compute_next_val = (self.value_targets == 'td')\n",
    "\n",
    "    def _advantages(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the advantages, Q-values or returns for the policy loss. \"\"\"\n",
    "        advantages = batch['returns']\n",
    "        if self.advantage_bias:\n",
    "            advantages -= values\n",
    "        return advantages\n",
    "\n",
    "    def _value_loss(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the value loss (if there is one). \"\"\"\n",
    "        targets = None\n",
    "        if self.value_targets == 'returns':\n",
    "            targets = batch['returns']\n",
    "        elif self.value_targets == 'td':\n",
    "            targets = batch['rewards'] + self.gamma * (~batch['dones'] * next_values)\n",
    "        return self.value_criterion(values, targets.detach())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "outputs": [],
   "source": [
    "class ActorCriticLearner(BiasedReinforceLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "        self.advantage_bootstrap = params.get('advantage_bootstrap', True)\n",
    "        self.compute_next_val = self.compute_next_val or self.advantage_bootstrap\n",
    "\n",
    "    def _advantages(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the advantages, Q-values or returns for the policy loss. \"\"\"\n",
    "        advantages = None\n",
    "        if self.advantage_bootstrap:\n",
    "            advantages = batch['rewards'] + self.gamma * (~batch['dones'] * next_values)\n",
    "        else:\n",
    "            advantages = batch['returns']\n",
    "        if self.advantage_bias:\n",
    "            advantages = advantages - values\n",
    "        return advantages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "outputs": [],
   "source": [
    "class OffpolicyActorCriticLearner(ActorCriticLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "\n",
    "    def _policy_loss(self, pi, advantages):\n",
    "        \"\"\" Computes the policy loss. \"\"\"\n",
    "        if self.old_pi is None:\n",
    "            self.old_pi = pi  # remember on-policy probabilities for off-policy losses\n",
    "            # Return the defaul on-policy loss\n",
    "            return super()._policy_loss(pi, advantages)\n",
    "        else:\n",
    "            # The loss for off-policy data\n",
    "            ratios = pi / self.pi_0.detach()\n",
    "            return -(advantages.detach() * ratios).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "outputs": [],
   "source": [
    "class PPOLearner(OffpolicyActorCriticLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "        self.ppo_clipping = params.get('ppo_clipping', False)\n",
    "        self.ppo_clip_eps = params.get('ppo_clip_eps', 0.2)\n",
    "\n",
    "    def _policy_loss(self, pi, advantages):\n",
    "        \"\"\" Computes the policy loss. \"\"\"\n",
    "        if self.old_pi is None:\n",
    "            # The loss for on-policy data does not change\n",
    "            return super()._policy_loss(pi, advantages)\n",
    "        else:\n",
    "            # The loss for off-policy data\n",
    "            ratios = pi / self.pi_0.detach()\n",
    "            loss = advantages.detach() * ratios\n",
    "            if self.ppo_clipping:\n",
    "                # off-policy loss with PPO clipping\n",
    "                ppo_loss = th.clamp(ratios, 1 - self.ppo_clip_eps, 1 + self.ppo_clip_eps) * advantages.detach()\n",
    "                loss = th.min(loss, ppo_loss)\n",
    "            return -loss.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "outputs": [],
   "source": [
    "class OPPOSDLearner(OffpolicyActorCriticLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "        self.num_actions = params.get('num_actions', 5)\n",
    "        self.batch_size = params.get('batch_size')\n",
    "        self.states_shape = params.get('states_shape')\n",
    "        self.w_grad_norm_clip = params.get('grad_norm_clip', 10)\n",
    "        self.parameters_w = list(self.model_w.parameters())\n",
    "        self.optimizer_w = th.optim.Adam(self.parameters_w, lr=1E-3)\n",
    "\n",
    "    def _policy_loss(self, pi, advantages):\n",
    "        # The loss for off-policy data\n",
    "        loss = advantages.detach() * pi\n",
    "        return -loss.mean()\n",
    "\n",
    "    def reset_w_net(self):\n",
    "        pass\n",
    "        # self.w_model = th.nn.Sequential(th.nn.Linear(self.states_shape, 128), th.nn.ReLU(),\n",
    "        #                                 th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "        #                                 th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "        #                                 th.nn.Linear(128, 1))\n",
    "        # self.w_parameters = list(self.w_model.parameters())\n",
    "        # self.w_optimizer = th.optim.Adam(self.w_parameters, lr=params.get('lr', 5E-4))\n",
    "\n",
    "    def update_policy_distribution(self, batch, ratios):\n",
    "        self.model_w.train(True)\n",
    "        batch_size = batch.size\n",
    "\n",
    "        next_states = batch['next_states']\n",
    "        with th.autograd.set_detect_anomaly(True):\n",
    "            w = self.model_w(batch['states'])\n",
    "            w_ = self.model_w(batch['next_states'])\n",
    "\n",
    "            w = w / th.mean(w)\n",
    "            w_ = w_ / th.mean(w_)\n",
    "\n",
    "            d = w * ratios - w_\n",
    "\n",
    "            k = th.zeros(batch_size, batch_size, self.states_shape)\n",
    "            for i in range(self.states_shape):\n",
    "                k[:, :, i] = next_states[:, i].view(1, -1) - next_states[:, i].view(-1, 1)\n",
    "\n",
    "            k = th.exp(-th.linalg.norm(k, dim=-1)/2)\n",
    "            prod = th.matmul(d, d.transpose(0, 1))\n",
    "\n",
    "            # n_lm = 3\n",
    "            # y = th.randn(n_lm, 4)\n",
    "            # dist_gt = th.zeros(n_lm, n_lm, 4)\n",
    "            # dist_gt[:,:,0] = y[:,0].view(1,-1) - y[:,0].view(-1,1)\n",
    "            # dist_gt[:,:,1] = y[:,1].view(1,-1) - y[:,1].view(-1,1)\n",
    "            # dist_gt[:,:,2] = y[:,2].view(1,-1) - y[:,2].view(-1,1)\n",
    "            # dist_gt[:,:,3] = y[:,3].view(1,-1) - y[:,3].view(-1,1)\n",
    "            # th.linalg.norm(dist_gt, dim=-1)\n",
    "            # k = (th.linalg.norm(dist_gt, dim=-1)<1).float()\n",
    "\n",
    "            D = th.sum(prod * k) / batch_size\n",
    "\n",
    "            self.optimizer_w.zero_grad()\n",
    "            D.backward()\n",
    "            th.nn.utils.clip_grad_norm_(self.parameters_w, self.w_grad_norm_clip)\n",
    "            self.optimizer_w.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "outputs": [],
   "source": [
    "experiments = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "params = default_params()\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "params['states_shape'] = state_dim\n",
    "params['num_actions'] = n_actions\n",
    "params['batch_size'] = int(1e5)\n",
    "params['mini_batch_size'] = 200\n",
    "\n",
    "# The model has n_action policy heads and one value head\n",
    "# model_actor = th.nn.Sequential(th.nn.Linear(state_dim, 32), th.nn.ReLU(),\n",
    "#                          th.nn.Linear(32, n_actions))\n",
    "model_actor = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions + 1))\n",
    "model_critic = th.nn.Sequential(th.nn.Linear(state_dim, 32), th.nn.ReLU(),\n",
    "                         th.nn.Linear(32, 1))\n",
    "model_w = th.nn.Sequential(th.nn.Linear(state_dim, 32), th.nn.ReLU(),\n",
    "                         th.nn.Linear(32, 1), th.nn.Softplus())\n",
    "model = [model_actor, model_critic, model_w]\n",
    "# experiment = BatchActorCriticExperiment(params, model, learner=OPPOSDLearner(model, params=params))\n",
    "# experiments_batch = experiment.get_transition_batch()\n",
    "# dbfile = open('cartpolePickle', 'ab')\n",
    "# pickle.dump(experiments_batch, dbfile)\n",
    "# dbfile.close()\n",
    "\n",
    "dbfile = open('cartpolePickle', 'rb')\n",
    "experiments_batch = pickle.load(dbfile)\n",
    "dbfile.close()\n",
    "heuristic = True\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAArrElEQVR4nO3debRU5Znv8e/DIIiiKByiIsgsVESMEhM1qNFWo3FgqNjJStLXdK6GXt4EOzcdu1eG7pvE7jZjJ51OGzsmJnZMJ9YRRBzjEI1jhIgCBYIyCGrkiCCCA4PP/ePdlVMcqs7Z55zatWv4fdaqVcPetfdTxWE/td/33c9r7o6IiDSvPmkHICIi6VIiEBFpckoEIiJNTolARKTJKRGIiDS5fmkH0F3Dhg3z0aNHpx2GiEhdWbx48Svu3lJqWd0lgtGjR7No0aK0wxARqStmtr7cMjUNiYg0OSUCEZEmp0QgItLklAhERJqcEoGISJNTIhARaXJKBCIiTU6JQCruySfhvvvSjkLKufFG2LQp7SikligRSMVdeinMng07d6YdiXS0cSN8/ONw1VVpRyK1RIlAKmrdOli8GLZuhfvvTzsa6SifD/c33wzvvJNuLFI7lAikolpbw/3AgZDLpRuL7KuQCDZuhD/8Id1YpHYoEUhF5XJw/PEwaxbMmwe7d6cdkRTL5+Hgg2G//ZSopZ0SgVTMhg3w2GOQzYY+gs2b4YEH0o5KiuXzMHUqnHVWSASaslxAiUAq6Oabw/3s2fChD8GgQfrVWUvcQyLIZEKyXr8+9OeIKBFIxeRycOyxMHFiSAIf/nBIDnv2pB2ZALz8MmzZEhLBhRdCv35K1BIkmgjMbK6ZLTOz5WZ2RYnlp5vZa2a2JLp9Ncl4JDkvvggPPxx+aRZks2G8+sMPpxeXtCt0FGcycOihcOaZoXNfzUOSWCIws2OAS4ETganA+WY2vsSqv3f346Lb15KKR5I1b144oBQngvPO0+ihWlKcCCD8Wz37LDz9dHoxSW1I8oxgMvC4u7/h7ruBB4BZCe5PUpTLhQPM5Mntrx14IJx7bvjVqTHr6cvnYcgQOOyw8Pyii6BPHyVqSTYRLAOmm9lQMxsEnAeMLLHeSWb2lJndYWbvLrUhM7vMzBaZ2aK2trYEQ5ae2LQJHnwwdBJ3lM2GZqPHHqt+XLK3fD4karPwvKUFTj8dbrpJzUPNLrFE4O4rgKuBu4E7gSVAx27DPwJHuftU4N+B+WW2da27T3P3aS0tJedelhTNnx9+8Rc3CxWcf77GrNeKwoihYtksPPNMe7ORNKdEO4vd/Tp3P8HdTwW2AKs6LN/m7tujx7cD/c1sWJIxSeXlcjBhAkyZsu+ygw6Cc85Rp2Ta2trCrWMimDkznCEoUTe3pEcNDY/uRxH6B27ssPwws3CiamYnRvFsTjImqazNm0Ol0Wy2vcmho9mz4fnnYdGi6sYm7VasCPcdE8Fhh8H06e2lQaQ5JX0dQauZ5YFbgcvdfauZzTGzOdHyLLDMzJ4CfgB81F2/G+vJLbeE6wRKNQsVaMx6+jqOGCqWzcLSpaGJSJpT0k1D09094+5T3f3e6LVr3P2a6PEP3f3d0fL3u/sjScYjlZfLwZgx8J73lF/nkEPgL/5CJQ3SlM+HUVwjSwzXmBWN5dNZQfPSlcXSY1u2wD33hKafcs1CBdksrFkDS5ZUJTTpoOOIoWIjRsBJJ+mMrZkpEUiP3Xor7NrVebNQwUUXQd++OtikZcWK0s1CBdlsmFnuueeqF5PUDiUC6bFcLjQ1nHhi1+sOGwYf/KDGrKdh69ZwLUdniaBwDYiah5pTv7QDkJ554AFYsAC+/e2um2UKbr8dvvvdyh2If/97uPzy+PvPZmHOnHARU78Sf3kzZsBnP1uZ2NJ25ZXlR0mNGAE//Wnp7yAJ5UYMFTvqKHjve8Pf01137bu8Xz/4znfgmGOSiVHSpTOCOnXVVeGgvnx5/Pd84xvh9H/nzsrcpk+Hz3wm/v4vvjjUH3rnnX23lc/D977X/e+hFq1fD9/8Jrzwwr6f89VX4YYbqjtPQ2cjhop96UswaVLpf+sHHoD/+I/kY5WUuHtd3U444QRvdq+84t63rzu4/+M/xnvPhg1h/auuSjS0Hvva19zN3LdvTzuS3vvud8N3vWrVvst27HAfNMh9zpzqxfP5z7vvv7/77t0938bFF7sPH967bUi6gEVe5riqM4I6VBi7P2JE/M7XwqQxcTp205DJhCarRhjLnsuFWcAmTNh3WRrzNOTz4Zd+374930ahpPhDD1UuLqkdSgR1KJeD0aNDO/Ty5e1twF29Z8qUMGlMLSo0W9R7zZsXXoBHHuk84Vb7oFqqxlB3nXsu7L+/Rn01KiWCOrN1axi7X5gXGLoe6fHSS+GgU6tnAwDjx4cOyXpPBHHOvKo5T8Prr4fyHr1NBCop3tiUCOpM8dj9I46AU07pOhGUmjSm1vTvH85W6j0RtLbCu98dmmLKKRxUb745+YPqypXhvreJAMLfz0svwaOP9n5bUluUCOpMx7H7s2eHq3Wffbbz90yeXJmDQZIymfpOBC+/HOZliJNwqzVPQ9wRQ3F8+MMwYICuNWhESgR1ZNu2MMa7uKRDV81DbW1h6F+pSWNqTSYTrmx96620I+mZ7px5VWuehnw+7Gfs2N5vq1BSXDWjGo8SQR257TZ4++29D+qjRoWzg3IHlM4mjak1mUyIddWqrtetRbkcHH10aBrqykEHwdlnJ39QzedDk1ulLl6bPRs2bIAnnqjM9qQ2KBHUkVwODj8cTj5579ez2XAV67p1pd8zfjwce2xVQuyVeh451NYGv/td5/MydJTNJn9QrcSIoWIXXBD6czR6qLEoEdSJHTvgjjtCyeA+Hf7VyjUPbd4M997bvYNTmiZODJ+tHhNB4dqO7jTBJT1PwxtvwNq1lU0EKinemJQI6sQdd8Cbb5Zu4hk7Fo4/ft8DyoIFXU8aU0sGDAhnL/WYCFpbw7/DccfFf0/hoJrUNJ7PPBO2W+lBAtlsSDBPPlnZ7Up6lAjqRC4HLS2hvk8p2WwYgbJx497vGT06JIl6kcnEu0CulhTmZejJmVeS8zRUcsRQMZUUbzxKBHXgzTdh4cLQLFSuTEChSaJwQdPWrfDb38abNKaWTJ4cOot37Uo7kvgWLIDdu3t25pXkQTWfD9suVeqiN4YOhTPOUEnxRqJEUAfuuiv0EXR2oJk4MZSQKBxQujNpTC3JZMJBtbPrImpNLhdGb02b1v33JjlPQz4fksB++1V2uxD+rp59Nsx1LPVPiaAO5HLhV9hpp3W+XjYbSkm89FJ4z5FHxps0ppbU28ih116Du+/uXYd8NgurV8OyZZWNratZyXpjxozQsa+LyxqDEkGNe/vt0PQwY0YYtteZbDb8qvz5z9svPOs4wqjWTZoUDqj1kggWLgz1+ntz5jVjRvjMlWweevvt8Is9qUQwfDiceqr6CRpFnR0mms9vfxsKh8U50GQyoY39618PB4J6axaCUKZ59Oj6SQS5XCgH/r739Xwb73pX5Q+qq1eHEWNJlhXJZsO/U738W0l5SgQ1rrUVhgwJnXNxZLNh/HipC8/qRb3UHNq+He68szJnXoWDaqVGTCU1YqjYzJnhTEbNQ/VPiaDC5s+HAw4IY+Ircbv++jCyJG6HX+EsoNSFZ/Uikwlj4HfvTjuS4EMfKv1vc8ghoS5SJc68Zs0K95U6K3j66XCQTnL+iUL123nzktuHVIcmr6+w//qvUEfmkksqs70+feDTn46//pQpYWL0c86pzP7TkMmEpq21ays/9LG7Vq0K/S0XXFC6htDw4eFg2FvFJcW/8pXeb2/hQjjppDCZTJKmT4dvfSuMUOuqD0tqlxJBBRXG7s+dC//yL+nEYAaf+lQ6+66U4pFDaSeCQrPHD38YhogmKZuFv/3b0L7fm8+9ejU89RR897uVi62c4uG+kycnvz9JRp02HtSmeh27X2sKB5Ra6CdobQ0dwUknAWhvHuptm3vh/dUoPV5vw32lNCWCCmpt3XvSGOmZwYPD95j2wWXtWli8uHqJvauS4nG1tobtVCN51dtwXylNiaBCXn89jCCZNau+SjrUqloYOVTNX9YF2WxIPmvX9uz969aFkuTVSl71NtxXSlMiqJDCpDFqFqqMQvG5NCdKz+VCwb4xY6q3z441o7orjeRVC0lbekeJoELKTRojPTN5cii2t359Ovt//nl4/PHqJ/ZyJcXjKiSvSkxNGVdhuO+ePdXbp1SWEkEF7NgBt99e32P3a03anZCFX+RpzPVcKCm+YUP33rdxY3hftZNX8XBfqU86bFVAZ5PGSM+kPXKotTVM75nkBVnl9LR5KK3klXbSlt5TIqiAriaNke479FA47LB0Di4vvggPP5xeYp84MSSh7jYP5XLhgsJqJ6+0k7b0nhJBL735Zugonjmz/KQx0jNpzVY2b16o4prmGd7s2SEZvfhivPVfeimUIE8j5sGDQ8lzJYL6pUTQS3ffHYqPqVmo8gqjUao9C1YuF37lpnmlbKGkeNw6PmknL40cqm+JJgIzm2tmy8xsuZld0cl67zWz3WZWd4fTXC40Y5x+etqRNJ5MJlyf8cIL1dvnyy/Dgw+mn9gLJcXjXmVcSF5JVhvtTC0M95WeSywRmNkxwKXAicBU4HwzG19ivb7A1cDdScWSlO5MGiPdl0Yn5Pz54WCWdiKAEMMDD8CmTZ2v19YW1ksz5kwmlD9//vn0YpCeS/KMYDLwuLu/4e67gQeAWSXW+yzQCnTx51577rkHtm2rjYNGI0ojEbS2hoJvU6ZUb5/lZLMhKc2f3/l6tZC8NHKoviWZCJYB081sqJkNAs4DRhavYGYjgJnAfyYYR2JyOTj4YDjzzLQjaUwtLWFy92odXDZvhvvu6938w5U0ZUpISl2NHsrl0k9eGjlU3xJLBO6+gvYmnzuBJUDHaw//DbjS3TttWTSzy8xskZktamtrSyDa7tu5M/wSu/DC+JPGSPdVsxPyllvC1bG1coZnFmK5776QpErZvBnuvTf95JXmcF/pvUQ7i939Onc/wd1PBbYAqzqsMg34HzNbB2SBH5nZjBLbudbdp7n7tJaWliRDju3++8P8A7Vy0GhU1Rw5lMuFAmrveU/y+4pr9uyQnG65pfTyBQvC8jSugO5II4fqV6IT05jZcHffZGajCP0D7y9e7u5jita9Hljo7vOTjKlSWlvhwAPh7LPTjqSxTZ4MW7aEX71DhiS3n7feCn0+c+fWRrNQwfHHh+R0ww3hIrOObrghLD/++GpHtq9MBn7+85C0a+k7lK4lPUNZq5kNBXYBl7v7VjObA+Du1yS870QtXAjnnQcDB6YdSWM77rhwf9ZZ1dnfxRdXZz9xmYWYvvlNeO97S6/zxS/WxoG3eLjvkUemHY10R6KJwN33KbpQLgG4+yVJxlJJmzeHKzk1AU3ypk8PzXDbtye/r0MOKX+wTdNXvxquUylV3bNPHzjttKqHVFLxyCElgvqiOYt7oFD2IK2Ld5qJmS7WO+AAOPfctKPoWnEiUJNpfVGJiR4odIgpEYi0q/ZwX6kcJYIeyOfDr7SRI7teV6SZTJ6cTqFA6R0lgh7I58MfvCahEdlbJgPLl1e/UKD0TpeHMjObZWarzew1M9tmZq+b2bZqBFer8nk1C4mUksmE4b5d1UeS2hLnN+03gQvd/WB3P8jdB7v7QUkHVqteey0Mj1MiENmXag7VpziJ4OWoXISgEUMinVEiqE9xho8uMrNfA/OBtwsvuns3Z1RtDBoxJFLe4YeHQoxKBPUlTiI4CHgDKB4Z7EDTJoKBA8Nl/SKyNzPVHKpHnSaCaNKYze7+hSrFU/PyeZg0SfMTi5STycCtt6YdhXRHp30E7r4HOKVKsdSFwtBRESktkwmjhl55Je1IJK44ncVLzGyBmX0yGko6y8xKzTTW8LZvh/Xr1T8g0pnC/w9dWFY/4vQRDAQ2A2cUvdaUfQQrV4Z7JQKR8opHDk3fp+yk1KIuE4G7f6oagdQDjRgS6drIkWGuDnUY148uE4GZ/YxwBrAXd//rRCKqYStWQP/+MG5c2pGI1C6z0I+mRFA/4jQNLSx6PJAw2fyLyYRT2/J5mDgxJAMRKW/y5DDjm9SHOE1DrcXPzexXwEOJRVTD8vnams9WpFZlMvCLX4R5vZOcYlQqoyf1MycAwysdSK17801Ys0b9AyJxaORQfYlTffT1qOrotqjq6K3AlcmHVltWrYJ33lEiEIlDNYfqS5ymocHVCKTWacSQSHyjR4dSLDojqA9xzgjujfNao8vnQ1mJCRPSjkSk9vXtG0qx6IygPpQ9IzCzgcAgYJiZHQJYtOggYEQVYqsp+TyMHw8DBqQdiUh9yGTg4YfTjkLi6OyM4DPAYmAS8Mfo8WLgFuCHyYdWWzQrmUj3ZDKhJMv27WlHIl0pmwjc/fvuPgb4gruPKbpNdfemSgQ7d8Lq1UoEIt1R+P9SKM0itSvO8NGfmtmXzexaADObYGbnJxxXTVm9GvbsUSIQ6Q6NHKofsRIBsBM4OXr+AvCNxCKqQRoxJNJ948aFq/CVCGpfnEQwzt2/CewCcPc3aO84bgr5fKifcvTRaUciUj/69Qv/Z5QIal+cRLDTzPYnKjxnZuMomru4GaxYAWPHwv77px2JSH3RtJX1IU4i+EfgTmCkmf0SuBf4YqJR1RjNSibSM5lMKM3y5ptpRyKd6TQRmFkf4BBgFnAJ8Ctgmrv/LvHIasTu3fDMM+ofEOmJTAbcw/8hqV1dzVn8DvBFd9/s7re5+0J3b6qZSNesCcNHlQhEuk8jh+pDnKahe8zsC2Y20swOLdwSj6xGaMSQSM9NmBDKTSgR1LY4E9P8ZXR/edFrDoytfDi1p/AHPGlSunGI1KP99gulWZQIaluc6qNjqhFIrcrnYdQoGKwarCI9opFDta8nE9M0FdUYEumdTAaefRbebqpB5/VFiaATe/aEawiUCER6LpMJ/5dWr047EilHiaAT69fDW28pEYj0hkYO1b44E9OYmX3CzL4aPR9lZifG2biZzTWzZWa23MyuKLH8IjN72syWmNkiM/tAtz9BTA89BOecA6+/Hv89GjEk0ntHHx1KtGi2stoV54zgR8BJwMei568D/9HVm8zsGOBS4ERgKnC+mY3vsNq9wFR3Pw74a+An8cLumbvvhttui7/+vfeGiWimTEkuJpFGt//+oUSLzghqV5xE8D53vxx4C8DdtwD7xXjfZOBxd3/D3XcDDxCuUP4zd9/u7h49PYConlESTj4ZDj8ccrl467tDa2s4izjwwKSiEmkOGjlU2+Ikgl1m1pf2onMtwDsx3rcMmG5mQ81sEHAeMLLjSmY208xWArcRzgr2YWaXRU1Hi9ra2mLsel99+sDMmXD77bBjR9frP/EEbNgA2WyPdiciRTKZUGZi9+60I5FS4iSCHwDzgOFmdhXwEPDPXb3J3VcAVwN3E4rWLQH2lFhvnrtPAmYAXy+zrWvdfZq7T2tpaYkRcmnZbCh+dccdXa+by4Va6hdc0OPdiUgkk4Fdu+C559KORErpMhG4+y8J1Ub/BXgJmOHuN8XZuLtf5+4nuPupwBZgVSfrPgiMNbNhsSLvgenToaWl6+Yh97DOWWfBkCFJRSPSPDRyqLaVTQQd6gptIlQevRF4OW6tITMbHt2PIvQP3Nhh+Xgzs+jx8cAAYHNPPkgc/fqF5qGFCzsvi/vkk7B2LcyenVQkIs2lUKJFiaA2dXZGsBhYFN23EX7Nr44eL465/VYzywO3Ape7+1Yzm2Nmc6Lls4FlZraEMBLpL4s6jxORzYY+grvvLr9OLhcKZV10UZKRiDSPAw+Eo45SIqhVZWsNFWoMmdl/AfPc/fbo+bmE9vwuufv0Eq9dU/T4akI/QtWcfjocemg42Jc60LvDTTfBGWfA0KHVjEyksWnkUO2K01n8/kISAHD3O2ifyL7u9O8fEsCCBaVrnyxdGuqiaLSQSGVNngwrV4ZyE1Jb4iSCF83sy2Y2Orp9CXgx6cCSlM3Ctm1wzz37LsvlwlDTGTOqHpZIQ8tkQsmWdevSjkQ6ipMIPga0EIaQzgOG036VcV0680w4+ODSo4dyOTjtNBg+vPpxiTQyjRyqXXGGj77q7nOBU4Hp7j7X3V9NPrTkDBgAF14I8+eHaSgL8vlQD0XNQiKVN3lyuFciqD1xis5NMbMnCVcKLzezxVEdobqWzcLWrXD//e2vtbaG4lgzZ6YWlkjDGjIEjjhCiaAWxWka+jHweXc/yt2PAv4vcG2yYSXv7LPDkLbW1vbXcjk45ZRQk0hEKk8jh2pTnERwgLv/+Xezu/+OUCCurg0cCOefD/Pmhfonq1bB00+rWUgkSZlMaH59J061MqmaOIlgjZl9pWjU0JeBNUkHVg3ZLLzyCjz4YPuZwaxZnb9HRHoukwkXdG7YkHYkUixOIvhrwqihm6PbMMpUCa03554LgwaFJqFcDt7/fhi5T31UEakUjRyqTWWvLC6I5h/4HEBUjvoAd9+WdGDVMGgQnHce/PKX4bqCb3877YhEGltxIjj33HRjkXZxRg3daGYHmdkBwFIgb2Z/l3xo1VG4uAxUZE4kaUOHhmt0NG1lbYnTNJSJzgBmAHcAY4BPJhlUNZ13Xug4njYNRo9OOxqRxpfJwA03hNF5HW9jxoQyL0lasSJMnVlq/4cfDldXtfpZbeiyaQjob2b9CYngh+6+y8wSrRBaTYMHw09/qr4BkWr5ylfg178uvez668PtO99Jbv/33RfKzF9yCezXYdLdu+4K/YVXXpnc/mtRnETwY2Ad8BTwoJkdBTREH0HBx+q6YIZIfTnjjHAr5YUXwoH4298OF3cmIZ+Hgw4KPwA77mPuXLjuujC8tU+c9pIGEafExA/cfYS7n+fBeuCDVYhNRJpMNgvPPw+LFiW3j3w+NE+VSjTNOry17BmBmX3C3f/bzD5fZpXvJhSTiDSpCy8MMwm2tsJ735vMPvL5cDFpKcWjmo46Kpn916LOzggKVw8PLnMTEamoQw8N1YFzuTBJVKW98gps2tR+wO+oWa9z6GyGsh9H9/+veuGISLPLZuHSS+Gpp+C44yq77cKw1XKJoDC8tdkSQZzrCMaa2a1m1mZmm8zsFjMbW43gRKT5zJgR5gwvNV9IbxUO8OUSQWGZEsG+bgR+AxwOHAHcBPwqyaBEpHkNGxbmFr/ppso3D+XzcMABnQ8XLySCJJqmalWcRDDI3W9w993R7b+BgUkHJiLNa/bsUBF4+fLKbnfFijBBTmdDQzOZUG3gpZcqu+9aFicR3GFmfx9VHj3KzL4I3G5mh5rZoUkHKCLNZ+bMMLyz0s1DhaGjnWnGDuM4ieBi4DPA/cDvgL8BPgosBhIc7Ssizeqww2D69L0njuqt114LF6wpEewrTvXRMdUIRESkWDYLn/scrFwJkyb1fntdjRgqGD48DGNtpkRQ9owgagIqPP5Ih2X/nGRQIiKFSaIqdVYQZ8QQhCapZhs51FnT0EeLHv9Dh2UfSiAWEZE/GzECTj65cv0E+XyoNBynynAmEzqqm2XkUGeJwMo8LvVcRKTisllYsgSefbb328rnQxNT375dr5vJwKuvQltb7/dbDzpLBF7mcannIiIVV5gsqhLNQ3FGDBU0W4dxZ4lgqpltM7PXgWOjx4XnU6oUn4g0sVGj4MQTe58Itm+H9euVCMopmwjcva+7H+Tug929X/S48Lx/NYMUkeY1ezY88UQ4kPfUypXhfvLkeOsfcUSYtKrpE4GISC0oNA/99Kfw3HP73uJcARx3xFBBs40cUiIQkZo2bhwcfzx87Wswfvy+tyOOgEce6Xwb+Tz07x+2FVczJYI4U1WKiKTq17+GRx/d93V3mDMHfvWrMNS0nHweJk4MySCuTAZ+9jPYvDmUp25kSgQiUvMKv/5LWbAgdCZ///vli8nl8+GsojsKzUgrVsAHPtC999YbNQ2JSF3LZkM/QakzBoA334Q1a+L3DxQ008ghJQIRqWsf/jAMGFD+CuRnnglNSN1NBKNGwaBBSgS9ZmZzzWyZmS03sytKLP+4mT1tZkvN7BEzm5pkPCLSeAYPhnPOCYngnXf2Xd7dEUMFffqE4aaFYnWNLLFEYGbHAJcCJwJTgfPNrGMr31rgNHefAnwduDapeESkcWWzsHFjuN6goxUrQlmJCRO6v91mGTmU5BnBZOBxd3/D3XcDDwCzildw90fcfUv09DHgyATjEZEGdcEFYURQqSuQ8/nQ0TxgQPe3m8mEBLNtW+9jrGVJJoJlwHQzG2pmg4DzgE5mCuXTwB0JxiMiDWrIEDjrrNA81LFiaHdqDHVUPHKokSWWCNx9BXA1cDdwJ7AE2FNqXTP7ICERXFlm+WVmtsjMFrU1SzlAEemW2bNh7Vp48sn213buhNWre58IGr15KNHOYne/zt1PcPdTgS3Aqo7rmNmxwE+Ai9x9c5ntXOvu09x9WktLS5Ihi0iduuii0BdQPHpo9WrYs6fniWDMmNCkpETQC2Y2PLofRegfuLHD8lHAzcAn3X2fJCEiEtfQoXDGGXDTTe3NQz0dMVTQt2+Yw0CJoHdazSwP3Apc7u5bzWyOmc2Jln8VGAr8yMyWmNmihOMRkQaWzYZJbJYuDc/z+VBA7uije77NZhg5lHTT0HR3z7j7VHe/N3rtGne/Jnr8v939EHc/LrpNSzIeEWlsM2aE8f+F5qF8HsaOhf337/k2MxlYtw527KhEhLVJVxaLSMMYPhxOO23vRNDTZqGCwvsLcxo0IiUCEWkos2eH4Z5Ll4byEpVKBI3cPKREICINZebM0C9w9dWwa1fvE8G4ceFiNSUCEZE6ccQRcMopYY4CiD89ZTn9+4fyFEoEIiJ1JJttL0A3aVLvt9foI4c0MY2INJxZs+CKK0Ip6cGDe7+9TAZuvjnMbRB3BNKOHWHSnF27er//gmOO6f4EO3EoEYhIwxk5Es48EypViCCTCWcYq1bB1JjF8n/+c7j88srsv+DKK5UIRERiu+228lNXdldx8bm4iWDp0lAMb/HiysQAcPDBldtWMSUCEWlIPSk7Xc7EiSGpdKefIJ+Hd787XNBW69RZLCLShQEDwpwG3U0EvR26Wi1KBCIiMXRn5FBbG7zyihKBiEhDyWRCWeudO7tet7dVT6tNiUBEJIZMBnbvDtVNu6JEICLSgLpTcyifD9cvjBiRbEyVokQgIhLD0UeHGkZxE0EmE9avB0oEIiIxDBoUpq6Mmwh6W+OompQIRERimjy560Tw6qvwpz/VT/8AKBGIiMSWyYQ5DnbvLr/OihXt69YLJQIRkZgymTB8dM2a8uvU24ghUCIQEYktzsihfD5UKD3qqOrEVAlKBCIiMRU6gAvNP6WsWBHWq1TBu2qoo1BFRNI1eHAocd3VGUE9NQuBEoGISLd0VnNo2zbYsEGJQESkoWUyofmnMBVmsZUr29epJ0oEIiLdkMmEKSvXr993WT2OGAIlAhGRbuls5FA+H+YuGDOmujH1lhKBiEg3FEYOlUsERx8N/eps7kclAhGRbjjkEDjssPKJoN6ahUCJQESk20qNHNqxA9atUyIQEWkKhUTg3v7aM8+E50oEIiJNIJOB7dth48b21+p1xBAoEYiIdFupkUP5fOgkHj8+nZh6Q4lARKSbyiWCiROhf/90YuoNJQIRkW5qaYFhw/YuPlcoNlePlAhERHqgeOTQ22/Ds8/WZ/8AKBGIiPRI8cihVatC7SElAhGRJpLJwJYt8PLL9T1iCBJOBGY218yWmdlyM7uixPJJZvaomb1tZl9IMhYRkUoq7jDO58NENBMnphtTTyVWEcPMjgEuBU4EdgJ3mtlCd3+2aLVXgc8BM5KKQ0QkCcU1h/J5GDcOBg5MN6aeSvKMYDLwuLu/4e67gQeAWcUruPsmd38C2JVgHCIiFXf44XDwwe2JoF6bhSDZRLAMmG5mQ81sEHAeMLInGzKzy8xskZktamtrq2iQIiI9YRYO/k89FTqLlQhKcPcVwNXA3cCdwBJgTw+3da27T3P3aS0tLZULUkSkFzIZeOwx2L1biaAsd7/O3U9w91OBLcCqJPcnIlJNmUz7lJX1nAgSnT7BzIa7+yYzG0XoH3h/kvsTEammwsHfDCZNSjeW3kh6Hp1WMxtK6Ay+3N23mtkcAHe/xswOAxYBBwHvRENMM+6+LeG4RER6rZAIRo+GQYNSDaVXEk0E7j69xGvXFD3+E3BkkjGIiCRl5Eg48MD6bhaC5M8IREQalhl85zswYULakfSOEoGISC9cdlnaEfSeag2JiDQ5JQIRkSanRCAi0uSUCEREmpwSgYhIk1MiEBFpckoEIiJNTolARKTJmbunHUO3mFkbsD7tOKpsGPBK2kGkrNm/g2b//KDvAHr3HRzl7iXr+NddImhGZrbI3aelHUeamv07aPbPD/oOILnvQE1DIiJNTolARKTJKRHUh2vTDqAGNPt30OyfH/QdQELfgfoIRESanM4IRESanBKBiEiTUyKoIWY20Mz+YGZPmdlyM/t/0etjzOxxM3vWzH5tZvulHWvSzKyvmT1pZguj5031HZjZOjNbamZLzGxR9NqhZvZbM1sd3R+SdpxJMbMhZpYzs5VmtsLMTmqyz3909G9fuG0zsyuS+g6UCGrL28AZ7j4VOA74kJm9H7ga+J67jwe2AJ9OL8SqmQusKHrejN/BB939uKJx438P3OvuE4B7o+eN6vvAne4+CZhK+Ftoms/v7s9E//bHAScAbwDzSOg7UCKoIR5sj572j24OnAHkotd/DsyofnTVY2ZHAh8GfhI9N5rsOyjjIsJnhwb+DszsYOBU4DoAd9/p7ltpks9fwpnAc+6+noS+AyWCGhM1iSwBNgG/BZ4Dtrr77miVjcCIlMKrln8Dvgi8Ez0fSvN9Bw7cbWaLzawwK+673P2l6PGfgHelE1rixgBtwM+i5sGfmNkBNM/n7+ijwK+ix4l8B0oENcbd90Sng0cCJwKT0o2ouszsfGCTuy9OO5aUfcDdjwfOBS43s1OLF3oY992oY7/7AccD/+nu7wF20KEJpME//59FfWEXAjd1XFbJ70CJoEZFp8L3AycBQ8ysX7ToSOCFtOKqglOAC81sHfA/hCah79Nc3wHu/kJ0v4nQNnwi8LKZHQ4Q3W9KL8JEbQQ2uvvj0fMcITE0y+cvdi7wR3d/OXqeyHegRFBDzKzFzIZEj/cHziJ0kt0PZKPV/hdwSyoBVoG7/4O7H+nuowmnxPe5+8dpou/AzA4ws8GFx8DZwDJgAeGzQwN/B+7+J2CDmR0dvXQmkKdJPn8HH6O9WQgS+g50ZXENMbNjCR1AfQlJ+jfu/jUzG0v4dXwo8CTwCXd/O71Iq8PMTge+4O7nN9N3EH3WedHTfsCN7n6VmQ0FfgOMIpRiv9jdX00pzESZ2XGEwQL7AWuATxH9n6AJPj/8+UfA88BYd38tei2RvwElAhGRJqemIRGRJqdEICLS5JQIRESanBKBiEiTUyIQEWlySgSSOjPbE1VYXGZmN5nZoE7WvcTMfhg9nmNmf1W9SOMxs+3R/RFmlutq/U62c0Vn30WJ9WeYWaan+5PmpUQgteDNqNLiMcBOYE6cN7n7Ne7+i2RDC8ysb3ff4+4vunu26zXLugKInQgIBciUCKTblAik1vweGB/VXZ9vZk+b2WPRxXZ7MbN/MrMvRI/Hm9k90VwOfzSzcWb2CzObUbT+L83sog7b6GNmP4rq3v/WzG43s2y0bJ2ZXW1mfwQ+YmaXmtkT0T5aC7/Wo7kSHo3mD/hG0bZHm9my6HFfM/tW9P6nzewz0eunm9nvimrv/9KCzwFHAPeb2f0lPvu/mlk+2ta3zexkQk2ab0VnV+Oi251R4brfm9mk6L3Xm9k1ZrbIzFZF9Z0ws3dbmA9jSbTdCb34d5R64u666ZbqDdge3fcjXDL/N8C/A/8YvX4GsCR6fAnww+jxPxGuPAZ4HJgZPR5I+CV9GjA/eu1gYC3Qr8O+s8DthB9FhxHmOshGy9YBXyxad2jR428An40eLwD+Knp8edHnGQ0six5fBnw5ejwAWESosnk68BqhflIf4FFCwbnC/oeV+L6GAs/QfkHokOj++kLs0fN7gQnR4/cRynUU1rsz2t8EQm2fgdF3/vFonf2A/dP+29CtOrdCES+RNO0fld6GcEZwHeHAPhvA3e8zs6FmdlCpN0d1eUa4+7xo/beiRQ9Ev/Zbom21ensp64IPADe5+zvAn0r8+v510eNjol/8Q4ADgbui108pxArcQJhEp6OzgWMLZxuExDSB0BT2B3ffGH2WJYQE8lCpzxp5DXgLuM7CDG4LO65gZgcCJwM3mVnh5QFFq/wm+syrzWwNocrto8CXLMwHcbO7r+4kBmkgSgRSC970UHr7z4oOXr31C+AThAJ2n+rB+3cUPb4emOHuT5nZJYRf8wVd1WoxwhnEXXu9GOopFddM2kMX/y/dfbeZnUgoxpYF/g/hrKlYH8IcDseV28y+m/UbzexxwqRAt5vZZ9z9vs5ikcagPgKpVb8HPg5/Pli+4u7bSq3o7q8DGwv9AWY2oGi0zfWETlfcPV/i7Q8Ds6O+gnex98G9o8HAS2bWvxBb0TY+Gj3++D7vCu4C/iZ6L2Y2MSoq1pnXo33uJfq1f7C73w78LWEqx73Wj76rtWb2keg9ZmZTizbzkegzjwPGAs9YKHa3xt1/QGii26dfRhqTEoHUqn8CTjCzp4F/pb30bjmfBD4Xrf8Iob0fD3XcVwA/K/O+VkIbeR74b+CPhKaXUr5CaLJ6GFhZ9PpcwuQxSyk/c9pPon38MepA/jFdn5FfC9xZorlqMLAw+qwPAZ+PXv8f4O8szOo1jpCUPm1mTwHLCdMcFjwP/AG4A5gTNaddDCyLmqeOIZxNSRNQ9VFpaNGZwVLgeI9K+ZZY50B3326hxO8fgFM81MRvSGZ2PbDQ3Xt8jYM0FvURSMMys78gdDx/r1wSiCy0MCHQfsDXGzkJiJSiMwIRkSanPgIRkSanRCAi0uSUCEREmpwSgYhIk1MiEBFpcv8f+Q3AkJr7x7QAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "return_dict = {}\n",
    "params = default_params()\n",
    "params['offpolicy_iterations'] = 10\n",
    "params['plot_train_samples'] = False\n",
    "params['plot_frequency'] = 4\n",
    "params['max_batch_episodes'] = int(200)\n",
    "params['batch_size'] = int(1e5)\n",
    "params['mini_batch_size'] = 200\n",
    "params['opposd'] = True\n",
    "params['opposd_iterations'] = 50\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "params['states_shape'] = state_dim\n",
    "params['num_actions'] = n_actions\n",
    "\n",
    "experiment = BatchActorCriticExperiment(params, model, learner=OPPOSDLearner(model, params=params))\n",
    "try:\n",
    "    experiment.run(experiments_batch)\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()\n",
    "\n",
    "return_dict.update({'model' : 'OPPOSD',\n",
    "                            'experiment': experiment})\n",
    "experiments = np.append(experiments, return_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40, epi-return 14 +- 3.71, length 14\n"
     ]
    }
   ],
   "source": [
    "return_dict = {}\n",
    "params = default_params()\n",
    "params['offpolicy_iterations'] = 10\n",
    "params['plot_train_samples'] = False\n",
    "params['plot_frequency'] = 4\n",
    "params['max_batch_episodes'] = int(50)\n",
    "params['batch_size'] = int(1e5)\n",
    "params['mini_batch_size'] = 1000\n",
    "params['opposd'] = True\n",
    "params['opposd_iterations'] = 50\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "params['states_shape'] = state_dim\n",
    "params['num_actions'] = n_actions\n",
    "\n",
    "# The model has n_action policy heads and one value head\n",
    "model_actor = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "model_critic = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 1))\n",
    "# model_w = th.nn.Sequential(th.nn.Linear(state_dim, 32), th.nn.ReLU(),\n",
    "#                          th.nn.Linear(32, 1), th.nn.Softplus())\n",
    "model_w = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 1), th.nn.Softplus())\n",
    "model = [model_actor, model_critic, model_w]\n",
    "experiment = BatchActorCriticExperiment(params, model, learner=OPPOSDLearner(model, params=params))\n",
    "try:\n",
    "    experiment.run(experiments_batch)\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()\n",
    "\n",
    "return_dict.update({'model' : 'OPPOSD',\n",
    "                            'experiment': experiment})\n",
    "experiments = np.append(experiments, return_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "return_dict = {}\n",
    "params = default_params()\n",
    "params['offpolicy_iterations'] = 10\n",
    "params['plot_train_samples'] = False\n",
    "params['plot_frequency'] = 4\n",
    "params['max_batch_episodes'] = int(50)\n",
    "params['batch_size'] = int(1e5)\n",
    "params['mini_batch_size'] = 1000\n",
    "params['opposd'] = False\n",
    "params['opposd_iterations'] = 50\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "params['states_shape'] = state_dim\n",
    "params['num_actions'] = n_actions\n",
    "\n",
    "# The model has n_action policy heads and one value head\n",
    "model_actor = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "model_critic = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 1))\n",
    "model_w = th.nn.Sequential(th.nn.Linear(state_dim, 32), th.nn.ReLU(),\n",
    "                         th.nn.Linear(32, 1), th.nn.Softplus())\n",
    "model = [model_actor, model_critic, model_w]\n",
    "experiment = BatchActorCriticExperiment(params, model, learner=OffpolicyActorCriticLearner(model, params=params))\n",
    "try:\n",
    "    experiment.run(experiments_batch)\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()\n",
    "\n",
    "return_dict.update({'model' : 'OFFPAC',\n",
    "                            'experiment': experiment})\n",
    "experiments = np.append(experiments, return_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "def plot_experiments(experiments):\n",
    "    # sns.set()\n",
    "    colors = ['r', 'b', 'g', 'k']\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "    i = 0\n",
    "    for exp in [experiments[0],experiments[2], experiments[1]]:\n",
    "        # Smooth curves\n",
    "        window = max(int(len(exp['experiment'].episode_returns) / 30), 1)\n",
    "        # if len(exp.episode_losses) < window + 2: return\n",
    "        returns = np.convolve(exp['experiment'].episode_returns, np.ones(window) / window, 'valid')\n",
    "        # Determine x-axis based on samples or episodes\n",
    "        x_returns = [i + window for i in range(len(returns))]\n",
    "        plt.plot(x_returns, returns, colors[i], label=exp['model'])\n",
    "        plt.xlabel('Policy gradient step')\n",
    "        plt.ylabel('Episode return')\n",
    "        i+=1\n",
    "    plt.legend()\n",
    "    plt.title('Cart pole environment')\n",
    "    plt.savefig('cartpole_comp.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_experiments(experiments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "data": {
      "text/plain": "{'model': 'PPO',\n 'experiment': <__main__.BatchActorCriticExperiment at 0x7fa10ed454c0>}"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}