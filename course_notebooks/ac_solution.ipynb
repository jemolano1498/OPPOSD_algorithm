{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Actor-Critic implementation and evaluation\n",
    "\n",
    "## List of Contents\n",
    "\n",
    "### Provided classes\n",
    "- [`default_params()`](#default_params) this dictionary defines the default hyper-parameters\n",
    "- [`TransitionBatch`](#TransitionBatch) the basic class to summarize transitions and build an experience replay buffer.\n",
    "- [`Runner`](#Runner) interacts with one environment\n",
    "- [`MultiRunner`](#MultiRunner) runs multiple `Runner` in parallel\n",
    "- [`QController`](#QController) translates the model outputs into greedy actions\n",
    "- [`ACController`](#ACController) interprets the first model outputs as logits of a softmax\n",
    "- [`EpsilonGreedyController`](#EpsilonGreedyController) performs epsilon-greedy exploration\n",
    "- [`ReinforceLearner`](#ReinforceLearner) trains the model with the REINFORCE algorithm\n",
    "- [`Experment`](#Experiment) encapsulates and executes a single experiment\n",
    "- [`ActorCriticExperiment`](#ActorCriticExperiment) performs online Q-learning\n",
    "\n",
    "\n",
    "\n",
    "### Exercises\n",
    "- [Q4.3a) Run REINFORCE](#q1)\n",
    "- [Q4.3b) Extend REINFORCE with a bias function](#q2)\n",
    "- [Q4.3c) Extend REINFORCE to the vanilla Actor-Critic algorithm](#q3)\n",
    "- [Q4.3d) Extend the Actor-Critic algorithm to off-policy learning](#q4)\n",
    "- [Q4.3e) Implement PPO clipping for off-policy learning](#q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pytorch and tools\n",
    "import torch as th\n",
    "from torch import Tensor, LongTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import numbers\n",
    "from datetime import datetime\n",
    "# Multi-threading\n",
    "import threading\n",
    "# Plotting\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "# Reinforcement learning\n",
    "import gym\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary defines the default hyper-paramerters that you will use in your experiments. <a id=default_params></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_params():\n",
    "    \"\"\" These are the default parameters used int eh framework. \"\"\"\n",
    "    return {# Debugging outputs and plotting during training\n",
    "            'plot_frequency': 10,             # plots a debug message avery n steps\n",
    "            'plot_train_samples': True,       # whether the x-axis is env.steps (True) or episodes (False)\n",
    "            'print_when_plot': True,          # prints debug message if True\n",
    "            'print_dots': False,              # prints dots for every gradient update\n",
    "            # Environment parameters\n",
    "            'env': 'CartPole-v0',             # the environment the agent is learning in\n",
    "            'run_steps': 2048,                # samples whole episodes if run_steps <= 0\n",
    "            'max_episode_length': 200,        # maximum number of steps per episode\n",
    "            # Runner parameters\n",
    "            'max_episodes': int(1E6),         # experiment stops after this many episodes\n",
    "            # 'max_episodes': 1000,         # experiment stops after this many episodes\n",
    "            'max_steps': int(2E6),            # experiment stops after this many steps\n",
    "            # 'max_steps': 10000,            # experiment stops after this many steps\n",
    "            'multi_runner': True,             # uses multiple runners if True\n",
    "            'parallel_environments': 4,       # number of parallel runners  (only if multi_runner==True)\n",
    "            # Exploration parameters\n",
    "            'epsilon_anneal_time': int(2),    # exploration anneals epsilon over these many steps\n",
    "            'epsilon_finish': 0.1,            # annealing stops at (and keeps) this epsilon\n",
    "            'epsilon_start': 1,               # annealing starts at this epsilon\n",
    "            # Optimization parameters\n",
    "            'lr': 5E-4,                       # learning rate of optimizer\n",
    "            'gamma': 0.99,                    # discount factor gamma\n",
    "            'mini_batch_size': 200,               # number of transitions in a mini-batch\n",
    "            'batch_size': 500,               # number of transitions in a mini-batch\n",
    "            'grad_norm_clip': 1,              # gradent clipping if grad norm is larger than this\n",
    "            # Actor-critic parameters\n",
    "            'value_loss_param': 0.1,          # governs the relative impact of the value relative to policy loss\n",
    "            'advantage_bias': True,           # whether the advantages have the value as bias\n",
    "            'advantage_bootstrap': True,      # whether advantages use bootstrapping (alternatively: returns)\n",
    "            'offpolicy_iterations': 0,        # how many off-policy iterations are performed\n",
    "            'value_targets': 'returns',       # either 'returns' or 'td' as regression targets of the value function\n",
    "            # PPO parameters\n",
    "            'ppo_clipping': True,             # whether we use the PPO loss\n",
    "            'ppo_clip_eps': 0.1,              # the epsilon for the PPO loss\n",
    "\n",
    "            'states_shape': (1,),  # Amount of states\n",
    "            'num_actions': 5,       # delay between the two stacked observations\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransitionBatches` are dictionaries of variables, e.g. states or actions, that are saved in contiguous Tensors. <a id=TransitionBatch></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in exercise sheet 3\n",
    "class TransitionBatch:\n",
    "    \"\"\" Simple implementation of a batchof transitionsm (or another dictionary-based tensor structure).\n",
    "        Read and write operations are thread-safe, but the iterator is not (you cannot interate\n",
    "        over the same TransitionBatch in two threads at the same time). \"\"\"\n",
    "    def __init__(self, max_size, transition_format, batch_size=32):\n",
    "        self.lock = threading.Lock()\n",
    "        self.indices = []\n",
    "        self.size = 0\n",
    "        self.first = 0\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dict = {}\n",
    "        for key, spec in transition_format.items():\n",
    "            self.dict[key] = th.zeros([max_size, *spec[0]], dtype=spec[1])\n",
    "            \n",
    "    def _clone_empty_batch(self, max_size=None, batch_size=None):\n",
    "        \"\"\" Clones this TransitionBatch without cloning the data. \"\"\"\n",
    "        max_size = self.max_size if max_size is None else max_size\n",
    "        batch_size = self.batch_size if batch_size is None else batch_size\n",
    "        return TransitionBatch(max_size=max_size, transition_format={}, batch_size=batch_size)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\" Access the TransitionBatch with the [] operator. Use as key either \n",
    "            - the string name of a variable to get the full tensor of that variable,\n",
    "            - a slice to get a time-slice over all variables in the batch,\n",
    "            - a LongTensor that selects a subset of indices for all variables in the batch. \"\"\"\n",
    "        # Return the entry of the transition called \"key\"\n",
    "        if isinstance(key, str): \n",
    "            return self.dict[key]\n",
    "        # Return a slice of the batch\n",
    "        if isinstance(key, slice):\n",
    "            key = slice(0 if key.start is None else key.start, self.size if key.stop is None else key.stop,\n",
    "                        1 if key.step is None else key.step)\n",
    "            self.lock.acquire()\n",
    "            try:\n",
    "                batch = self._clone_empty_batch()\n",
    "                batch.size = (key.stop - key.start) // key.step \n",
    "                for k, v in self.dict.items():\n",
    "                    batch.dict[k] = v[key] \n",
    "            finally: self.lock.release()\n",
    "            return batch\n",
    "        # Collect and return a set of transitions specified by the LongTensor \"key\" \n",
    "        if isinstance(key, th.Tensor):\n",
    "            self.lock.acquire()\n",
    "            try:\n",
    "                batch = self._clone_empty_batch(max_size=key.shape[0])\n",
    "                batch.size = key.shape[0]\n",
    "                for k, v in self.dict.items():\n",
    "                    key = key.view(batch.size, *[1 for _ in range(len(v.shape[1:]))])\n",
    "                    batch.dict[k] = v.gather(dim=0, index=key.expand(batch.size, *v.shape[1:]))\n",
    "            finally: self.lock.release()\n",
    "            return batch\n",
    "        return None\n",
    "    \n",
    "    def get_first(self):\n",
    "        \"\"\" Returns a batch of the oldest entries of all variables. \"\"\"\n",
    "        batch = self._clone_empty_batch(max_size=1)\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            batch.size = 1\n",
    "            for k, v in self.dict.items():\n",
    "                batch.dict[k] = v[self.first].unsqueeze(dim=0)\n",
    "        finally: self.lock.release()\n",
    "        return batch    \n",
    "    \n",
    "    def get_last(self):\n",
    "        \"\"\" Returns a batch of the newest entries of all variables. \"\"\"\n",
    "        batch = self._clone_empty_batch(max_size=1)\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            batch.size = 1\n",
    "            for k, v in self.dict.items():\n",
    "                batch.dict[k] = v[(self.first + self.size - 1) % self.size].unsqueeze(dim=0)\n",
    "        finally: self.lock.release()\n",
    "        return batch\n",
    "    \n",
    "    def add(self, trans:dict):\n",
    "        \"\"\" Adding transition dictionaries, which can contain Tensors of arbitrary length. \"\"\"\n",
    "        if isinstance(trans, TransitionBatch):\n",
    "            trans = trans.dict\n",
    "        # Add all data in the dict\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            n = 0\n",
    "            idx = None\n",
    "            for k, v in trans.items():\n",
    "                if idx is None:\n",
    "                    n = v.shape[0]\n",
    "                    idx = th.LongTensor([(self.first + self.size + i) % self.max_size for i in range(n)])\n",
    "                else:\n",
    "                    assert n == v.shape[0], 'all tensors in a transition need to have the same batch_size'\n",
    "                idx = idx.view(idx.shape[0], *[1 for _ in range(len(v.shape) - 1)])\n",
    "                self.dict[k].scatter_(dim=0, index=idx.expand_as(v), src=v)\n",
    "            # Increase the size (and handle overflow)\n",
    "            self.size += n\n",
    "            if self.size > self.max_size:\n",
    "                self.first = (self.first + n) % self.max_size\n",
    "                self.size = self.max_size\n",
    "        finally: self.lock.release()\n",
    "        return self\n",
    "            \n",
    "    def trim(self):\n",
    "        \"\"\" Reduces the length of the max_size to its actual size (in-place). Returns self. \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            for k, v in self.dict.items():\n",
    "                self.dict[k] = v[:self.size]\n",
    "            self.max_size = self.size\n",
    "        finally: self.lock.release()\n",
    "        return self\n",
    "    \n",
    "    def replace(self, batch, index=0):\n",
    "        \"\"\" Replaces parts of this batch with another batch (which must be smaller). \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            #assert batch.max_size <= self.max_size - index, \"Replacement is larger then target area in batch.\"\n",
    "            assert batch.size <= self.max_size - index, \"Replacement is larger then target area in batch.\"\n",
    "            for k, v in batch.dict.items():\n",
    "                if batch.size < batch.max_size:\n",
    "                    v = v[:batch.size]\n",
    "                self.dict[k][index:(index + batch.max_size)] = v    \n",
    "        finally: self.lock.release()\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\" Samples a random mini-batch from the batch. \"\"\"\n",
    "        return self[th.randint(high=self.size, size=(self.batch_size,1))]\n",
    "            \n",
    "    def __len__(self): \n",
    "        \"\"\" Returns the length of the batch. \"\"\"\n",
    "        return self.size\n",
    "    \n",
    "    def __iter__(self):  \n",
    "        \"\"\" Initializes an iterator over the batch. \"\"\"\n",
    "        self.indices = list(range(self.size))\n",
    "        np.random.shuffle(self.indices)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):  \n",
    "        \"\"\" Iterates through batch, returns list of contiguous tensors. \"\"\"\n",
    "        if len(self.indices) == 0: raise StopIteration\n",
    "        size = min(self.batch_size, len(self.indices))\n",
    "        batch = self[th.LongTensor(self.indices[-size:])]\n",
    "        self.indices = self.indices[:-size]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Runner` implements a simple runner class that uses a controller to interact with the environment by calling `run()` or `run_episode()`. <a id=Runner></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in exercise sheet 3\n",
    "class Runner:\n",
    "    \"\"\" Implements a simple single-thread runner class. \"\"\"\n",
    "    def __init__(self, controller, params={}, exploration_step=1):\n",
    "        self.env = gym.make(params.get('env', 'CartPole-v0'))\n",
    "        self.cont_actions = isinstance(self.env.action_space, gym.spaces.Box)\n",
    "        self.controller = controller\n",
    "        self.epi_len = params.get('max_episode_length', self.env._max_episode_steps)\n",
    "        self.gamma = params.get('gamma', 0.99)\n",
    "        self.use_pixels = params.get('pixel_observations', False)\n",
    "        if self.use_pixels:\n",
    "            self.grayscale = params.get('pixel_grayscale', True)\n",
    "            self.add_last_obs = params.get('pixel_add_last_obs', False)\n",
    "            self.last_obs_delay = params.get('pixel_last_obs_delay', 4)\n",
    "            n_colors = 1 if self.grayscale else 3\n",
    "            n_feats = n_colors * (2 if self.add_last_obs else 1)\n",
    "            resolution = params.get('pixel_resolution', (25, 25))\n",
    "            self.state_shape = (n_feats, *resolution)\n",
    "            self.last_observations = TransitionBatch(max_size=self.last_obs_delay, \n",
    "                                                     transition_format={'img': ((n_colors, *resolution), th.float32)})\n",
    "        else:\n",
    "            self.state_shape = self.env.observation_space.shape\n",
    "        # Set up current state and time step\n",
    "        self.sum_rewards = 0\n",
    "        self.state = None\n",
    "        self.time = 0\n",
    "        self._next_step()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Closes the underlying environment. Should always when ending an experiment. \"\"\"\n",
    "        self.env.close()\n",
    "    \n",
    "    def transition_format(self):\n",
    "        \"\"\" Returns the format of transtions: a dictionary of (shape, dtype) entries for each key. \"\"\"\n",
    "        return {'actions': ((1,), th.long),\n",
    "                'states': (self.state_shape, th.float32),\n",
    "                'next_states': (self.state_shape, th.float32),\n",
    "                'rewards': ((1,),  th.float32),\n",
    "                'dones': ((1,), th.bool),\n",
    "                'returns': ((1,), th.float32)}\n",
    "    \n",
    "    def _wrap_transition(self, s, a, r, ns, d):\n",
    "        \"\"\" Takes a transition and returns a corresponding dictionary. \"\"\"\n",
    "        trans = {}\n",
    "        form = self.transition_format()\n",
    "        for key, val in [('states', s), ('actions', a), ('rewards', r), ('next_states', ns), ('dones', d)]:\n",
    "            if not isinstance(val, th.Tensor): \n",
    "                if isinstance(val, numbers.Number) or isinstance(val, bool): val = [val]\n",
    "                val = th.tensor(val, dtype=form[key][1])\n",
    "            if len(val.shape) < len(form[key][0]) + 1: val = val.unsqueeze(dim=0)\n",
    "            trans[key] = val\n",
    "        return trans\n",
    "    \n",
    "    def _pixel_observation(self, reset=False):\n",
    "        \"\"\" Returns the pixel-observation fo the current state. Opens extra window for rendering. \"\"\"\n",
    "        img = self.env.render(mode='rgb_array')\n",
    "        img = cv2.resize(img, dsize=self.state_shape[1:], interpolation=cv2.INTER_CUBIC)\n",
    "        img = th.from_numpy(img.astype(np.float32) / 255).transpose(dim0=0, dim1=2).unsqueeze(dim=0)\n",
    "        if self.grayscale: img = img.mean(dim=1, keepdim=True)\n",
    "        if self.add_last_obs:\n",
    "            if reset: self.last_observations.size = 0\n",
    "            if self.last_observations.size < self.last_observations.max_size:\n",
    "                obs = img * 0\n",
    "            else:\n",
    "                obs = self.last_observations.get_first()['img'].clone()\n",
    "            self.last_observations.add({'img': img})\n",
    "            img = th.cat([obs, img], dim=1)\n",
    "        return img\n",
    "    \n",
    "    def _run_step(self, a):\n",
    "        \"\"\" Make a step in the environment (and update internal bookeeping) \"\"\"\n",
    "        ns, r, d, _ = self.env.step(a.item())\n",
    "        self.sum_rewards += r\n",
    "        if self.use_pixels: ns = self._pixel_observation()\n",
    "        return r, ns, d\n",
    "    \n",
    "    def _next_step(self, done=True, next_state=None):\n",
    "        \"\"\" Switch to the next time-step (and update internal bookeeping) \"\"\"\n",
    "        self.time = 0 if done else self.time + 1\n",
    "        if done:\n",
    "            self.sum_rewards = 0\n",
    "            self.state = self.env.reset()\n",
    "            if self.use_pixels: self.state = self._pixel_observation(reset=True)\n",
    "        else:\n",
    "            self.state = next_state\n",
    "        \n",
    "    \n",
    "    def run(self, n_steps, transition_buffer=None, trim=True, return_dict=None):\n",
    "        \"\"\" Runs n_steps in the environment and stores them in the trainsition_buffer (newly created if None).\n",
    "            If n_steps <= 0, stops at the end of an episode and optionally trins the transition_buffer.\n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        my_transition_buffer = TransitionBatch(n_steps if n_steps > 0 else self.epi_len, self.transition_format())\n",
    "        time, episode_start, episode_lengths, episode_rewards = 0, 0, [], []\n",
    "        max_steps = n_steps if n_steps > 0 else self.epi_len\n",
    "        for t in range(max_steps):\n",
    "            # One step in the envionment\n",
    "            a = self.controller.choose(self.state)\n",
    "            r, ns, d = self._run_step(a)\n",
    "            terminal = d and self.time < self.epi_len - 1\n",
    "            my_transition_buffer.add(self._wrap_transition(self.state, a, r, ns, terminal)) \n",
    "            if t == self.epi_len - 1: d = True\n",
    "            # Compute discounted returns if episode has ended or max_steps has been reached\n",
    "            if d or t == (max_steps - 1):\n",
    "                my_transition_buffer['returns'][t] = my_transition_buffer['rewards'][t]\n",
    "                for i in range(t - 1, episode_start - 1, -1):\n",
    "                    my_transition_buffer['returns'][i] = my_transition_buffer['rewards'][i] \\\n",
    "                                                         + self.gamma * my_transition_buffer['returns'][i + 1]\n",
    "                episode_start = t + 1\n",
    "            # Remember statistics and advance (potentially initilaizing a new episode)\n",
    "            if d:\n",
    "                episode_lengths.append(self.time + 1)\n",
    "                episode_rewards.append(self.sum_rewards)\n",
    "            self._next_step(done=d, next_state=ns)\n",
    "            time += 1\n",
    "            # If n_steps <= 0, we return after one episode (trimmed if specified)\n",
    "            if d and n_steps <= 0: \n",
    "                my_transition_buffer.trim()\n",
    "                break\n",
    "        # Add the sampled transitions to the given transition buffer\n",
    "        transition_buffer = my_transition_buffer if transition_buffer is None \\\n",
    "                            else transition_buffer.add(my_transition_buffer)\n",
    "        if trim: transition_buffer.trim()\n",
    "        # Return statistics (mean reward, mean length and environment steps)\n",
    "        if return_dict is None: return_dict = {}\n",
    "        return_dict.update({'buffer': transition_buffer,\n",
    "                            'episode_reward': None if len(episode_rewards) == 0 else np.mean(episode_rewards),\n",
    "                            'episode_length': None if len(episode_lengths) == 0 else np.mean(episode_lengths),\n",
    "                            'env_steps': time})\n",
    "        return return_dict\n",
    "        \n",
    "    def run_episode(self, transition_buffer=None, trim=True, return_dict=None):\n",
    "        \"\"\" Runs one episode in the environemnt. \n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        return self.run(0, transition_buffer, trim, return_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultiRunner` runs a number of `Runner` instances in parallel. <a id=MultiRunner></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in exercise sheet 3\n",
    "class MultiRunner:\n",
    "    \"\"\" Simple class that runs multiple Runner objects in parallel and merges their outputs. \"\"\"\n",
    "    def __init__(self, controller, params={}):\n",
    "        self.workers = []\n",
    "        self.runners = []\n",
    "        n = params.get('parallel_environments', 1)\n",
    "        for _ in range(n):\n",
    "            self.runners.append(Runner(controller=controller, params=params))\n",
    "            \n",
    "    def transition_format(self):\n",
    "        \"\"\" Same transition-format as underlying Runners. \"\"\"\n",
    "        return self.runners[0].transition_format()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\" Closes the underlying environment. Should always when ending an experiment. \"\"\"\n",
    "        # Join all workers\n",
    "        for w in self.workers:\n",
    "            w.join()\n",
    "        # Exit all environments\n",
    "        for r in self.runners:\n",
    "            r.close()\n",
    "    \n",
    "    def fork(self, target, common_args=None, specific_args=None):\n",
    "        \"\"\" Executes the function \"target\" on all runners. \"common_args\" is a dictionary of \n",
    "            arguments that are passed to all runners, \"specific_args\" is a list of \n",
    "            dictionaries that contain individual parameters for each runner. \"\"\" \n",
    "        # Fork all runners\n",
    "        self.workers = []\n",
    "        for i, r in enumerate(self.runners):\n",
    "            r_args = [] if specific_args is None else [arg[i] for arg in specific_args]\n",
    "            self.workers.append(threading.Thread(target=target, args=(r, *common_args, *r_args)))\n",
    "            self.workers[-1].start()\n",
    "        # Join all runners\n",
    "        for w in self.workers:\n",
    "            w.join()\n",
    "    \n",
    "    def run(self, n_steps, transition_buffer=None, trim=True):\n",
    "        \"\"\" Runs n_steps, split amongst runners, and stores them in the trainsition_buffer (newly created if None).\n",
    "            If n_steps <= 0, stops at the end of an episode and optionally trims the transition_buffer.\n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        n_steps = n_steps // len(self.runners)\n",
    "        if transition_buffer is None:\n",
    "            buffer_len = len(self.runners) * (n_steps if n_steps > 0 else self.runners[0].epi_len)\n",
    "            transition_buffer = TransitionBatch(buffer_len, self.runners[0].transition_format())\n",
    "        return_dicts = [{} for _ in self.runners]\n",
    "        self.fork(target=Runner.run, common_args=(n_steps, transition_buffer, False), specific_args=(return_dicts,))\n",
    "        if trim: transition_buffer.trim()\n",
    "        rewards = [d['episode_reward'] for d in return_dicts if d['episode_reward'] is not None]\n",
    "        lengths = [d['episode_length'] for d in return_dicts if d['episode_reward'] is not None]\n",
    "        return {'buffer': transition_buffer, \n",
    "                'episode_reward': np.mean(rewards) if len(rewards) > 0 else None,\n",
    "                'episode_length': np.mean(lengths) if len(lengths) > 0 else None,\n",
    "                'env_steps': len(transition_buffer)}\n",
    "\n",
    "    def run_episode(self, transition_buffer=None, trim=True):\n",
    "        \"\"\" Runs one episode in the environemnt. \n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        return self.run(0, transition_buffer, trim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `QController` translates model responses into actions. Call `choose()` to select actions or `probabilities()` to get the probabilities with which the controller would choose the actions. <a id=QController></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in exercise sheet 3\n",
    "class QController:\n",
    "    \"\"\" Controller for Q-value functions, synchronizes the model calls. \"\"\"\n",
    "    def __init__(self, model, num_actions=None, params={}):\n",
    "        self.lock = threading.Lock()\n",
    "        self.num_actions = model[-1].out_features if num_actions is None else num_actions\n",
    "        self.model = model\n",
    "        \n",
    "    def copy(self):\n",
    "        \"\"\" Shallow copy of this controller that does not copy the model. \"\"\"\n",
    "        return QController(model=self.model, num_actions=self.num_actions)\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\" Returns a generator of the underlying model parameters. \"\"\"\n",
    "        return self.model.parameters()\n",
    "    \n",
    "    def sanitize_inputs(self, observation, **kwargs):\n",
    "        \"\"\" Casts numpy arrays as Tensors. \"\"\"\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = th.Tensor(observation).unsqueeze(dim=0)\n",
    "        return observation\n",
    "                \n",
    "    def choose(self, observation, **kwargs):\n",
    "        \"\"\" Returns the greedy actions the agent would choose when facing an \"observation\". \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try: \n",
    "            mx = self.model(self.sanitize_inputs(observation))\n",
    "            if mx.shape[-1] > self.num_actions: mx = mx[:, :self.num_actions]\n",
    "        finally: self.lock.release()\n",
    "        return th.max(mx, dim=-1)[1]\n",
    "\n",
    "    def probabilities(self, observation, **kwargs):\n",
    "        \"\"\" Returns the probabilities with which the agent would choose actions (here one-hot because greedy). \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try: \n",
    "            mx = self.model(self.sanitize_inputs(observation))\n",
    "            if mx.shape[-1] > self.num_actions: mx = mx[:, :self.num_actions]\n",
    "        finally: self.lock.release()\n",
    "        return th.zeros(*mx.shape).scatter_(dim=-1, index=th.max(mx, dim=-1)[1].unsqueeze(dim=-1), src=th.ones(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `ACController` overwrites the `QController` and interpretes the model output as logits of a softmax policy (followed by additional outputs, like a value head, that are of no interest to the contoller).  <a id=ACController></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACController (QController):\n",
    "    \"\"\" A controller that interprets the first num_actions model outputs as logits of a softmax distribution. \"\"\"\n",
    "    def probabilities(self, observation, precomputed=False, **kwargs):\n",
    "        self.lock.acquire()\n",
    "        try: mx = observation if precomputed else self.model(self.sanitize_inputs(observation))[:, :self.num_actions] \n",
    "        finally: self.lock.release()\n",
    "        return th.nn.functional.softmax(mx, dim=-1)\n",
    "\n",
    "    def choose(self, observation, **kwargs):\n",
    "        return th.distributions.Categorical(probs=self.probabilities(observation, **kwargs)).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `EpsilonGreedyController` is a controller that autonomously anneals an expsilon greedy exploration strategy. <a id=EpsilonGreedyController></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in exercise sheet 3\n",
    "class EpsilonGreedyController:\n",
    "    \"\"\" A wrapper that makes any controller into an epsilon-greedy controller. \n",
    "        Keeps track of training-steps to decay exploration automatically. \"\"\"\n",
    "    def __init__(self, controller, params={}, exploration_step=1):\n",
    "        self.controller = controller\n",
    "        self.num_actions = controller.num_actions\n",
    "        self.max_eps = params.get('epsilon_start', 1.0)\n",
    "        self.min_eps = params.get('epsilon_finish', 0.05)\n",
    "        self.anneal_time = int(params.get('epsilon_anneal_time', 10000) / exploration_step)\n",
    "        self.num_decisions = 0\n",
    "    \n",
    "    def epsilon(self):\n",
    "        \"\"\" Returns current epsilon. \"\"\"\n",
    "        return max(1 - self.num_decisions / (self.anneal_time - 1), 0) \\\n",
    "                * (self.max_eps - self.min_eps) + self.min_eps\n",
    "    \n",
    "    def choose(self, observation, increase_counter=True, **kwargs):\n",
    "        \"\"\" Returns the (possibly random) actions the agent takes when faced with \"observation\".\n",
    "            Decays epsilon only when increase_counter=True\". \"\"\"\n",
    "        eps = self.epsilon()\n",
    "        if increase_counter: self.num_decisions += 1\n",
    "        if np.random.rand() < eps: \n",
    "            return th.randint(self.controller.num_actions, (1,), dtype=th.long)\n",
    "        else: \n",
    "            return self.controller.choose(observation, **kwargs)\n",
    "    \n",
    "    def probabilities(self, observation, **kwargs):\n",
    "        \"\"\" Returns the probabilities with which the agent would choose actions. \"\"\"\n",
    "        eps = self.epsilon()\n",
    "        return eps * th.ones(1, 1) / self.num_actions + \\\n",
    "               (1 - eps) * self.controller.probabilities(observation, **kwargs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `ReinforceLearner` is a learner class that trains an actor-critic model with the REINFORCE algorithm. <a id=ReinforceLearner></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceLearner:\n",
    "    \"\"\" A learner that performs a version of REINFORCE. \"\"\"\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        self.model = model\n",
    "        self.controller = controller\n",
    "        self.value_loss_param = params.get('value_loss_param', 1)\n",
    "        self.offpolicy_iterations = params.get('offpolicy_iterations', 0)\n",
    "        self.all_parameters = list(model.parameters())\n",
    "        self.optimizer = th.optim.Adam(self.all_parameters, lr=params.get('lr', 5E-4))\n",
    "        self.grad_norm_clip = params.get('grad_norm_clip', 10)\n",
    "        self.compute_next_val = False  # whether the next state's value is computed\n",
    "        self.old_pi = None  # this variable can be used for your PPO implementation\n",
    "        \n",
    "    def set_controller(self, controller):\n",
    "        \"\"\" This function is called in the experiment to set the controller. \"\"\"\n",
    "        self.controller = controller\n",
    "    \n",
    "    def _advantages(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the advantages, Q-values or returns for the policy loss. \"\"\"\n",
    "        return batch['returns']\n",
    "    \n",
    "    def _value_loss(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the value loss (if there is one). \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def _policy_loss(self, pi, advantages):\n",
    "        \"\"\" Computes the policy loss. \"\"\"\n",
    "        return -(advantages.detach() * pi.log()).mean()\n",
    "\n",
    "    def train(self, batch):\n",
    "        assert self.controller is not None, \"Before train() is called, a controller must be specified. \"\n",
    "        self.model.train(True)\n",
    "        self.old_pi, loss_sum = None, 0.0\n",
    "        for _ in range(1 + self.offpolicy_iterations):\n",
    "            # Compute the model-output for given batch\n",
    "            out = self.model(batch['states'])   # compute both policy and values\n",
    "            val = out[:, -1].unsqueeze(dim=-1)  # last entry are the values\n",
    "            next_val = self.model(batch['next_states'])[:, -1].unsqueeze(dim=-1) if self.compute_next_val else None\n",
    "            pi = self.controller.probabilities(out[:, :-1], precomputed=True).gather(dim=-1, index=batch['actions'])\n",
    "            # Combine policy and value loss\n",
    "            loss = self._policy_loss(pi, self._advantages(batch, val, next_val)) \\\n",
    "                    + self.value_loss_param * self._value_loss(batch, val, next_val)\n",
    "            # Backpropagate loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = th.nn.utils.clip_grad_norm_(self.all_parameters, self.grad_norm_clip)\n",
    "            self.optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "        return loss_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `Experiment` is an abstract class that starts and maintains a single learning experiment (i.e. random seed). The experiment is started using `run()` and can be interrupted at any time using `close()`. Afterwards the experiment can be restarted at any time calling `run()` again. <a id=Experiment></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in exercise sheet 3\n",
    "# import seaborn as sns\n",
    "class Experiment:\n",
    "    \"\"\" Abstract class of an experiment. Contains logging and plotting functionality.\"\"\"\n",
    "    def __init__(self, params, model, **kwargs):\n",
    "        self.params = params\n",
    "        self.plot_frequency = params.get('plot_frequency', 10)\n",
    "        self.plot_train_samples = params.get('plot_train_samples', True)\n",
    "        self.print_when_plot = params.get('print_when_plot', False)\n",
    "        self.print_dots = params.get('print_dots', False)\n",
    "        self.episode_returns = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_losses  = []\n",
    "        self.env_steps = []\n",
    "        self.total_run_time = 0.0\n",
    "        \n",
    "    def plot_training(self, update=False):\n",
    "        \"\"\" Plots logged training results. Use \"update=True\" if the plot is continuously updated\n",
    "            or use \"update=False\" if this is the final call (otherwise there will be double plotting). \"\"\" \n",
    "        # Smooth curves\n",
    "        window = max(int(len(self.episode_returns) / 3), 1)\n",
    "        if len(self.episode_losses) < window + 2: return\n",
    "        returns = np.convolve(self.episode_returns, np.ones(window)/window, 'valid')\n",
    "        lengths = np.convolve(self.episode_lengths, np.ones(window)/window, 'valid')\n",
    "        losses = np.convolve(self.episode_losses, np.ones(window)/window, 'valid')\n",
    "        env_steps = np.convolve(self.env_steps, np.ones(window)/window, 'valid')\n",
    "        # Determine x-axis based on samples or episodes\n",
    "        if self.plot_train_samples:\n",
    "            x_returns = env_steps\n",
    "            x_losses = env_steps[(len(env_steps) - len(losses)):]\n",
    "        else:\n",
    "            x_returns = [i + window for i in range(len(returns))]\n",
    "            x_losses = [i + len(returns) - len(losses) + window for i in range(len(losses))]\n",
    "        # Create plot\n",
    "        colors = ['b', 'g', 'r']\n",
    "        # fig.set_size_inches(16, 4)\n",
    "        plt.clf()\n",
    "        # Plot the losses in the left subplot\n",
    "        # pl.subplot(1, 3, 1)\n",
    "        plt.plot(x_returns, returns, colors[0])\n",
    "        plt.xlabel('Environment steps' if self.plot_train_samples else 'Batch episodes')\n",
    "        plt.ylabel('Episode return')\n",
    "        # plt.title('Average training score for CartPole with off-PAC with PPO')\n",
    "        # Plot the episode lengths in the middle subplot\n",
    "        # ax = pl.subplot(1, 3, 2)\n",
    "        # ax.plot(env_steps, lengths, colors[0])\n",
    "        # ax.set_xlabel('environment steps' if self.plot_train_samples else 'episodes')\n",
    "        # ax.set_ylabel('episode length')\n",
    "        # Plot the losses in the right subplot\n",
    "        # ax = pl.subplot(1, 3, 3)\n",
    "        # ax.plot(x_losses, losses, colors[0])\n",
    "        # ax.set_xlabel('environment steps' if self.plot_train_samples else 'episodes')\n",
    "        # ax.set_ylabel('loss')\n",
    "        # dynamic plot update\n",
    "        display.clear_output(wait=True)\n",
    "        if update:\n",
    "            display.display(pl.gcf())\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\" Frees all allocated runtime ressources, but allows to continue the experiment later. \n",
    "            Calling the run() method after close must be able to pick up the experiment where it was. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\" Starts (or continues) the experiment. \"\"\"\n",
    "        assert False, \"You need to extend the Expeirment class and override the method run(). \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ActorCriticExperiment` performs policy gradient learning using `ReinforceLearner`. One can specify another learner, which you will do in later exercises. <a id=ActorCriticExperiment></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticExperiment (Experiment):\n",
    "    def __init__(self, params, model, learner=None, **kwargs):\n",
    "        super().__init__(params, model, **kwargs)\n",
    "        self.max_episodes = params.get('max_episodes', int(1E6))\n",
    "        self.max_batch_episodes = params.get('max_batch_episodes', int(1E6))\n",
    "        self.max_steps = params.get('max_steps', int(1E9))\n",
    "        self.grad_repeats = params.get('grad_repeats', 1)\n",
    "        self.batch_size = params.get('batch_size', 1024)\n",
    "        self.controller = ACController(model, num_actions=gym.make(params['env']).action_space.n, params=params)\n",
    "        self.controller = EpsilonGreedyController(controller=self.controller, params=params)\n",
    "        self.runner = MultiRunner(self.controller, params=params) if params.get('multi_runner', True) \\\n",
    "                      else Runner(self.controller, params=params)\n",
    "        self.learner = ReinforceLearner(model, params=params) if learner is None else learner\n",
    "        self.learner.set_controller(self.controller)\n",
    "        self.opposd = params.get('opposd', False)\n",
    "        self.opposd_iterations = params.get('opposd_iterations', 50)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Overrides Experiment.close() \"\"\"\n",
    "        self.runner.close()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Overrides Experiment.run() \"\"\"\n",
    "        # Plot past results if available\n",
    "        if self.plot_frequency is not None and len(self.episode_losses) > 2:\n",
    "            self.plot_training(update=True)\n",
    "        # Run the experiment\n",
    "        transition_buffer = TransitionBatch(self.batch_size, self.runner.transition_format(), self.batch_size)\n",
    "        env_steps = 0 if len(self.env_steps) == 0 else self.env_steps[-1]\n",
    "        for e in range(self.max_batch_episodes):\n",
    "            # Run the policy for batch_size steps\n",
    "            batch = self.runner.run(self.batch_size, transition_buffer)\n",
    "            if self.opposd:\n",
    "                # print('OK1')\n",
    "                self.learner.reset_w_net()\n",
    "                # print('OK2')\n",
    "            env_steps += batch['env_steps']\n",
    "            if batch['episode_length'] is not None:\n",
    "                self.env_steps.append(env_steps)\n",
    "                self.episode_lengths.append(batch['episode_length'])\n",
    "                self.episode_returns.append(batch['episode_reward'])\n",
    "            # Make a gradient update step\n",
    "            loss = self.learner.train(batch['buffer'])\n",
    "            self.episode_losses.append(loss)\n",
    "            # Quit if maximal number of environment steps is reached\n",
    "            if env_steps >= self.max_steps: break\n",
    "            # Show intermediate results\n",
    "            if self.print_dots:\n",
    "                print('.', end='')\n",
    "            if self.plot_frequency is not None and (e + 1) % self.plot_frequency == 0 \\\n",
    "                                               and len(self.episode_losses) > 2:\n",
    "                self.plot_training(update=True)\n",
    "                if self.print_when_plot:\n",
    "                    print('Episode %u, 100-epi-return %.4g +- %.3g, length %u, loss %g' % \n",
    "                          (len(self.episode_returns), np.mean(self.episode_returns[-100:]), \n",
    "                           np.std(self.episode_returns[-100:]), np.mean(self.episode_lengths[-100:]), \n",
    "                           np.mean(self.episode_losses[-100:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BatchActorCriticExperiment (Experiment):\n",
    "    def __init__(self, params, model, learner=None, **kwargs):\n",
    "        super().__init__(params, model, **kwargs)\n",
    "        self.max_episodes = params.get('max_episodes', int(1E6))\n",
    "        self.max_batch_episodes = params.get('max_batch_episodes', int(1E6))\n",
    "        self.max_steps = params.get('max_steps', int(1E9))\n",
    "        self.grad_repeats = params.get('grad_repeats', 1)\n",
    "        self.batch_size = params.get('batch_size', 1e5)\n",
    "        self.mini_batch_size = params.get('mini_batch_size', 200)\n",
    "        self.controller = ACController(model, num_actions=gym.make(params['env']).action_space.n, params=params)\n",
    "        self.controller = EpsilonGreedyController(controller=self.controller, params=params)\n",
    "        self.runner = MultiRunner(self.controller, params=params) if params.get('multi_runner', True) \\\n",
    "                      else Runner(self.controller, params=params)\n",
    "        self.learner = ReinforceLearner(model, params=params) if learner is None else learner\n",
    "        self.learner.set_controller(self.controller)\n",
    "        self.opposd = params.get('opposd', False)\n",
    "        self.opposd_iterations = params.get('opposd_iterations', 50)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Overrides Experiment.close() \"\"\"\n",
    "        self.runner.close()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Overrides Experiment.run() \"\"\"\n",
    "        # Plot past results if available\n",
    "        if self.plot_frequency is not None and len(self.episode_losses) > 2:\n",
    "            self.plot_training(update=True)\n",
    "        # Run the experiment\n",
    "        transition_buffer = TransitionBatch(self.batch_size, self.runner.transition_format(), self.mini_batch_size)\n",
    "        batch = self.runner.run(self.batch_size, transition_buffer)\n",
    "        env_steps = 0 if len(self.env_steps) == 0 else self.env_steps[-1]\n",
    "        for e in range(self.max_batch_episodes):\n",
    "            # Make a gradient update step\n",
    "            loss = self.learner.train(batch)\n",
    "            self.episode_losses.append(loss)\n",
    "            # Quit if maximal number of environment steps is reached\n",
    "            if env_steps >= self.max_steps: break\n",
    "            # Show intermediate results\n",
    "            if self.print_dots:\n",
    "                print('.', end='')\n",
    "            if self.plot_frequency is not None and (e + 1) % self.plot_frequency == 0 \\\n",
    "                                               and len(self.episode_losses) > 2:\n",
    "                self.plot_training(update=True)\n",
    "                if self.print_when_plot:\n",
    "                    print('Episode %u, 100-epi-return %.4g +- %.3g, length %u, loss %g' %\n",
    "                          (len(self.episode_returns), np.mean(self.episode_returns[-100:]),\n",
    "                           np.std(self.episode_returns[-100:]), np.mean(self.episode_lengths[-100:]),\n",
    "                           np.mean(self.episode_losses[-100:])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.3a) Run the given online REINFORCE algorithm <a id=q1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q4.3a) Run the given Online Q-learning algorithm without target networks or experience replay\n",
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "# The model has n_action policy heads and one value head\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions + 1))\n",
    "experiment = ActorCriticExperiment(params, model, learner=ReinforceLearner(model, params=params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.3b) Add a value bias to REINFORCE <a id=q2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BiasedReinforceLearner (ReinforceLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "        self.value_criterion = th.nn.MSELoss()\n",
    "        self.advantage_bias = params.get('advantage_bias', True)\n",
    "        self.value_targets = params.get('value_targets', 'returns')\n",
    "        self.gamma = params.get('gamma')\n",
    "        self.compute_next_val = (self.value_targets == 'td')\n",
    "    \n",
    "    def _advantages(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the advantages, Q-values or returns for the policy loss. \"\"\"\n",
    "        advantages = batch['returns']\n",
    "        if self.advantage_bias:\n",
    "            advantages -= values\n",
    "        return advantages\n",
    "    \n",
    "    def _value_loss(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the value loss (if there is one). \"\"\"\n",
    "        targets = None\n",
    "        if self.value_targets == 'returns':\n",
    "            targets = batch['returns']\n",
    "        elif self.value_targets == 'td':\n",
    "            targets = batch['rewards'] + self.gamma * (~batch['dones'] * next_values)\n",
    "        return self.value_criterion(values, targets.detach())\n",
    "\n",
    "\n",
    "# # Executing this code-block defines a new experiment\n",
    "# params = default_params()\n",
    "# env = gym.make(params['env'])\n",
    "# n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "# # The model has n_action policy heads and one value head\n",
    "# model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "#                          th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "#                          th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "#                          th.nn.Linear(128, n_actions + 1))\n",
    "# experiment = ActorCriticExperiment(params, model, learner=BiasedReinforceLearner(model, params=params))\n",
    "#\n",
    "# # Re-executing this code-block picks up the experiment where you left off\n",
    "# try:\n",
    "#     experiment.run()\n",
    "# except KeyboardInterrupt:\n",
    "#     experiment.close()\n",
    "# experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.3c) Add an advantage to RENFORCE <a id=q3></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ActorCriticLearner (BiasedReinforceLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "        self.advantage_bootstrap = params.get('advantage_bootstrap', True)\n",
    "        self.compute_next_val = self.compute_next_val or self.advantage_bootstrap\n",
    "    \n",
    "    def _advantages(self, batch, values=None, next_values=None):\n",
    "        \"\"\" Computes the advantages, Q-values or returns for the policy loss. \"\"\"\n",
    "        advantages = None\n",
    "        if self.advantage_bootstrap: \n",
    "            advantages = batch['rewards'] + self.gamma * (~batch['dones'] * next_values)\n",
    "        else:\n",
    "            advantages = batch['returns']\n",
    "        if self.advantage_bias: \n",
    "            advantages = advantages - values\n",
    "        return advantages\n",
    "    \n",
    "\n",
    "# # Executing this code-block defines a new experiment\n",
    "# params = default_params()\n",
    "# env = gym.make(params['env'])\n",
    "# n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "# # The model has n_action policy heads and one value head\n",
    "# model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "#                          th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "#                          th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "#                          th.nn.Linear(128, n_actions + 1))\n",
    "# experiment = ActorCriticExperiment(params, model, learner=ActorCriticLearner(model, params=params))\n",
    "#\n",
    "# # Re-executing this code-block picks up the experiment where you left off\n",
    "# try:\n",
    "#     experiment.run()\n",
    "# except KeyboardInterrupt:\n",
    "#     experiment.close()\n",
    "# experiment.plot_training()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "###Actor-critic algorithm with off-policy updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['offpolicy_iterations'] = 128\n",
    "params['plot_train_samples'] = False\n",
    "params['plot_frequency'] = 4\n",
    "params['max_batch_episodes'] = int(100)\n",
    "params['batch_size'] = 1000\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "\n",
    "# The model has n_action policy heads and one value head\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions + 1))\n",
    "experiment = ActorCriticExperiment(params, model, learner=ActorCriticLearner(model, params=params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class OffpolicyActorCriticLearner (ActorCriticLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "\n",
    "    def _policy_loss(self, pi, advantages):\n",
    "        \"\"\" Computes the policy loss. \"\"\"\n",
    "        if self.old_pi is None:\n",
    "            self.old_pi = pi  # remember on-policy probabilities for off-policy losses\n",
    "            # Return the defaul on-policy loss\n",
    "            return super()._policy_loss(pi, advantages)\n",
    "        else:\n",
    "            # The loss for off-policy data\n",
    "            ratios = pi / self.old_pi.detach()\n",
    "            return -(advantages.detach() * ratios).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['offpolicy_iterations'] = 128\n",
    "params['plot_train_samples'] = False\n",
    "params['plot_frequency'] = 4\n",
    "params['max_batch_episodes'] = int(100)\n",
    "params['batch_size'] = 1000\n",
    "\n",
    "# The model has n_action policy heads and one value head\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions + 1))\n",
    "experiment = ActorCriticExperiment(params, model, learner=OffpolicyActorCriticLearner(model, params=params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.3e) Add PPO clipping to the off-policy actor critic  <a id=q5></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class PPOLearner (OffpolicyActorCriticLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "        self.ppo_clipping = params.get('ppo_clipping', False)\n",
    "        self.ppo_clip_eps = params.get('ppo_clip_eps', 0.2)\n",
    "\n",
    "    def _policy_loss(self, pi, advantages):\n",
    "        \"\"\" Computes the policy loss. \"\"\"\n",
    "        if self.old_pi is None:\n",
    "            # The loss for on-policy data does not change\n",
    "            return super()._policy_loss(pi, advantages)\n",
    "        else:\n",
    "            # The loss for off-policy data\n",
    "            ratios = pi / self.old_pi.detach()\n",
    "            loss = advantages.detach() * ratios\n",
    "            if self.ppo_clipping:\n",
    "                # off-policy loss with PPO clipping\n",
    "                ppo_loss = th.clamp(ratios, 1-self.ppo_clip_eps, 1+self.ppo_clip_eps) * advantages.detach()\n",
    "                loss = th.min(loss, ppo_loss)\n",
    "            return -loss.mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['offpolicy_iterations'] = 128\n",
    "params['plot_train_samples'] = False\n",
    "params['plot_frequency'] = 4\n",
    "params['max_batch_episodes'] = int(100)\n",
    "params['batch_size'] = 1000\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "\n",
    "# The model has n_action policy heads and one value head\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions + 1))\n",
    "experiment = ActorCriticExperiment(params, model, learner=PPOLearner(model, params=params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class OPPOSDLearner(OffpolicyActorCriticLearner):\n",
    "    def __init__(self, model, controller=None, params={}):\n",
    "        super().__init__(model=model, controller=controller, params=params)\n",
    "        self.num_actions = params.get('num_actions', 5)\n",
    "        self.batch_size = params.get('batch_size')\n",
    "        self.states_shape = params.get('states_shape')\n",
    "        self.w_grad_norm_clip = params.get('grad_norm_clip', 10)\n",
    "        self.pi_0 = 1 * th.ones(self.batch_size, 1) / self.num_actions\n",
    "        self.w_model = th.nn.Sequential(th.nn.Linear(self.states_shape, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 1))\n",
    "        self.w_parameters = list(self.w_model.parameters())\n",
    "        self.w_optimizer = th.optim.Adam(self.w_parameters, lr=params.get('lr', 5E-4))\n",
    "\n",
    "    def train(self, batch):\n",
    "        assert self.controller is not None, \"Before train() is called, a controller must be specified. \"\n",
    "        self.model.train(True)\n",
    "        loss_sum = 0.0\n",
    "        # out = self.model(batch['states'])\n",
    "        # self.pi_0 = self.controller.probabilities(out[:, :-1], precomputed=True).gather(dim=-1, index=batch['actions'])\n",
    "        pi = None\n",
    "\n",
    "        for it in range(1 + self.offpolicy_iterations):\n",
    "            # Compute the model-output for given batch\n",
    "            out = self.model(batch['states'])  # compute both policy and values\n",
    "            val = out[:, -1].unsqueeze(dim=-1)  # last entry are the values\n",
    "            next_val = self.model(batch['next_states'])[:, -1].unsqueeze(dim=-1) if self.compute_next_val else None\n",
    "            pi = self.controller.probabilities(out[:, :-1], precomputed=True).gather(dim=-1, index=batch['actions'])\n",
    "            ratio = pi.detach() / self.pi_0\n",
    "            for _ in range(50):\n",
    "                # batch_w = self.runner.run(self.batch_size, transition_buffer)\n",
    "                batch_w = batch.sample()\n",
    "                self.update_policy_distribution(batch_w, ratio)\n",
    "\n",
    "            w = self.w_model(batch['states']).detach()\n",
    "            w /= th.mean(w)\n",
    "            # Combine policy and value loss\n",
    "            loss = self._policy_loss(ratio * w, self._advantages(batch, val, next_val)) \\\n",
    "                   + self.value_loss_param * self._value_loss(batch, val, next_val)\n",
    "            # Backpropagate loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = th.nn.utils.clip_grad_norm_(self.all_parameters, self.grad_norm_clip)\n",
    "            self.optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "            print('OffP it. %d'% it)\n",
    "        print('Pass')\n",
    "        self.pi_0 = pi.detach()\n",
    "        return loss_sum\n",
    "\n",
    "    def _policy_loss(self, pi, advantages):\n",
    "        # The loss for off-policy data\n",
    "\n",
    "        if self.pi_0 is None:\n",
    "            self.pi_0 = pi  # remember on-policy probabilities for off-policy losses\n",
    "            # Return the defaul on-policy loss\n",
    "            return super()._policy_loss(pi, advantages)\n",
    "        else:\n",
    "            loss = advantages.detach() * pi\n",
    "            return -loss.mean()\n",
    "\n",
    "    def reset_w_net(self):\n",
    "        self.w_model = th.nn.Sequential(th.nn.Linear(self.states_shape, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 1))\n",
    "        self.w_parameters = list(self.w_model.parameters())\n",
    "        self.w_optimizer = th.optim.Adam(self.w_parameters, lr=params.get('lr', 5E-4))\n",
    "\n",
    "    def update_policy_distribution(self, batch, ratios):\n",
    "        self.w_model.train(True)\n",
    "        batch_size = batch.size\n",
    "\n",
    "        next_states = batch['next_states']\n",
    "        with th.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            w = self.w_model(batch['states'])\n",
    "            w_ = self.w_model(batch['next_states'])\n",
    "\n",
    "            w = w / th.mean(w)\n",
    "            w_ = w_ / th.mean(w)\n",
    "\n",
    "            d = w * ratios - w_\n",
    "\n",
    "            k = th.zeros(batch_size, batch_size, self.states_shape)\n",
    "            for i in range(self.states_shape):\n",
    "                k[:,:,i] = next_states[:,i].view(1,-1) - next_states[:,i].view(-1,1)\n",
    "\n",
    "            k = (th.linalg.norm(k, dim=-1)<1).float()\n",
    "            prod = th.matmul(d, d.transpose(0,1))\n",
    "\n",
    "            # n_lm = 3\n",
    "            # y = th.randn(n_lm, 4)\n",
    "            # dist_gt = th.zeros(n_lm, n_lm, 4)\n",
    "            # dist_gt[:,:,0] = y[:,0].view(1,-1) - y[:,0].view(-1,1)\n",
    "            # dist_gt[:,:,1] = y[:,1].view(1,-1) - y[:,1].view(-1,1)\n",
    "            # dist_gt[:,:,2] = y[:,2].view(1,-1) - y[:,2].view(-1,1)\n",
    "            # dist_gt[:,:,3] = y[:,3].view(1,-1) - y[:,3].view(-1,1)\n",
    "            # th.linalg.norm(dist_gt, dim=-1)\n",
    "            # k = (th.linalg.norm(dist_gt, dim=-1)<1).float()\n",
    "\n",
    "            D = th.sum(prod * k)/batch_size\n",
    "\n",
    "            self.w_optimizer.zero_grad()\n",
    "            D.backward()\n",
    "            self.w_optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['offpolicy_iterations'] = 128\n",
    "params['plot_train_samples'] = False\n",
    "params['plot_frequency'] = 4\n",
    "params['max_batch_episodes'] = int(100)\n",
    "params['batch_size'] = 1000\n",
    "params['opposd'] = True\n",
    "params['opposd_iterations'] = 50\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "params['states_shape'] = state_dim\n",
    "params['num_actions'] = n_actions\n",
    "\n",
    "# The model has n_action policy heads and one value head\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions + 1))\n",
    "experiment = ActorCriticExperiment(params, model, learner=OPPOSDLearner(model, params=params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}