{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom QDN implementation and evaluation\n",
    "\n",
    "## List of Contents\n",
    "\n",
    "### Provided classes\n",
    "- [`default_params()`](#default_params) this dictionary defines the default hyper-parameters\n",
    "- [`TransitionBatch`](#TransitionBatch) the basic class to summarize transitions and build an experience replay buffer.\n",
    "- [`Runner`](#Runner) interacts with one environment\n",
    "- [`MultiRunner`](#MultiRunner) runs multiple `Runner` in parallel\n",
    "- [`QController`](#QController) translates the model outputs into greedy actions\n",
    "- [`EpsilonGreedyController`](#EpsilonGreedyController) performs epsilon-greedy exploration\n",
    "- [`QLearner`](#QLearner) trains the model with Q-learning loss\n",
    "- [`Experiment`](#Experiment) encapsulates and executes a single experiment\n",
    "- [`QLearningExperiment`](#QLearningExperiment) performs online Q-learning\n",
    "\n",
    "\n",
    "\n",
    "### Exercises\n",
    "- [Q3.2a) Run online Q-learning](#q1)\n",
    "- [Q3.2b) Use a replay buffer in Q-learning](#q2)\n",
    "- [Q3.2c) Implement target networks wth hard updates](#q3)\n",
    "- [Q3.2d) Implement target networks with soft updates](#q4)\n",
    "- [Q3.2e) Implement Double Q-learning](#q5)\n",
    "- [Q3.2f) Run double Q-learning on MountainCar](#q6)\n",
    "- [Q3.2g) Run double Q-learning on LunarLander](#q7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pytorch and tools\n",
    "import torch as th\n",
    "from torch import Tensor, LongTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import numbers\n",
    "from datetime import datetime\n",
    "# Multi-threading\n",
    "import threading\n",
    "# Plotting\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "# Reinforcement learning\n",
    "import gym\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary defines the default hyper-paramerters that you will use in your experiments. <a id=default_params></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_params():\n",
    "    \"\"\" These are the default parameters used int eh framework. \"\"\"\n",
    "    return {# Debugging outputs and plotting during training\n",
    "            'plot_frequency': 10,             # plots a debug message avery n steps\n",
    "            'plot_train_samples': True,       # whether the x-axis is env.steps (True) or episodes (False)\n",
    "            'print_when_plot': True,          # prints debug message if True\n",
    "            'print_dots': False,              # prints dots for every gradient update\n",
    "            # Environment parameters\n",
    "            'env': 'CartPole-v0',             # the environment the agent is learning in\n",
    "            'run_steps': 0,                   # samples whole episodes if run_steps <= 0\n",
    "            'max_episode_length': 300,        # maximum number of steps per episode\n",
    "            # Runner parameters\n",
    "            'max_episodes': int(1E6),         # experiment stops after this many episodes\n",
    "            'max_steps': int(1E9),            # experiment stops after this many steps\n",
    "            'multi_runner': False,            # uses multiple runners if True\n",
    "            'parallel_environments': 4,       # number of parallel runners  (only if multi_runner==True)\n",
    "            # Exploration parameters\n",
    "            'epsilon_anneal_time': int(5E3),  # exploration anneals epsilon over these many steps\n",
    "            'epsilon_finish': 0.1,            # annealing stops at (and keeps) this epsilon\n",
    "            'epsilon_start': 1,               # annealing starts at this epsilon\n",
    "            # Optimization parameters\n",
    "            'lr': 5E-4,                       # learning rate of optimizer\n",
    "            'gamma': 0.99,                    # discount factor gamma\n",
    "            'batch_size': 2048,               # number of transitions in a mini-batch    \n",
    "            'grad_norm_clip': 1,              # gradent clipping if grad norm is larger than this \n",
    "            # DQN parameters\n",
    "            'replay_buffer_size': int(1E5),   # the number of transitions in the replay buffer\n",
    "            'use_last_episode': True,         # whether the last episode is always sampled from the buffer\n",
    "            'target_model': True,             # whether a target model is used in DQN\n",
    "            'target_update': 'soft',          # 'soft' target update or hard update by regular 'copy'\n",
    "            'target_update_interval': 10,     # interval for the 'copy' target update\n",
    "            'soft_target_update_param': 0.01, # update parameter for the 'soft' target update\n",
    "            'double_q': True,                 # whether DQN uses double Q-learning\n",
    "            'grad_repeats': 1,                # how many gradient updates / runner call\n",
    "            # Image input parameters          \n",
    "            'pixel_observations': False,      # use pixel observations (we will not use this feature here)\n",
    "            'pixel_resolution': (78, 78),     # scale image to this resoluton\n",
    "            'pixel_grayscale': True,          # convert image into grayscale\n",
    "            'pixel_add_last_obs': True,       # stacks 2 observations\n",
    "            'pixel_last_obs_delay': 3,        # delay between the two stacked observations\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransitionBatches` are dictionaries of variables, e.g. states or actions, that are saved in contiguous Tensors. <a id=TransitionBatch></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionBatch:\n",
    "    \"\"\" Simple implementation of a batchof transitionsm (or another dictionary-based tensor structure).\n",
    "        Read and write operations are thread-safe, but the iterator is not (you cannot interate\n",
    "        over the same TransitionBatch in two threads at the same time). \"\"\"\n",
    "    def __init__(self, max_size, transition_format, batch_size=32):\n",
    "        self.lock = threading.Lock()\n",
    "        self.indices = []\n",
    "        self.size = 0\n",
    "        self.first = 0\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dict = {}\n",
    "        for key, spec in transition_format.items():\n",
    "            self.dict[key] = th.zeros([max_size, *spec[0]], dtype=spec[1])\n",
    "            \n",
    "    def _clone_empty_batch(self, max_size=None, batch_size=None):\n",
    "        \"\"\" Clones this TransitionBatch without cloning the data. \"\"\"\n",
    "        max_size = self.max_size if max_size is None else max_size\n",
    "        batch_size = self.batch_size if batch_size is None else batch_size\n",
    "        return TransitionBatch(max_size=max_size, transition_format={}, batch_size=batch_size)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\" Access the TransitionBatch with the [] operator. Use as key either \n",
    "            - the string name of a variable to get the full tensor of that variable,\n",
    "            - a slice to get a time-slice over all variables in the batch,\n",
    "            - a LongTensor that selects a subset of indices for all variables in the batch. \"\"\"\n",
    "        # Return the entry of the transition called \"key\"\n",
    "        if isinstance(key, str): \n",
    "            return self.dict[key]\n",
    "        # Return a slice of the batch\n",
    "        if isinstance(key, slice):\n",
    "            key = slice(0 if key.start is None else key.start, self.size if key.stop is None else key.stop,\n",
    "                        1 if key.step is None else key.step)\n",
    "            self.lock.acquire()\n",
    "            try:\n",
    "                batch = self._clone_empty_batch()\n",
    "                batch.size = (key.stop - key.start) // key.step \n",
    "                for k, v in self.dict.items():\n",
    "                    batch.dict[k] = v[key] \n",
    "            finally: self.lock.release()\n",
    "            return batch\n",
    "        # Collect and return a set of transitions specified by the LongTensor \"key\" \n",
    "        if isinstance(key, th.Tensor):\n",
    "            self.lock.acquire()\n",
    "            try:\n",
    "                batch = self._clone_empty_batch(max_size=key.shape[0])\n",
    "                batch.size = key.shape[0]\n",
    "                for k, v in self.dict.items():\n",
    "                    key = key.view(batch.size, *[1 for _ in range(len(v.shape[1:]))])\n",
    "                    batch.dict[k] = v.gather(dim=0, index=key.expand(batch.size, *v.shape[1:]))\n",
    "            finally: self.lock.release()\n",
    "            return batch\n",
    "        return None\n",
    "    \n",
    "    def get_first(self):\n",
    "        \"\"\" Returns a batch of the oldest entries of all variables. \"\"\"\n",
    "        batch = self._clone_empty_batch(max_size=1)\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            batch.size = 1\n",
    "            for k, v in self.dict.items():\n",
    "                batch.dict[k] = v[self.first].unsqueeze(dim=0)\n",
    "        finally: self.lock.release()\n",
    "        return batch    \n",
    "    \n",
    "    def get_last(self):\n",
    "        \"\"\" Returns a batch of the newest entries of all variables. \"\"\"\n",
    "        batch = self._clone_empty_batch(max_size=1)\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            batch.size = 1\n",
    "            for k, v in self.dict.items():\n",
    "                batch.dict[k] = v[(self.first + self.size - 1) % self.size].unsqueeze(dim=0)\n",
    "        finally: self.lock.release()\n",
    "        return batch\n",
    "    \n",
    "    def add(self, trans:dict):\n",
    "        \"\"\" Adding transition dictionaries, which can contain Tensors of arbitrary length. \"\"\"\n",
    "        if isinstance(trans, TransitionBatch):\n",
    "            trans = trans.dict\n",
    "        # Add all data in the dict\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            n = 0\n",
    "            idx = None\n",
    "            for k, v in trans.items():\n",
    "                if idx is None:\n",
    "                    n = v.shape[0]\n",
    "                    idx = th.LongTensor([(self.first + self.size + i) % self.max_size for i in range(n)])\n",
    "                else:\n",
    "                    assert n == v.shape[0], 'all tensors in a transition need to have the same batch_size'\n",
    "                idx = idx.view(idx.shape[0], *[1 for _ in range(len(v.shape) - 1)])\n",
    "                self.dict[k].scatter_(dim=0, index=idx.expand_as(v), src=v)\n",
    "            # Increase the size (and handle overflow)\n",
    "            self.size += n\n",
    "            if self.size > self.max_size:\n",
    "                self.first = (self.first + n) % self.max_size\n",
    "                self.size = self.max_size\n",
    "        finally: self.lock.release()\n",
    "        return self\n",
    "            \n",
    "    def trim(self):\n",
    "        \"\"\" Reduces the length of the max_size to its actual size (in-place). Returns self. \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            for k, v in self.dict.items():\n",
    "                self.dict[k] = v[:self.size]\n",
    "            self.max_size = self.size\n",
    "        finally: self.lock.release()\n",
    "        return self\n",
    "    \n",
    "    def replace(self, batch, index=0):\n",
    "        \"\"\" Replaces parts of this batch with another batch (which must be smaller). \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            #assert batch.max_size <= self.max_size - index, \"Replacement is larger then target area in batch.\"\n",
    "            assert batch.size <= self.max_size - index, \"Replacement is larger then target area in batch.\"\n",
    "            for k, v in batch.dict.items():\n",
    "                if batch.size < batch.max_size:\n",
    "                    v = v[:batch.size]\n",
    "                self.dict[k][index:(index + batch.max_size)] = v    \n",
    "        finally: self.lock.release()\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\" Samples a random mini-batch from the batch. \"\"\"\n",
    "        return self[th.randint(high=self.size, size=(self.batch_size,1))]\n",
    "            \n",
    "    def __len__(self): \n",
    "        \"\"\" Returns the length of the batch. \"\"\"\n",
    "        return self.size\n",
    "    \n",
    "    def __iter__(self):  \n",
    "        \"\"\" Initializes an iterator over the batch. \"\"\"\n",
    "        self.indices = list(range(self.size))\n",
    "        np.random.shuffle(self.indices)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):  \n",
    "        \"\"\" Iterates through batch, returns list of contiguous tensors. \"\"\"\n",
    "        if len(self.indices) == 0: raise StopIteration\n",
    "        size = min(self.batch_size, len(self.indices))\n",
    "        batch = self[th.LongTensor(self.indices[-size:])]\n",
    "        self.indices = self.indices[:-size]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Runner` implements a simple runner class that uses a controller to interact with the environment by calling `run()` or `run_episode()`. <a id=Runner></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    \"\"\" Implements a simple single-thread runner class. \"\"\"\n",
    "    def __init__(self, controller, params={}, exploration_step=1):\n",
    "        self.env = gym.make(params.get('env', 'CartPole-v0'))\n",
    "        self.cont_actions = isinstance(self.env.action_space, gym.spaces.Box)\n",
    "        self.controller = controller\n",
    "        self.epi_len = params.get('max_episode_length', self.env._max_episode_steps)\n",
    "        self.gamma = params.get('gamma', 0.99)\n",
    "        self.use_pixels = params.get('pixel_observations', False)\n",
    "        if self.use_pixels:\n",
    "            self.grayscale = params.get('pixel_grayscale', True)\n",
    "            self.add_last_obs = params.get('pixel_add_last_obs', False)\n",
    "            self.last_obs_delay = params.get('pixel_last_obs_delay', 4)\n",
    "            n_colors = 1 if self.grayscale else 3\n",
    "            n_feats = n_colors * (2 if self.add_last_obs else 1)\n",
    "            resolution = params.get('pixel_resolution', (25, 25))\n",
    "            self.state_shape = (n_feats, *resolution)\n",
    "            self.last_observations = TransitionBatch(max_size=self.last_obs_delay, \n",
    "                                                     transition_format={'img': ((n_colors, *resolution), th.float32)})\n",
    "        else:\n",
    "            self.state_shape = self.env.observation_space.shape\n",
    "        # Set up current state and time step\n",
    "        self.sum_rewards = 0\n",
    "        self.state = None\n",
    "        self.time = 0\n",
    "        self._next_step() \n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\" Closes the underlying environment. Should always when ending an experiment. \"\"\"\n",
    "        self.env.close()\n",
    "    \n",
    "    def transition_format(self):\n",
    "        \"\"\" Returns the format of transtions: a dictionary of (shape, dtype) entries for each key. \"\"\"\n",
    "        return {'actions': ((1,), th.long),\n",
    "                'states': (self.state_shape, th.float32),\n",
    "                'next_states': (self.state_shape, th.float32),\n",
    "                'rewards': ((1,),  th.float32),\n",
    "                'dones': ((1,), th.bool),\n",
    "                'returns': ((1,), th.float32)}\n",
    "    \n",
    "    def _wrap_transition(self, s, a, r, ns, d):\n",
    "        \"\"\" Takes a transition and returns a corresponding dictionary. \"\"\"\n",
    "        trans = {}\n",
    "        form = self.transition_format()\n",
    "        for key, val in [('states', s), ('actions', a), ('rewards', r), ('next_states', ns), ('dones', d)]:\n",
    "            if not isinstance(val, th.Tensor): \n",
    "                if isinstance(val, numbers.Number) or isinstance(val, bool): val = [val]\n",
    "                val = th.tensor(val, dtype=form[key][1])\n",
    "            if len(val.shape) < len(form[key][0]) + 1: val = val.unsqueeze(dim=0)\n",
    "            trans[key] = val\n",
    "        return trans\n",
    "    \n",
    "    def _pixel_observation(self, reset=False):\n",
    "        \"\"\" Returns the pixel-observation fo the current state. Opens extra window for rendering. \"\"\"\n",
    "        img = self.env.render(mode='rgb_array')\n",
    "        img = cv2.resize(img, dsize=self.state_shape[1:], interpolation=cv2.INTER_CUBIC)\n",
    "        img = th.from_numpy(img.astype(np.float32) / 255).transpose(dim0=0, dim1=2).unsqueeze(dim=0)\n",
    "        if self.grayscale: img = img.mean(dim=1, keepdim=True)\n",
    "        if self.add_last_obs:\n",
    "            if reset: self.last_observations.size = 0\n",
    "            if self.last_observations.size < self.last_observations.max_size:\n",
    "                obs = img * 0\n",
    "            else:\n",
    "                obs = self.last_observations.get_first()['img'].clone()\n",
    "            self.last_observations.add({'img': img})\n",
    "            img = th.cat([obs, img], dim=1)\n",
    "        return img\n",
    "    \n",
    "    def _run_step(self, a):\n",
    "        \"\"\" Make a step in the environment (and update internal bookeeping) \"\"\"\n",
    "        ns, r, d, _ = self.env.step(a.item())\n",
    "        self.sum_rewards += r\n",
    "        if self.use_pixels: ns = self._pixel_observation()\n",
    "        return r, ns, d\n",
    "    \n",
    "    def _next_step(self, done=True, next_state=None):\n",
    "        \"\"\" Switch to the next time-step (and update internal bookeeping) \"\"\"\n",
    "        self.time = 0 if done else self.time + 1\n",
    "        if done:\n",
    "            self.sum_rewards = 0\n",
    "            self.state = self.env.reset()\n",
    "            if self.use_pixels: self.state = self._pixel_observation(reset=True)\n",
    "        else:\n",
    "            self.state = next_state\n",
    "        \n",
    "    \n",
    "    def run(self, n_steps, transition_buffer=None, trim=True, return_dict=None):\n",
    "        \"\"\" Runs n_steps in the environment and stores them in the trainsition_buffer (newly created if None).\n",
    "            If n_steps <= 0, stops at the end of an episode and optionally trins the transition_buffer.\n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        my_transition_buffer = TransitionBatch(n_steps if n_steps > 0 else self.epi_len, self.transition_format())\n",
    "        time, episode_start, episode_lengths, episode_rewards = 0, 0, [], []\n",
    "        max_steps = n_steps if n_steps > 0 else self.epi_len\n",
    "        for t in range(max_steps):\n",
    "            # One step in the envionment\n",
    "            a = self.controller.choose(self.state)\n",
    "            r, ns, d = self._run_step(a)\n",
    "            terminal = d and self.time < self.epi_len - 1\n",
    "            my_transition_buffer.add(self._wrap_transition(self.state, a, r, ns, terminal)) \n",
    "            if t == self.epi_len - 1: d = True\n",
    "            # Compute discounted returns if episode has ended or max_steps has been reached\n",
    "            if d or t == (max_steps - 1):\n",
    "                my_transition_buffer['returns'][t] = my_transition_buffer['rewards'][t]\n",
    "                for i in range(t - 1, episode_start - 1, -1):\n",
    "                    my_transition_buffer['returns'][i] = my_transition_buffer['rewards'][i] \\\n",
    "                                                         + self.gamma * my_transition_buffer['returns'][i + 1]\n",
    "                episode_start = t + 1\n",
    "            # Remember statistics and advance (potentially initilaizing a new episode)\n",
    "            if d:\n",
    "                episode_lengths.append(self.time + 1)\n",
    "                episode_rewards.append(self.sum_rewards)\n",
    "            self._next_step(done=d, next_state=ns)\n",
    "            time += 1\n",
    "            # If n_steps <= 0, we return after one episode (trimmed if specified)\n",
    "            if d and n_steps <= 0: \n",
    "                my_transition_buffer.trim()\n",
    "                break\n",
    "        # Add the sampled transitions to the given transition buffer\n",
    "        transition_buffer = my_transition_buffer if transition_buffer is None \\\n",
    "                            else transition_buffer.add(my_transition_buffer)\n",
    "        if trim: transition_buffer.trim()\n",
    "        # Return statistics (mean reward, mean length and environment steps)\n",
    "        if return_dict is None: return_dict = {}\n",
    "        return_dict.update({'buffer': transition_buffer,\n",
    "                            'episode_reward': None if len(episode_rewards) == 0 else np.mean(episode_rewards),\n",
    "                            'episode_length': None if len(episode_lengths) == 0 else np.mean(episode_lengths),\n",
    "                            'env_steps': time})\n",
    "        return return_dict\n",
    "        \n",
    "    def run_episode(self, transition_buffer=None, trim=True, return_dict=None):\n",
    "        \"\"\" Runs one episode in the environemnt. \n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        return self.run(0, transition_buffer, trim, return_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultiRunner` runs a number of `Runner` instances in parallel. <a id=MultiRunner></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRunner:\n",
    "    \"\"\" Simple class that runs multiple Runner objects in parallel and merges their outputs. \"\"\"\n",
    "    def __init__(self, controller, params={}):\n",
    "        self.workers = []\n",
    "        self.runners = []\n",
    "        n = params.get('parallel_environments', 1)\n",
    "        for _ in range(n):\n",
    "            self.runners.append(Runner(controller=controller, params=params))\n",
    "            \n",
    "    def transition_format(self):\n",
    "        \"\"\" Same transition-format as underlying Runners. \"\"\"\n",
    "        return self.runners[0].transition_format()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\" Closes the underlying environment. Should always when ending an experiment. \"\"\"\n",
    "        # Join all workers\n",
    "        for w in self.workers:\n",
    "            w.join()\n",
    "        # Exit all environments\n",
    "        for r in self.runners:\n",
    "            r.close()\n",
    "    \n",
    "    def fork(self, target, common_args=None, specific_args=None):\n",
    "        \"\"\" Executes the function \"target\" on all runners. \"common_args\" is a dictionary of \n",
    "            arguments that are passed to all runners, \"specific_args\" is a list of \n",
    "            dictionaries that contain individual parameters for each runner. \"\"\" \n",
    "        # Fork all runners\n",
    "        self.workers = []\n",
    "        for i, r in enumerate(self.runners):\n",
    "            r_args = [] if specific_args is None else [arg[i] for arg in specific_args]\n",
    "            self.workers.append(threading.Thread(target=target, args=(r, *common_args, *r_args)))\n",
    "            self.workers[-1].start()\n",
    "        # Join all runners\n",
    "        for w in self.workers:\n",
    "            w.join()\n",
    "    \n",
    "    def run(self, n_steps, transition_buffer=None, trim=True):\n",
    "        \"\"\" Runs n_steps, split amongst runners, and stores them in the trainsition_buffer (newly created if None).\n",
    "            If n_steps <= 0, stops at the end of an episode and optionally trims the transition_buffer.\n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        n_steps = n_steps // len(self.runners)\n",
    "        if transition_buffer is None:\n",
    "            buffer_len = len(self.runners) * (n_steps if n_steps > 0 else self.runners[0].epi_len)\n",
    "            transition_buffer = TransitionBatch(buffer_len, self.runners[0].transition_format())\n",
    "        return_dicts = [{} for _ in self.runners]\n",
    "        self.fork(target=Runner.run, common_args=(n_steps, transition_buffer, False), specific_args=(return_dicts,))\n",
    "        if trim: transition_buffer.trim()\n",
    "        rewards = [d['episode_reward'] for d in return_dicts if d['episode_reward'] is not None]\n",
    "        lengths = [d['episode_length'] for d in return_dicts if d['episode_reward'] is not None]\n",
    "        return {'buffer': transition_buffer, \n",
    "                'episode_reward': np.mean(rewards) if len(rewards) > 0 else None,\n",
    "                'episode_length': np.mean(lengths) if len(lengths) > 0 else None,\n",
    "                'env_steps': len(transition_buffer)}\n",
    "\n",
    "    def run_episode(self, transition_buffer=None, trim=True):\n",
    "        \"\"\" Runs one episode in the environemnt. \n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        return self.run(0, transition_buffer, trim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `QController` translates model responses into actions. Call `choose()` to select actions or `probabilities()` to get the probabilities with which the controller would choose the actions. <a id=QController></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QController:\n",
    "    \"\"\" Controller for Q-value functions, synchronizes the model calls. \"\"\"\n",
    "    def __init__(self, model, num_actions=None, params={}):\n",
    "        self.lock = threading.Lock()\n",
    "        self.num_actions = model[-1].out_features if num_actions is None else num_actions\n",
    "        self.model = model\n",
    "        \n",
    "    def copy(self):\n",
    "        \"\"\" Shallow copy of this controller that does not copy the model. \"\"\"\n",
    "        return QController(model=self.model, num_actions=self.num_actions)\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\" Returns a generator of the underlying model parameters. \"\"\"\n",
    "        return self.model.parameters()\n",
    "    \n",
    "    def sanitize_inputs(self, observation, **kwargs):\n",
    "        \"\"\" Casts numpy arrays as Tensors. \"\"\"\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = th.Tensor(observation).unsqueeze(dim=0)\n",
    "        return observation\n",
    "                \n",
    "    def choose(self, observation, **kwargs):\n",
    "        \"\"\" Returns the greedy actions the agent would choose when facing an \"observation\". \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try: \n",
    "            mx = self.model(self.sanitize_inputs(observation))\n",
    "            if mx.shape[-1] > self.num_actions: mx = mx[:, :self.num_actions]\n",
    "        finally: self.lock.release()\n",
    "        return th.max(mx, dim=-1)[1]\n",
    "\n",
    "    def probabilities(self, observation, **kwargs):\n",
    "        \"\"\" Returns the probabilities with which the agent would choose actions (here one-hot because greedy). \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try: \n",
    "            mx = self.model(self.sanitize_inputs(observation))\n",
    "            if mx.shape[-1] > self.num_actions: mx = mx[:, :self.num_actions]\n",
    "        finally: self.lock.release()\n",
    "        return th.zeros(*mx.shape).scatter_(dim=-1, index=th.max(mx, dim=-1)[1].unsqueeze(dim=-1), src=th.ones(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `EpsilonGreedyController` is a controller that autonomously anneals an expsilon greedy exploration strategy. <a id=EpsilonGreedyController></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyController:\n",
    "    \"\"\" A wrapper that makes any controller into an epsilon-greedy controller. \n",
    "        Keeps track of training-steps to decay exploration automatically. \"\"\"\n",
    "    def __init__(self, controller, params={}, exploration_step=1):\n",
    "        self.controller = controller\n",
    "        self.num_actions = controller.num_actions\n",
    "        self.max_eps = params.get('epsilon_start', 1.0)\n",
    "        self.min_eps = params.get('epsilon_finish', 0.05)\n",
    "        self.anneal_time = int(params.get('epsilon_anneal_time', 10000) / exploration_step)\n",
    "        self.num_decisions = 0\n",
    "    \n",
    "    def epsilon(self):\n",
    "        \"\"\" Returns current epsilon. \"\"\"\n",
    "        return max(1 - self.num_decisions / (self.anneal_time - 1), 0) \\\n",
    "                * (self.max_eps - self.min_eps) + self.min_eps\n",
    "    \n",
    "    def choose(self, observation, increase_counter=True, **kwargs):\n",
    "        \"\"\" Returns the (possibly random) actions the agent takes when faced with \"observation\".\n",
    "            Decays epsilon only when increase_counter=True\". \"\"\"\n",
    "        eps = self.epsilon()\n",
    "        if increase_counter: self.num_decisions += 1\n",
    "        if np.random.rand() < eps: \n",
    "            return th.randint(self.controller.num_actions, (1,), dtype=th.long)\n",
    "        else: \n",
    "            return self.controller.choose(observation, **kwargs)\n",
    "    \n",
    "    def probabilities(self, observation, **kwargs):\n",
    "        \"\"\" Returns the probabilities with which the agent would choose actions. \"\"\"\n",
    "        eps = self.epsilon()\n",
    "        return eps * th.ones(1, 1) / self.num_actions + \\\n",
    "               (1 - eps) * self.controller.probabilities(observation, **kwargs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `QLearner` is a learner class that performs Q-learning. At the moment this does not include target models or double Q-learning, which you will add in later exercises. <a id=QLearner></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\" A basic learner class that performs Q-learning train() steps. \"\"\"\n",
    "    def __init__(self, model, params={}):\n",
    "        self.model = model\n",
    "        self.all_parameters = list(model.parameters())\n",
    "        self.gamma = params.get('gamma', 0.99)\n",
    "        self.optimizer = th.optim.Adam(self.all_parameters, lr=params.get('lr', 5E-4))\n",
    "        self.criterion = th.nn.MSELoss() \n",
    "        self.grad_norm_clip = params.get('grad_norm_clip', 10)\n",
    "        self.target_model = None  # Target models are not yet implemented!\n",
    "    \n",
    "    def target_model_update(self):\n",
    "        \"\"\" This function updates the target network. No target network is implemented yet. \"\"\"\n",
    "        pass\n",
    "\n",
    "    def q_values(self, states, target=False):\n",
    "        \"\"\" Reutrns the Q-values of the given \"states\". \n",
    "            I supposed to use the target network if \"target=True\", but this is not implemented here. \"\"\"\n",
    "        return self.model(states)\n",
    "                    \n",
    "    def _current_values(self, batch):\n",
    "        \"\"\" Computes the Q-values of the 'states' and 'actions' of the given \"batch\". \"\"\"\n",
    "        qvalues = self.q_values(batch['states'])\n",
    "        return qvalues.gather(dim=-1, index=batch['actions'])\n",
    "        \n",
    "    def _next_state_values(self, batch):\n",
    "        \"\"\" Computes the Q-values of the 'next_states' of the given \"batch\".\n",
    "            Is greedy w.r.t. to current Q-network or target-network, depending on parameters. \"\"\"\n",
    "        with th.no_grad():  # Next state values do not need gradients in DQN\n",
    "            # Compute the next states values (with target or current network)\n",
    "            qvalues = self.q_values(batch['next_states'], target=True)\n",
    "            # Compute the maximum over Q-values\n",
    "            return qvalues.max(dim=-1, keepdim=True)[0]\n",
    "    \n",
    "    def train(self, batch):\n",
    "        \"\"\" Performs one gradient decent step of DQN. \"\"\"\n",
    "        self.model.train(True)\n",
    "        # Compute TD-loss\n",
    "        targets = batch['rewards'] + self.gamma * (~batch['dones'] * self._next_state_values(batch))\n",
    "        loss = self.criterion(self._current_values(batch), targets.detach())\n",
    "        # Backpropagate loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = th.nn.utils.clip_grad_norm_(self.all_parameters, self.grad_norm_clip)\n",
    "        self.optimizer.step()\n",
    "        # Update target network (if specified) and return loss\n",
    "        self.target_model_update()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `Experiment` is an abstract class that starts and maintains a single learning experiment (i.e. random seed). The experiment is started using `run()` and can be interrupted at any time using `close()`. Afterwards the experiment can be restarted at any time calling `run()` again. <a id=Experiment></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\" Abstract class of an experiment. Contains logging and plotting functionality.\"\"\"\n",
    "    def __init__(self, params, model, **kwargs):\n",
    "        self.params = params\n",
    "        self.plot_frequency = params.get('plot_frequency', 100)\n",
    "        self.plot_train_samples = params.get('plot_train_samples', True)\n",
    "        self.print_when_plot = params.get('print_when_plot', False)\n",
    "        self.print_dots = params.get('print_dots', False)\n",
    "        self.episode_returns = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_losses  = []\n",
    "        self.env_steps = []\n",
    "        self.total_run_time = 0.0\n",
    "        \n",
    "    def plot_training(self, update=False):\n",
    "        \"\"\" Plots logged training results. Use \"update=True\" if the plot is continuously updated\n",
    "            or use \"update=False\" if this is the final call (otherwise there will be double plotting). \"\"\" \n",
    "        # Smooth curves\n",
    "        window = max(int(len(self.episode_returns) / 50), 10)\n",
    "        if len(self.episode_losses) < window + 2: return\n",
    "        returns = np.convolve(self.episode_returns, np.ones(window)/window, 'valid')\n",
    "        lengths = np.convolve(self.episode_lengths, np.ones(window)/window, 'valid')\n",
    "        losses = np.convolve(self.episode_losses, np.ones(window)/window, 'valid')\n",
    "        env_steps = np.convolve(self.env_steps, np.ones(window)/window, 'valid')\n",
    "        # Determine x-axis based on samples or episodes\n",
    "        if self.plot_train_samples:\n",
    "            x_returns = env_steps\n",
    "            x_losses = env_steps[(len(env_steps) - len(losses)):]\n",
    "        else:\n",
    "            x_returns = [i + window for i in range(len(returns))]\n",
    "            x_losses = [i + len(returns) - len(losses) + window for i in range(len(losses))]\n",
    "        # Create plot\n",
    "        colors = ['b', 'g', 'r']\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(16, 4)\n",
    "        plt.clf()\n",
    "        # Plot the losses in the left subplot\n",
    "        pl.subplot(1, 3, 1)\n",
    "        pl.plot(env_steps, returns, colors[0])\n",
    "        pl.xlabel('environment steps' if self.plot_train_samples else 'episodes')\n",
    "        pl.ylabel('episode return')\n",
    "        # Plot the episode lengths in the middle subplot\n",
    "        ax = pl.subplot(1, 3, 2)\n",
    "        ax.plot(env_steps, lengths, colors[0])\n",
    "        ax.set_xlabel('environment steps' if self.plot_train_samples else 'episodes')\n",
    "        ax.set_ylabel('episode length')\n",
    "        # Plot the losses in the right subplot\n",
    "        ax = pl.subplot(1, 3, 3)\n",
    "        ax.plot(x_losses, losses, colors[0])\n",
    "        ax.set_xlabel('environment steps' if self.plot_train_samples else 'episodes')\n",
    "        ax.set_ylabel('loss')\n",
    "        # dynamic plot update\n",
    "        display.clear_output(wait=True)\n",
    "        if update:\n",
    "            display.display(pl.gcf())\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\" Frees all allocated runtime ressources, but allows to continue the experiment later. \n",
    "            Calling the run() method after close must be able to pick up the experiment where it was. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\" Starts (or continues) the experiment. \"\"\"\n",
    "        assert False, \"You need to extend the Expeirment class and override the method run(). \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QLearningExperiment` performs online Q-learning using `QLearner`. One can specify another learner, which you will do in later exercises. <a id=QLearningExperiment></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningExperiment (Experiment):\n",
    "    \"\"\" Experiment that perfoms DQN. You can provide your own learner. \"\"\"\n",
    "    def __init__(self, params, model, learner=None, **kwargs):\n",
    "        super().__init__(params, model, **kwargs)\n",
    "        self.max_episodes = params.get('max_episodes', int(1E6))\n",
    "        self.max_steps = params.get('max_steps', int(1E9))\n",
    "        self.run_steps = params.get('run_steps', 0)\n",
    "        self.grad_repeats = params.get('grad_repeats', 1)\n",
    "        self.controller = QController(model, num_actions=gym.make(params['env']).action_space.n, params=params)\n",
    "        self.controller = EpsilonGreedyController(controller=self.controller, params=params)\n",
    "        self.runner = MultiRunner(self.controller, params=params) if params.get('multi_runner', True) \\\n",
    "                      else Runner(self.controller, params=params)\n",
    "        self.learner = QLearner(model, params=params) if learner is None else learner\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Overrides Experiment.close(). \"\"\"\n",
    "        self.runner.close()\n",
    "        \n",
    "    def _learn_from_episode(self, episode):\n",
    "        \"\"\" This function uses the episode to train.\n",
    "            Although not implemented, one could also add the episode to a replay buffer here.\n",
    "            Returns the training loss for logging or None if train() was not called. \"\"\"\n",
    "        # Call train (params['grad_repeats']) times\n",
    "        total_loss = 0\n",
    "        for i in range(self.grad_repeats):\n",
    "            total_loss += self.learner.train(episode['buffer']) \n",
    "        return total_loss / self.grad_repeats\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\" Starts (or continues) the experiment. \"\"\"\n",
    "        # Plot previous results if they exist\n",
    "        if self.plot_frequency is not None and len(self.episode_losses) > 2:\n",
    "            self.plot_training(update=True)\n",
    "        # Start (or continue experiment)\n",
    "        env_steps = 0 if len(self.env_steps) == 0 else self.env_steps[-1]\n",
    "        for e in range(self.max_episodes):\n",
    "            begin_time = datetime.now()\n",
    "            # Run an episode (or parts of it)\n",
    "            if self.run_steps > 0:\n",
    "                episode = self.runner.run(n_steps=self.run_steps, trim=False)\n",
    "            else:\n",
    "                episode = self.runner.run_episode()\n",
    "            # Log the results\n",
    "            env_steps += episode['env_steps']\n",
    "            if episode['episode_length'] is not None:\n",
    "                self.episode_lengths.append(episode['episode_length'])\n",
    "                self.episode_returns.append(episode['episode_reward'])\n",
    "                self.env_steps.append(env_steps)\n",
    "            # Make one (or more) learning steps with the episode\n",
    "            loss = self._learn_from_episode(episode)\n",
    "            if loss is not None: self.episode_losses.append(loss)\n",
    "            self.total_run_time += (datetime.now() - begin_time).total_seconds()\n",
    "            # Quit if maximal number of environment steps is reached\n",
    "            if env_steps >= self.max_steps:\n",
    "                break\n",
    "            # Show intermediate results\n",
    "            if self.print_dots:\n",
    "                print('.', end='')\n",
    "            if self.plot_frequency is not None and (e + 1) % self.plot_frequency == 0 \\\n",
    "                                               and len(self.episode_losses) > 2:\n",
    "                self.plot_training(update=True)\n",
    "                if self.print_when_plot:\n",
    "                    print('Update %u, 100-epi-return %.4g +- %.3g, length %u, loss %g, run-time %g sec.' % \n",
    "                          (len(self.episode_returns), np.mean(self.episode_returns[-100:]), \n",
    "                           np.std(self.episode_returns[-100:]), np.mean(self.episode_lengths[-100:]), \n",
    "                           np.mean(self.episode_losses[-100:]), self.total_run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2a) Run the given online Q-learning algorithm <a id=q1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2a) Run the given Online Q-learning algorithm without target networks or experience replay\n",
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['max_steps'] = int(2E5)\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "experiment = QLearningExperiment(params, model, learner=QLearner(model, params=params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2b) Use a replay buffer in Q-learning <a id=q2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2b) Extend the QLearningExperiment class to use a replay buffer during training.\n",
    "#        If the replay buffer does not contain enough transitions for a mini-batch, training should be omitted.\n",
    "#        Bonus: make sure the last episode is always used for optimization (params['use_last_episode']=True)\n",
    "class DQNExperiment (QLearningExperiment):\n",
    "    \"\"\" Experiment that perfoms DQN. You can provide your own learner. \"\"\"\n",
    "    def __init__(self, params, model, learner=None, **kwargs):\n",
    "        super().__init__(params, model, learner=learner, **kwargs)\n",
    "        self.use_last_episode = params.get('use_last_episode', True)\n",
    "        self.replay_buffer = TransitionBatch(params.get('replay_buffer_size', int(1E5)), \n",
    "                                             self.runner.transition_format(), \n",
    "                                             batch_size=params.get('batch_size', 1024))\n",
    "\n",
    "    # YOUR CODE HERE!!!\n",
    "\n",
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['max_steps'] = int(2E5) \n",
    "params['use_last_episode'] = True\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "experiment = DQNExperiment(params, model)\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2c) Implement target networks with hard updates <a id=q3></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2c) Implement a target network with hard target updates (params['target_update'] = 'copy') \n",
    "#        every (params['target_update_interval'] = 10) gradient update steps.\n",
    "#        Make sure (params['target_model'] = False) maintains the old functionality.\n",
    "#        Hint: use (self.target_model.load_state_dict(self.model.state_dict())) to copy the model\n",
    "class QLearnerHardTarget (QLearner):\n",
    "    def __init__(self, model, params={}):\n",
    "        super().__init__(model, params)\n",
    "        self.target_update = params.get('target_update', 'hard')\n",
    "        self.target_update_interval = params.get('target_update_interval', 200)\n",
    "        self.target_update_calls = 0\n",
    "        if params.get('target_model', True):\n",
    "            self.target_model = deepcopy(model)\n",
    "            for p in self.target_model.parameters():\n",
    "                p.requires_grad = False\n",
    "        assert self.target_model is None or self.target_update == 'soft' or self.target_update == 'copy',\\\n",
    "            'If a target model is specified, it needs to be updated using the \"soft\" or \"copy\" options.'\n",
    "    \n",
    "    # YOUR CODE HERE!!!\n",
    "\n",
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['max_steps'] = int(2E5)\n",
    "params['target_model'] = True \n",
    "params['target_update'] = 'copy'\n",
    "params['target_update_interval'] = 10\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "experiment = DQNExperiment(params, model, learner=QLearnerHardTarget(model, params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2d) Implement target networks with soft updates <a id=q4></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2d) Implement a target network with soft target updates (params['target_update'] = 'soft'). \n",
    "#        The decay parameter is given by params['soft_target_update_param').\n",
    "#        Make sure all other parameters maintain the old functionality.\n",
    "#        Hint: you can iterate through model.parameters()\n",
    "class QLearnerSoftTarget (QLearnerHardTarget):\n",
    "    def __init__(self, model, params={}):\n",
    "        super().__init__(model, params)\n",
    "        self.target_update = params.get('target_update', 'soft')\n",
    "        self.soft_target_update_param = params.get('soft_target_update_param', 0.1)\n",
    "    \n",
    "    # YOUR CODE HERE!!!\n",
    "\n",
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['max_steps'] = int(2E5)\n",
    "params['target_model'] = True\n",
    "params['target_update'] = 'soft'\n",
    "params['soft_target_update_param'] = 0.01\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "experiment = DQNExperiment(params, model, learner=QLearnerSoftTarget(model, params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2e) Implement double Q-learning <a id=q5></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2e) Implement double Q-leanring when (params['double_q'] = True)\n",
    "class DoubleQLearner (QLearnerSoftTarget):\n",
    "    def __init__(self, model, params={}):\n",
    "        super().__init__(model, params)\n",
    "        self.double_q = params.get('double_q', True)\n",
    "    \n",
    "    # YOUR CODE HERE!!!\n",
    "\n",
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['max_steps'] = int(2E5)\n",
    "params['double_q'] = True\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "experiment = DQNExperiment(params, model, learner=DoubleQLearner(model, params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2f) Run double Q-learning on MountainCar <a id=q6></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2f) Run your implementation of DoubleQLearner on the MountainCar-v0 environment.\n",
    "#        Why does the agent not learn to solve the task?\n",
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['env'] = 'MountainCar-v0'\n",
    "params['max_steps'] = int(2E5)\n",
    "params['epsilon_anneal_time'] = int(1E5)  # exploration is probably important  \n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "experiment = DQNExperiment(params, model, learner=DoubleQLearner(model, params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2g) Run double Q-learning on LunarLander <a id=q7></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2g) Run your implementation of DoubleQLearner on the LunarLander-v2 environment for 2M time steps.\n",
    "#        Do you get similar curves for \"episode return\", \"epsode length\" and \"loss\" as in the lecture? \n",
    "\n",
    "# Executing this code-block defines a new experiment\n",
    "params = default_params()\n",
    "params['env'] = 'LunarLander-v2'\n",
    "params['max_steps'] = int(1E7)\n",
    "params['replay_buffer_size'] = int(1E6)   # This should probably be a bit larger than usual\n",
    "params['epsilon_anneal_time'] = int(5E5)  # You can play around with this parameter as well\n",
    "env = gym.make(params['env'])\n",
    "n_actions, state_dim = env.action_space.n, env.observation_space.shape[0]\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(), \n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(), \n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions))\n",
    "experiment = DQNExperiment(params, model, learner=DoubleQLearner(model, params))\n",
    "\n",
    "# Re-executing this code-block picks up the experiment where you left off\n",
    "try:\n",
    "    experiment.run()\n",
    "except KeyboardInterrupt:\n",
    "    experiment.close()\n",
    "experiment.plot_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}