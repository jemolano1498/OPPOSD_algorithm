# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xQMAhoTk6kzINNxu724QEDy3EnDU3x0I
"""

import matplotlib.pyplot as plt
import numpy as np
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
from torch.autograd import Variable
from tqdm.notebook import tqdm
from sklearn.linear_model import LinearRegression
import seaborn as sns
import random
from data_analysis.RunningEnv import  EnvWrapper

#number of episodes to run
NUM_EPISODES = 1000

#max steps per episode
MAX_STEPS = 10000

#device to run model on
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


memory=10000
store=[[dict()] for i in range(memory)]
#discount factor for future utilities
gamma=0.99
EPS_START = 0.9
EPS_END = 0.05
EPS_DECAY = 200

FRAMES = 500
pref_pace = 181
target_pace = pref_pace*1.1

# Network variables
input_size = 1 #number of features
hidden_size = 2 #number of features in hidden state
num_layers = 1 #number of stacked lstm layers

num_classes = 5 #number of output classes

def get_agent_score(network):
    env.reset()
    env.step(0)
    scores = []

    for _ in range(50):
        env.reset()
        state = env.step(0)[0]
        score = 0
        time_step = 0
        while env.steps < FRAMES:
            if network.name == "LSTM1":
                state = torch.from_numpy(state)
                state = state.type('torch.FloatTensor')
                hidden = network.init_hidden(1)
                output=network(state.unsqueeze(0), hidden)
                action=(output.argmax()).item()
            else:
                action, _ = select_action(network, state)
            new_state, reward, done = env.step(action)
            score += reward
            state = new_state
            if action > 0:
                time_step = time_step + env.times[action]
            else:
                time_step = time_step + 1
        scores.append(score)
    return np.array(scores).mean()

class model(nn.Module):
    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):
        super(model, self).__init__()
        self.name = "LSTM1"
        self.num_classes = num_classes  # number of classes
        self.num_layers = num_layers  # number of layers
        self.input_size = input_size  # input size
        self.hidden_size = hidden_size  # hidden state
        self.seq_length = seq_length  # sequence length

        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)  # lstm
        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected 1
        self.fc = nn.Linear(128, num_classes)  # fully connected last layer

        self.relu = nn.ReLU()

    def init_hidden(self, batch_size):
        return [torch.zeros(self.num_layers, batch_size, self.hidden_size), Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))]

    def forward(self, x, hidden):
        x = torch.reshape(x, (x.shape[0], 1, x.shape[1]))
        output, _ = self.lstm(x, (hidden[0], hidden[1]))  # lstm with input, hidden, and internal state
        output = output.view(-1, self.hidden_size)  # reshaping the data for Dense layer next
        out = self.relu(output)
        out = self.fc_1(out)  # first Dense
        out = self.relu(out)  # relu
        actions = self.fc(out)  # Final Output

        return actions

def addEpisode(ind,prev,curr,reward,act):
    if len(store[ind]) ==0:
        store[ind][0]={'prev':prev,'curr':curr,'reward':reward,'action':act}
    else:
        store[ind].append({'prev':prev,'curr':curr,'reward':reward,'action':act})

def trainNet(total_episodes):
    if total_episodes==0:
        return
    ep=random.randint(0,total_episodes-1)
    if len(store[ep]) < 8:
        return
    else:
        start=1
        length=random.randint(2,len(store[ep])-1)
        inp=[]
        target=[]
        rew=torch.Tensor(1,length-start)
        actions=torch.Tensor(1,length-start)

        for i in range(start,length,1):
            inp.append((store[ep][i]).get('prev'))
            target.append((store[ep][i]).get('curr'))
            rew[0][i-start]=store[ep][i].get('reward')
            actions[0][i-start]=store[ep][i].get('action')
        targets = torch.Tensor(target[0].shape[0],target[0].shape[1])
        torch.cat(target, out=targets)
        ccs=torch.Tensor(inp[0].shape[0],inp[0].shape[1])
        torch.cat(inp, out=ccs)
        hidden = policy.init_hidden(length-start)
        qvals= target_net(targets,hidden)
        actions=actions.type('torch.LongTensor')
        actions=actions.reshape(length-start,1)
        hidden = policy.init_hidden(length-start)
        inps=policy(ccs,hidden).gather(1,actions)
        p1,p2=qvals.detach().max(1)
        targ = torch.Tensor(1,p1.shape[0])
        for num in range(start,length,1):
            if num==len(store[ep])-1:
                targ[0][num-start]=rew[0][num-start]
            else:
                targ[0][num-start]=rew[0][num-start]+gamma*p1[num-start]
        optimizer.zero_grad()
        inps=inps.reshape(1,length-start)
        loss = criterion(inps,targ)
        loss.backward()
        for param in policy.parameters():
            param.grad.data.clamp(-1,1)
        optimizer.step()

def trainDRQN(episodes):
  scores_LSTM = []
  avg_scores_LSTM = []
  steps_done=0
  for i in tqdm(range(0,episodes,1)):
      print("Episode",i)
      env.reset()
      prev=env.step(0)[0]
      prev = torch.from_numpy(prev)
      prev = prev.type('torch.FloatTensor')
      done=False
      steps=0
      rew=0
      while env.steps < FRAMES:
          steps+=1
          hidden = policy.init_hidden(1)
          output=policy(prev.unsqueeze(0), hidden)
          action=(output.argmax()).item()
          rand= random.uniform(0,1)
          if rand < 0.05:
              action=random.randint(0,4)
          sc,reward,done = env.step(action)

          sc = torch.from_numpy(sc)
          sc = sc.type('torch.FloatTensor')
          reward = torch.from_numpy(reward)
          reward = reward.type('torch.FloatTensor')
          done = torch.from_numpy(done)
          done = done.type('torch.FloatTensor')

          rew=rew+reward
          addEpisode(i,prev.unsqueeze(0),sc.unsqueeze(0),reward,action)
          trainNet(i)
          prev=sc
          steps_done+=1
      scores_LSTM = np.append(scores_LSTM, rew)
      if i%10==0:
          target_net.load_state_dict(policy.state_dict())
      if (((i+1)%100) == 0):
          avg_scores_LSTM = np.append(avg_scores_LSTM, get_agent_score(policy))
  return scores_LSTM, avg_scores_LSTM

#Make environment
env = EnvWrapper(pref_pace, target_pace)
env.reset()

policy=model(num_classes, input_size, hidden_size, num_layers, 1)
target_net=model(num_classes, input_size, hidden_size, num_layers, 1)
target_net.load_state_dict(policy.state_dict())
target_net.eval()

optimizer = optim.RMSprop(policy.parameters())
criterion = F.smooth_l1_loss

scores_LSTM, avg_scores_LSTM = trainDRQN(100)

print(np.mean(avg_scores_LSTM))

# plt.plot(avg_scores_LSTM)

done = False
env.reset()
state = env.step(0)[0]
try_scores = []

for _ in tqdm(range(50)):
    env.reset()
    state = env.step(0)[0]
    done = False
    score = 0
    time_step = 0
    while env.steps < FRAMES:
        # env.render()
        state = torch.from_numpy(state)
        state = state.type('torch.FloatTensor')
        hidden = policy.init_hidden(1)
        output=policy(state.unsqueeze(0), hidden)
        action=(output.argmax()).item()
        new_state, reward, done = env.step(action)
        score += reward
        state = new_state
        if action > 0:
            time_step = time_step + env.times[action%5]
        else:
            time_step = time_step + 1
    try_scores.append(score)
np.array(try_scores).mean()

env.reset()
state = env.step(0)[0]

while env.steps < FRAMES:
    state = torch.from_numpy(state)
    state = state.type('torch.FloatTensor')
    hidden = policy.init_hidden(1)
    output=policy(state.unsqueeze(0), hidden)
    action=(output.argmax()).item()
    new_state, reward, done = env.step(action)
    # if reward < 0:
    #     print(action, state, new_state, reward)
    if (action != 0):
    #     # print(action, (state+1)*pref_pace, (new_state+1)*pref_pace, reward)
        print(action, state, new_state, reward)
    state = new_state

x = np.linspace(0, len(env.env_pacing), len(env.env_pacing))
plt.scatter(x[np.array(env.env_pacing)==1], np.array(env.pace)[np.array(env.env_pacing)==1], marker="x", label='Paced steps')
plt.scatter(x[np.array(env.env_pacing)==0], np.array(env.pace)[np.array(env.env_pacing)==0], marker="x", label='Not-paced steps')


# plt.scatter(x[np.array(env_pacing)==1], np.array(pace)[np.array(env_pacing)==1], marker="x", label='Paced steps')
# plt.scatter(x[np.array(env_pacing)==0], np.array(pace)[np.array(env_pacing)==0], marker="x", label='Not-paced steps')

# plt.scatter(x[np.array(pacing)==1], np.array(pacing)[np.array(pacing)==1]*181, color='r', marker="x")
plt.axhline(y=target_pace, color='k', linestyle='--', label='Target Pace')

plt.plot(x, env.state_traj, 'r-', linewidth=2)
plt.legend()
plt.show()

# print(np.sum(env.final_rewards))