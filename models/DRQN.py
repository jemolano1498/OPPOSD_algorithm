# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xQMAhoTk6kzINNxu724QEDy3EnDU3x0I
"""

import matplotlib.pyplot as plt
import numpy as np
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
from torch.autograd import Variable
from tqdm.notebook import tqdm
from sklearn.linear_model import LinearRegression
import seaborn as sns
import random

class MathModel:
    def __init__(self, logaritmic, z, grad=0):
        self.z = z
        self.logaritmic = logaritmic
        self.rand_comp = 0
        self.grad = 1
        if self.logaritmic:
            self.rand_comp = np.random.uniform(0.2, 1.5)
            if grad:
                self.grad = -1
        else:
            self.rand_comp = np.random.uniform(-5, 5)
            if grad:
                self.grad = 1 if np.random.random() < 0.5 else -1

    def calculate_x(self, x):
        if self.logaritmic:
            return self.grad * self.z[0] * np.log(self.rand_comp * x) + self.z[1]
        else:
            return (self.grad * (self.z[0] + (self.rand_comp * 1e-5))) * x + self.z[1]

    def inverse_value(self, y):
        if self.logaritmic:
            return np.exp(self.grad * (y - self.z[1]) / self.z[0]) / self.rand_comp
        else:
            return 0

    def adjust_intercept(self, new_val):
        self.z[1] = new_val

class PaceSimulator:
    def __init__(self):
        self.time_step = 0
        self.last_value = None
        self.current_model = None
        self.models = [None] * 4

    def calculate_model(self, pacing, percentage, grad=0):
        input_model = int(str(pacing) + str(1 if percentage > 0 else 0), 2)
        if input_model == 3:  # 1-1
            z = [0.00261429, 0.98637389]
            model = MathModel(1, z, grad)

        elif input_model == 2:  # 1-0
            z = [-0.00899492, 1.03965412]
            model = MathModel(1, z, grad)

        elif input_model == 1:  # 0-1
            z = [-3.17034405e-05, 9.86148032e-01]
            model = MathModel(0, z)

        else:  # 0-0
            z = [-3.19261596e-06, 1.01227226e00]
            model = MathModel(0, z, 0)

        return model

    def predict(self, pacing, pref_pace, target_pace):
        percentage = abs(pref_pace - target_pace) / pref_pace
        prediction_noise = np.random.uniform(0, 5e-3)
        if percentage > 0:
            pace_noise = np.random.uniform(target_pace - 4, target_pace+3)
        else:
            pace_noise = np.random.uniform(pref_pace - 5, pref_pace + 5)

        input_model = int(str(pacing) + str(1 if percentage > 0 else 0), 2)
        if self.time_step == 0:
            self.last_value = np.random.uniform(0.9, 1.08)
            self.time_step = self.time_step + 1
            return (self.last_value + prediction_noise) * pace_noise

        if input_model != self.current_model:
            if input_model == 1:  # 0-1
                self.current_model = 1
                self.models[self.current_model] = self.calculate_model(0, 0.1)
                self.models[self.current_model].adjust_intercept(self.last_value)
                self.time_step = 1

            elif input_model == 2:  # 1-0
                self.current_model = 2
                if self.last_value > 1:
                    self.models[self.current_model] = self.calculate_model(1, 0)
                else:
                    self.models[self.current_model] = self.calculate_model(1, 1)
                self.time_step = self.models[self.current_model].inverse_value(
                    self.last_value
                )

            elif input_model == 3:  # 1-1
                self.current_model = 3
                if self.last_value > 1:
                    self.models[self.current_model] = self.calculate_model(1, 0)
                else:
                    self.models[self.current_model] = self.calculate_model(1, 1)
                self.time_step = self.models[self.current_model].inverse_value(
                    self.last_value
                )

            else:  # 0-0
                self.current_model = 0
                self.models[self.current_model] = self.calculate_model(0, 0)
                self.models[self.current_model].adjust_intercept(self.last_value)
                self.time_step = 1

        self.time_step = self.time_step + 1
        self.last_value = self.models[self.current_model].calculate_x(self.time_step)

        return (self.last_value + prediction_noise) * pace_noise

class Timer:
    def __init__(self, time_limit=30):
        self.time_step = 0
        self.time_limit = time_limit

    def tick(self):
        self.time_step = self.time_step + 1
        if self.time_step == self.time_limit:
            self.time_step = 0

    def timer_on(self):
        if self.time_step == 0:
            return 0
        else:
            return 1

    def get_remaining_percentage(self):
        return 1 - ((self.time_limit - self.time_step) / self.time_limit)

class EwmaBiasState:
    def __init__(self, rho=0.95):
        self.rho = rho  # Rho value for smoothing
        self.s_prev = 0  # Initial value ewma value
        self.timestep = 0
        self.s_cur = 0
        self.s_cur_bc = 0

    def get_next_state(self, input):
        self.s_cur = self.rho * self.s_prev + (1 - self.rho) * input
        self.s_cur_bc = (self.s_cur) / (1 - math.pow(self.rho, self.timestep + 1))
        self.s_prev = self.s_cur
        self.timestep = self.timestep + 1

        return self.s_cur_bc

class RunningEnv:
    def __init__(self, pref_pace, time_limit=30):
        self.state = EwmaBiasState()
        self.simulator = PaceSimulator()
        self.pace_active = 0
        self.pref_pace = pref_pace
        self.time_limit = time_limit
        self.current_timer = Timer(self.time_limit)

    def step(self, action, target_pace):
        done = 0
        if action == 1:
            self.current_timer = Timer(self.time_limit)
            self.current_timer.tick()
            self.pace_active = 1
            pacing_value = 1
        else:
            self.pace_active = 0
            pacing_value = 0
            if self.current_timer.timer_on():
                pacing_value = 1
                self.current_timer.tick()

        current_pace = self.simulator.predict(pacing_value, self.pref_pace, target_pace)
        avg_pace = self.state.get_next_state(current_pace)
        new_state = (avg_pace / target_pace) - 1

        reward = self.get_distance_reward(target_pace, avg_pace)

        return np.array([current_pace]), np.array([new_state], dtype=float), np.array([reward],
                                                                                         dtype=float), np.array(
            [pacing_value]), np.array([done])

    def get_distance_reward(self, target_pace, current_pace):
        reward = 0
        if abs(target_pace - current_pace) > 5:
            reward = -10 * (abs(target_pace - current_pace) - 5)
        if abs(target_pace - current_pace) < 4:
            reward = -5
        if abs(target_pace - current_pace) < 2:
            reward = 10
        if abs(target_pace - current_pace) < 1:
            reward = 20
        return reward

    def reset(self):
        self.state = EwmaBiasState()
        self.simulator = PaceSimulator()
        self.pace_active = 0
        self.current_timer = Timer(self.time_limit)

class EnvWrapper:
    def __init__(self, pref_pace, target_pace):
        self.times = [0, 20, 25, 30, 40]
        self.pref_pace = pref_pace
        self.target_pace = target_pace
        self.running_env = RunningEnv(pref_pace, 1)

        self.state_traj = np.empty(0)
        self.pace = np.empty(0)
        self.final_rewards = np.empty(0)
        self.action_rewards = np.empty(0)
        self.state_rewards = np.empty(0)
        self.env_pacing = np.empty(0)

        self.steps = 0

    def step(self, action):
        total_reward = 0
        action_reward = 0
        state_reward = 0
        new_state = 0
        for i in range(self.times[action]):
            current_pace, new_state, reward, real_pacing, _ = self.running_env.step(1, self.target_pace)
            self.state_traj = np.append(self.state_traj, (new_state[0] + 1) * self.target_pace)
            self.pace = np.append(self.pace, current_pace)
            self.action_rewards = np.append(self.action_rewards, -1)
            self.state_rewards = np.append(self.state_rewards, reward)
            self.final_rewards = np.append(self.final_rewards, -1)
            self.env_pacing = np.append(self.env_pacing, real_pacing)
            self.steps = self.steps + 1
            state_reward = state_reward + reward
            action_reward = action_reward - 1
        if action == 0:
            current_pace, new_state, state_reward, real_pacing, done = self.running_env.step(0, self.target_pace)
            self.state_traj = np.append(self.state_traj, (new_state[0] + 1) * self.target_pace)
            self.pace = np.append(self.pace, current_pace)
            self.action_rewards = np.append(self.action_rewards, 0)
            self.state_rewards = np.append(self.state_rewards, state_reward)
            self.final_rewards = np.append(self.final_rewards, state_reward)
            self.env_pacing = np.append(self.env_pacing, real_pacing)
            self.steps = self.steps + 1
            return new_state, state_reward, done, state_reward, np.array([0])
        total_reward = state_reward + action_reward
        # self.total_rewards = np.append(self.total_rewards, state_reward)
        return new_state, np.array([action_reward], dtype=float), np.array([0]), np.array([state_reward], dtype=float), np.array([action_reward], dtype=float)

    def reset(self):
        self.running_env.reset()
        self.state_traj = np.empty(0)
        self.pace = np.empty(0)
        self.final_rewards = np.empty(0)
        self.action_rewards = np.empty(0)
        self.state_rewards = np.empty(0)
        self.env_pacing = np.empty(0)
        self.steps = 0

#number of episodes to run
NUM_EPISODES = 1000

#max steps per episode
MAX_STEPS = 10000

#device to run model on
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


memory=10000
store=[[dict()] for i in range(memory)]
#discount factor for future utilities
gamma=0.99
EPS_START = 0.9
EPS_END = 0.05
EPS_DECAY = 200

FRAMES = 500
pref_pace = 181
target_pace = pref_pace*1.1

# Network variables
input_size = 1 #number of features
hidden_size = 2 #number of features in hidden state
num_layers = 1 #number of stacked lstm layers

num_classes = 5 #number of output classes

def get_agent_score(network):
    env.reset()
    env.step(0)
    scores = []

    for _ in range(50):
        env.reset()
        state = env.step(0)[0]
        score = 0
        time_step = 0
        while env.steps < FRAMES:
            if network.name == "LSTM1":
                state = torch.from_numpy(state)
                state = state.type('torch.FloatTensor')
                hidden = network.init_hidden(1)
                output=network(state.unsqueeze(0), hidden)
                action=(output.argmax()).item()
            else:
                action, _ = select_action(network, state)
            new_state, reward, done, _, _ = env.step(action)
            score += reward
            state = new_state
            if action > 0:
                time_step = time_step + env.times[action]
            else:
                time_step = time_step + 1
        scores.append(score)
    return np.array(scores).mean()

class model(nn.Module):
    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):
        super(model, self).__init__()
        self.name = "LSTM1"
        self.num_classes = num_classes  # number of classes
        self.num_layers = num_layers  # number of layers
        self.input_size = input_size  # input size
        self.hidden_size = hidden_size  # hidden state
        self.seq_length = seq_length  # sequence length

        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)  # lstm
        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected 1
        self.fc = nn.Linear(128, num_classes)  # fully connected last layer

        self.relu = nn.ReLU()

    def init_hidden(self, batch_size):
        return [torch.zeros(self.num_layers, batch_size, self.hidden_size), Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))]

    def forward(self, x, hidden):
        x = torch.reshape(x, (x.shape[0], 1, x.shape[1]))
        output, _ = self.lstm(x, (hidden[0], hidden[1]))  # lstm with input, hidden, and internal state
        output = output.view(-1, self.hidden_size)  # reshaping the data for Dense layer next
        # output = torch.reshape(output, (output.shape[0], 1))  # reshaping the data for Dense layer next
        out = self.relu(output)
        out = self.fc_1(out)  # first Dense
        out = self.relu(out)  # relu
        actions = self.fc(out)  # Final Output

        return actions

def addEpisode(ind,prev,curr,reward,act):
    if len(store[ind]) ==0:
        store[ind][0]={'prev':prev,'curr':curr,'reward':reward,'action':act}
    else:
        store[ind].append({'prev':prev,'curr':curr,'reward':reward,'action':act})

def trainNet(total_episodes):
    if total_episodes==0:
        return
    ep=random.randint(0,total_episodes-1)
    if len(store[ep]) < 8:
        return
    else:
        start=1
        length=random.randint(2,len(store[ep])-1)
        inp=[]
        target=[]
        rew=torch.Tensor(1,length-start)
        actions=torch.Tensor(1,length-start)

        for i in range(start,length,1):
            inp.append((store[ep][i]).get('prev'))
            target.append((store[ep][i]).get('curr'))
            rew[0][i-start]=store[ep][i].get('reward')
            actions[0][i-start]=store[ep][i].get('action')
        targets = torch.Tensor(target[0].shape[0],target[0].shape[1])
        torch.cat(target, out=targets)
        ccs=torch.Tensor(inp[0].shape[0],inp[0].shape[1])
        torch.cat(inp, out=ccs)
        hidden = policy.init_hidden(length-start)
        qvals= target_net(targets,hidden)
        actions=actions.type('torch.LongTensor')
        actions=actions.reshape(length-start,1)
        hidden = policy.init_hidden(length-start)
        inps=policy(ccs,hidden).gather(1,actions)
        p1,p2=qvals.detach().max(1)
        targ = torch.Tensor(1,p1.shape[0])
        for num in range(start,length,1):
            if num==len(store[ep])-1:
                targ[0][num-start]=rew[0][num-start]
            else:
                targ[0][num-start]=rew[0][num-start]+gamma*p1[num-start]
        optimizer.zero_grad()
        inps=inps.reshape(1,length-start)
        loss = criterion(inps,targ)
        loss.backward()
        for param in policy.parameters():
            param.grad.data.clamp(-1,1)
        optimizer.step()

def trainDRQN(episodes):
  scores_LSTM = []
  avg_scores_LSTM = []
  steps_done=0
  for i in tqdm(range(0,episodes,1)):
      # print("Episode",i)
      env.reset()
      prev=env.step(0)[0]
      prev = torch.from_numpy(prev)
      prev = prev.type('torch.FloatTensor')
      done=False
      steps=0
      rew=0
      while env.steps < FRAMES:
          steps+=1
          hidden = policy.init_hidden(1)
          output=policy(prev.unsqueeze(0), hidden)
          action=(output.argmax()).item()
          rand= random.uniform(0,1)
          if rand < 0.05:
              action=random.randint(0,4)
          sc,reward,done, _, _ = env.step(action)

          sc = torch.from_numpy(sc)
          sc = sc.type('torch.FloatTensor')
          reward = torch.from_numpy(reward)
          reward = reward.type('torch.FloatTensor')
          done = torch.from_numpy(done)
          done = done.type('torch.FloatTensor')

          rew=rew+reward
          addEpisode(i,prev.unsqueeze(0),sc.unsqueeze(0),reward,action)
          trainNet(i)
          prev=sc
          steps_done+=1
      scores_LSTM = np.append(scores_LSTM, rew)
      if i%10==0:
          target_net.load_state_dict(policy.state_dict())
      if (((i+1)%150) == 0):
          avg_scores_LSTM = np.append(avg_scores_LSTM, get_agent_score(policy))
  return scores_LSTM, avg_scores_LSTM

#Make environment
env = EnvWrapper(pref_pace, target_pace)
env.reset()

policy=model(num_classes, input_size, hidden_size, num_layers, 1)
target_net=model(num_classes, input_size, hidden_size, num_layers, 1)
target_net.load_state_dict(policy.state_dict())
target_net.eval()

optimizer = optim.RMSprop(policy.parameters())
criterion = F.smooth_l1_loss

scores_LSTM, avg_scores_LSTM = trainDRQN(300)

sns.set()

plt.plot(avg_scores_LSTM, color='r')