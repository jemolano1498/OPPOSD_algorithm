{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "# Pytorch and tools\n",
    "import torch as th\n",
    "import numpy as np\n",
    "# Multi-threading\n",
    "import threading\n",
    "import numbers\n",
    "from RunningEnv import EnvWrapper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "pref_pace = 181\n",
    "target_pace = pref_pace * 1.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "def default_params():\n",
    "    \"\"\" These are the default parameters used int eh framework. \"\"\"\n",
    "    return {  # Debugging outputs and plotting during training\n",
    "        'plot_frequency': 10,  # plots a debug message avery n steps\n",
    "        'plot_train_samples': True,  # whether the x-axis is env.steps (True) or episodes (False)\n",
    "        'print_when_plot': True,  # prints debug message if True\n",
    "        'print_dots': False,  # prints dots for every gradient update\n",
    "        # Environment parameters\n",
    "        'env': 'CartPole-v0',  # the environment the agent is learning in\n",
    "        'run_steps': 0,  # samples whole episodes if run_steps <= 0\n",
    "        'max_episode_length': 500,  # maximum number of steps per episode\n",
    "        # Runner parameters\n",
    "        'max_episodes': int(1E6),  # experiment stops after this many episodes\n",
    "        'max_batch_episodes': int(1E6),  # experiment stops after this many batch\n",
    "        'max_steps': int(1E9),  # experiment stops after this many steps\n",
    "        'multi_runner': False,  # uses multiple runners if True\n",
    "        'parallel_environments': 4,  # number of parallel runners  (only if multi_runner==True)\n",
    "        # Exploration parameters\n",
    "        'epsilon_anneal_time': int(5E3),  # exploration anneals epsilon over these many steps\n",
    "        'epsilon_finish': 0.1,  # annealing stops at (and keeps) this epsilon\n",
    "        'epsilon_start': 1,  # annealing starts at this epsilon\n",
    "        # Optimization parameters\n",
    "        'lr': 1E-4,  # 5E-4,                       # learning rate of optimizer\n",
    "        'gamma': 0.99,  # discount factor gamma\n",
    "        'batch_size': 2048,  # number of transitions in a mini-batch\n",
    "        'grad_norm_clip': 1,  # gradent clipping if grad norm is larger than this\n",
    "        # DQN parameters\n",
    "        'replay_buffer_size': int(1E5),  # the number of transitions in the replay buffer\n",
    "        'use_last_episode': True,  # whether the last episode is always sampled from the buffer\n",
    "        'target_model': True,  # whether a target model is used in DQN\n",
    "        'target_update': 'soft',  # 'soft' target update or hard update by regular 'copy'\n",
    "        'target_update_interval': 10,  # interval for the 'copy' target update\n",
    "        'soft_target_update_param': 0.01,  # update parameter for the 'soft' target update\n",
    "        'double_q': True,  # whether DQN uses double Q-learning\n",
    "        'grad_repeats': 1,  # how many gradient updates / runner call\n",
    "\n",
    "        # Runners env\n",
    "        'pref_pace': 181,  # Athlete's preferred pace\n",
    "        'target_pace': pref_pace * 1.1,  # Athlete's target pace\n",
    "        'states_shape': (1,),  # Amount of states\n",
    "        'num_actions': 5,  # Possible actions\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpisodeBatch:\n",
    "    \"\"\" Simple implementation of a batchof transitionsm (or another dictionary-based tensor structure).\n",
    "        Read and write operations are thread-safe, but the iterator is not (you cannot interate\n",
    "        over the same TransitionBatch in two threads at the same time). \"\"\"\n",
    "\n",
    "    def __init__(self, max_size, episode_format, batch_size=32):\n",
    "        self.lock = threading.Lock()\n",
    "        self.indices = []\n",
    "        self.size = 0\n",
    "        self.first = 0\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dict = {}\n",
    "        for key, spec in episode_format.items():\n",
    "            self.dict[key] = th.zeros([max_size, *spec[0]], dtype=spec[1])\n",
    "\n",
    "    def _clone_empty_batch(self, max_size=None, batch_size=None):\n",
    "        \"\"\" Clones this TransitionBatch without cloning the data. \"\"\"\n",
    "        max_size = self.max_size if max_size is None else max_size\n",
    "        batch_size = self.batch_size if batch_size is None else batch_size\n",
    "        return EpisodeBatch(max_size=max_size, episode_format={}, batch_size=batch_size)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\" Access the TransitionBatch with the [] operator. Use as key either\n",
    "            - the string name of a variable to get the full tensor of that variable,\n",
    "            - a slice to get a time-slice over all variables in the batch,\n",
    "            - a LongTensor that selects a subset of indices for all variables in the batch. \"\"\"\n",
    "        # Return the entry of the transition called \"key\"\n",
    "        if isinstance(key, str):\n",
    "            return self.dict[key]\n",
    "        # Return a slice of the batch\n",
    "        if isinstance(key, slice):\n",
    "            key = slice(0 if key.start is None else key.start, self.size if key.stop is None else key.stop,\n",
    "                        1 if key.step is None else key.step)\n",
    "            self.lock.acquire()\n",
    "            try:\n",
    "                batch = self._clone_empty_batch()\n",
    "                batch.size = (key.stop - key.start) // key.step\n",
    "                for k, v in self.dict.items():\n",
    "                    batch.dict[k] = v[key]\n",
    "            finally:\n",
    "                self.lock.release()\n",
    "            return batch\n",
    "        # Collect and return a set of transitions specified by the LongTensor \"key\"\n",
    "        if isinstance(key, th.Tensor):\n",
    "            self.lock.acquire()\n",
    "            try:\n",
    "                batch = self._clone_empty_batch(max_size=key.shape[0])\n",
    "                batch.size = key.shape[0]\n",
    "                for k, v in self.dict.items():\n",
    "                    key = key.view(batch.size, *[1 for _ in range(len(v.shape[1:]))])\n",
    "                    batch.dict[k] = v.gather(dim=0, index=key.expand(batch.size, *v.shape[1:]))\n",
    "            finally:\n",
    "                self.lock.release()\n",
    "            return batch\n",
    "        return None\n",
    "\n",
    "    def get_first(self):\n",
    "        \"\"\" Returns a batch of the oldest entries of all variables. \"\"\"\n",
    "        batch = self._clone_empty_batch(max_size=1)\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            batch.size = 1\n",
    "            for k, v in self.dict.items():\n",
    "                batch.dict[k] = v[self.first].unsqueeze(dim=0)\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return batch\n",
    "\n",
    "    def get_last(self):\n",
    "        \"\"\" Returns a batch of the newest entries of all variables. \"\"\"\n",
    "        batch = self._clone_empty_batch(max_size=1)\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            batch.size = 1\n",
    "            for k, v in self.dict.items():\n",
    "                batch.dict[k] = v[(self.first + self.size - 1) % self.size].unsqueeze(dim=0)\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return batch\n",
    "\n",
    "    def add(self, trans: dict):\n",
    "        \"\"\" Adding transition dictionaries, which can contain Tensors of arbitrary length. \"\"\"\n",
    "        if isinstance(trans, EpisodeBatch):\n",
    "            trans = trans.dict\n",
    "        # Add all data in the dict\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            n = 0\n",
    "            idx = None\n",
    "            for k, v in trans.items():\n",
    "                if idx is None:\n",
    "                    n = v.shape[0]\n",
    "                    idx = th.LongTensor([(self.first + self.size + i) % self.max_size for i in range(n)])\n",
    "                else:\n",
    "                    assert n == v.shape[0], 'all tensors in a transition need to have the same batch_size'\n",
    "                idx = idx.view(idx.shape[0], *[1 for _ in range(len(v.shape) - 1)])\n",
    "                self.dict[k].scatter_(dim=0, index=idx.expand_as(v), src=v)\n",
    "            # Increase the size (and handle overflow)\n",
    "            self.size += n\n",
    "            if self.size > self.max_size:\n",
    "                self.first = (self.first + n) % self.max_size\n",
    "                self.size = self.max_size\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return self\n",
    "\n",
    "    def trim(self):\n",
    "        \"\"\" Reduces the length of the max_size to its actual size (in-place). Returns self. \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            for k, v in self.dict.items():\n",
    "                self.dict[k] = v[:self.size]\n",
    "            self.max_size = self.size\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return self\n",
    "\n",
    "    def replace(self, batch, index=0):\n",
    "        \"\"\" Replaces parts of this batch with another batch (which must be smaller). \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            # assert batch.max_size <= self.max_size - index, \"Replacement is larger then target area in batch.\"\n",
    "            assert batch.size <= self.max_size - index, \"Replacement is larger then target area in batch.\"\n",
    "            for k, v in batch.dict.items():\n",
    "                if batch.size < batch.max_size:\n",
    "                    v = v[:batch.size]\n",
    "                self.dict[k][index:(index + batch.max_size)] = v\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\" Samples a random mini-batch from the batch. \"\"\"\n",
    "        return self[th.randint(high=self.size, size=(self.batch_size, 1))]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the length of the batch. \"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Initializes an iterator over the batch. \"\"\"\n",
    "        self.indices = list(range(self.size))\n",
    "        np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\" Iterates through batch, returns list of contiguous tensors. \"\"\"\n",
    "        if len(self.indices) == 0: raise StopIteration\n",
    "        size = min(self.batch_size, len(self.indices))\n",
    "        batch = self[th.LongTensor(self.indices[-size:])]\n",
    "        self.indices = self.indices[:-size]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "memory=10000\n",
    "store=[[dict()] for i in range(memory)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def addEpisode(ind,prev,curr,reward,act):\n",
    "    if not(store[ind][0]):\n",
    "        store[ind][0]={'prev':prev,'curr':curr,'reward':reward,'action':act}\n",
    "    else:\n",
    "        store[ind].append({'prev':prev,'curr':curr,'reward':reward,'action':act})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "class EpisodesRunner:\n",
    "    \"\"\" Implements a simple single-thread runner class. \"\"\"\n",
    "\n",
    "    def __init__(self, controller, params={}, exploration_step=1):\n",
    "        self.env = EnvWrapper(params.get('pref_pace'), params.get('target_pace'))\n",
    "        self.cont_actions = False\n",
    "        self.controller = controller\n",
    "        self.epi_len = params.get('max_episode_length', 500)\n",
    "        self.gamma = params.get('gamma', 0.99)\n",
    "\n",
    "        # MANUALLY SET\n",
    "        self.state_shape = params.get('states_shape')\n",
    "        # Set up current state and time step\n",
    "        self.sum_rewards = 0\n",
    "        self.state = None\n",
    "        self.time = 0\n",
    "        self._next_step()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\" Closes the underlying environment. Should always when ending an experiment. \"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "    def transition_format(self):\n",
    "        \"\"\" Returns the format of transtions: a dictionary of (shape, dtype) entries for each key. \"\"\"\n",
    "        return {'episode': {'actions': ((1,), th.long),\n",
    "                            'states': (self.state_shape, th.float32),\n",
    "                            'next_states': (self.state_shape, th.float32),\n",
    "                            'rewards': ((1,), th.float32),\n",
    "                            'dones': ((1,), th.bool),\n",
    "                            'returns': ((1,), th.float32)}}\n",
    "\n",
    "    def _wrap_transition(self, ep, s, a, r, ns, d):\n",
    "        \"\"\" Takes a transition and returns a corresponding dictionary. \"\"\"\n",
    "        trans = {ep:{}}\n",
    "        form = self.transition_format()\n",
    "        for key, val in [('states', s), ('actions', a), ('rewards', r), ('next_states', ns), ('dones', d)]:\n",
    "            if not isinstance(val, th.Tensor):\n",
    "                if isinstance(val, numbers.Number) or isinstance(val, bool): val = [val]\n",
    "                val = th.tensor(val, dtype=form[key][1])\n",
    "            if len(val.shape) < len(form[key][0]) + 1: val = val.unsqueeze(dim=0)\n",
    "            trans[key] = val\n",
    "        return trans\n",
    "\n",
    "    def _run_step(self, a):\n",
    "        \"\"\" Make a step in the environment (and update internal bookeeping) \"\"\"\n",
    "        ns, r, d = self.env.step(a.item())\n",
    "        self.sum_rewards += r\n",
    "        return r, ns, d\n",
    "\n",
    "    def _next_step(self, done=True, next_state=None):\n",
    "        \"\"\" Switch to the next time-step (and update internal bookeeping) \"\"\"\n",
    "        self.time = 0 if done else self.time + 1\n",
    "        if done:\n",
    "            self.sum_rewards = 0\n",
    "            self.state = self.env.reset()\n",
    "        else:\n",
    "            self.state = next_state\n",
    "\n",
    "    def run(self, n_steps, transition_buffer=None, trim=True, return_dict=None):\n",
    "        \"\"\" Runs n_steps in the environment and stores them in the trainsition_buffer (newly created if None).\n",
    "            If n_steps <= 0, stops at the end of an episode and optionally trims the transition_buffer.\n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        my_transition_buffer = EpisodeBatch(n_steps if n_steps > 0 else self.epi_len, self.transition_format())\n",
    "        time, episode_start, episode_lengths, episode_rewards = 0, 0, [], []\n",
    "        max_steps = n_steps if n_steps > 0 else self.epi_len\n",
    "        i = 0\n",
    "        for t in range(max_steps):\n",
    "            # One step in the envionment\n",
    "            a = self.controller.choose(self.state)\n",
    "            r, ns, d = self._run_step(a)\n",
    "            terminal = d and self.time < self.epi_len - 1\n",
    "            my_transition_buffer.add(self._wrap_transition(self.state, a, r, ns, terminal))\n",
    "            addEpisode(i,self.state,ns,r,a)\n",
    "            if t == self.epi_len - 1: d = True\n",
    "            # Compute discounted returns if episode has ended or max_steps has been reached\n",
    "            if d or t == (max_steps - 1):\n",
    "                my_transition_buffer['returns'][t] = my_transition_buffer['rewards'][t]\n",
    "                for i in range(t - 1, episode_start - 1, -1):\n",
    "                    my_transition_buffer['returns'][i] = my_transition_buffer['rewards'][i] \\\n",
    "                                                         + self.gamma * my_transition_buffer['returns'][i + 1]\n",
    "                episode_start = t + 1\n",
    "            # Remember statistics and advance (potentially initializing a new episode)\n",
    "            if d:\n",
    "                i += 1\n",
    "                episode_lengths.append(self.time + 1)\n",
    "                episode_rewards.append(self.sum_rewards)\n",
    "            self._next_step(done=d, next_state=ns)\n",
    "            time += 1\n",
    "            # If n_steps <= 0, we return after one episode (trimmed if specified)\n",
    "            if d and n_steps <= 0:\n",
    "                my_transition_buffer.trim()\n",
    "                break\n",
    "        # Add the sampled transitions to the given transition buffer\n",
    "        transition_buffer = my_transition_buffer if transition_buffer is None \\\n",
    "            else transition_buffer.add(my_transition_buffer)\n",
    "        if trim: transition_buffer.trim()\n",
    "        # Return statistics (mean reward, mean length and environment steps)\n",
    "        if return_dict is None: return_dict = {}\n",
    "        return_dict.update({'buffer': transition_buffer,\n",
    "                            'episode_reward': None if len(episode_rewards) == 0 else np.mean(episode_rewards),\n",
    "                            'episode_length': None if len(episode_lengths) == 0 else np.mean(episode_lengths),\n",
    "                            'episodes_amount' : len(episode_rewards),\n",
    "                            'env_steps': time})\n",
    "        return return_dict\n",
    "\n",
    "    def run_episode(self, transition_buffer=None, trim=True, return_dict=None):\n",
    "        \"\"\" Runs one episode in the environemnt.\n",
    "            Returns a dictionary containing the transition_buffer and episode statstics. \"\"\"\n",
    "        return self.run(0, transition_buffer, trim, return_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "class QController:\n",
    "    \"\"\" Controller for Q-value functions, synchronizes the model calls. \"\"\"\n",
    "\n",
    "    def __init__(self, model, num_actions=None, params={}):\n",
    "        self.lock = threading.Lock()\n",
    "        self.num_actions = model[-1].out_features if num_actions is None else num_actions\n",
    "        self.model = model\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\" Shallow copy of this controller that does not copy the model. \"\"\"\n",
    "        return QController(model=self.model, num_actions=self.num_actions)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\" Returns a generator of the underlying model parameters. \"\"\"\n",
    "        return self.model.parameters()\n",
    "\n",
    "    def sanitize_inputs(self, observation, **kwargs):\n",
    "        \"\"\" Casts numpy arrays as Tensors. \"\"\"\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = th.Tensor(observation).unsqueeze(dim=0)\n",
    "        return observation\n",
    "\n",
    "    def choose(self, observation, **kwargs):\n",
    "        \"\"\" Returns the greedy actions the agent would choose when facing an \"observation\". \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            mx = self.model(self.sanitize_inputs(observation))\n",
    "            if mx.shape[-1] > self.num_actions: mx = mx[:, :self.num_actions]\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return th.max(mx, dim=-1)[1]\n",
    "\n",
    "    def probabilities(self, observation, **kwargs):\n",
    "        \"\"\" Returns the probabilities with which the agent would choose actions (here one-hot because greedy). \"\"\"\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            mx = self.model(self.sanitize_inputs(observation))\n",
    "            if mx.shape[-1] > self.num_actions: mx = mx[:, :self.num_actions]\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "        return th.zeros(*mx.shape).scatter_(dim=-1, index=th.max(mx, dim=-1)[1].unsqueeze(dim=-1), src=th.ones(1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "params = default_params()\n",
    "n_actions, state_dim = params.get('num_actions'), params.get('states_shape')[0]\n",
    "# The model has n_action policy heads and one value head\n",
    "model = th.nn.Sequential(th.nn.Linear(state_dim, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, 512), th.nn.ReLU(),\n",
    "                         th.nn.Linear(512, 128), th.nn.ReLU(),\n",
    "                         th.nn.Linear(128, n_actions + 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "controller = QController(model, num_actions=params.get('num_actions'), params=params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "runner = Runner(controller, params=params)\n",
    "episode = runner.run(trim=False, n_steps=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'buffer': <__main__.TransitionBatch object at 0x7f57438f3ee0>, 'episode_reward': -516.6666666666666, 'episode_length': 25.666666666666668, 'episodes_amount': 3, 'env_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "print(episode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}